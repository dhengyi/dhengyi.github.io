<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[漫谈--大学期间Java Web后端成长计划，帮你斩获大厂Offer]]></title>
    <url>%2F2019%2F09%2F22%2F%E6%BC%AB%E8%B0%88-%E5%A4%A7%E5%AD%A6%E6%9C%9F%E9%97%B4Java-Web%E5%90%8E%E7%AB%AF%E6%88%90%E9%95%BF%E8%AE%A1%E5%88%92%EF%BC%8C%E5%B8%AE%E4%BD%A0%E6%96%A9%E8%8E%B7%E5%A4%A7%E5%8E%82Offer%2F</url>
    <content type="text"><![CDATA[注：以下所有面试提问的热点问题的归纳都基于博主自身的应聘经历~不同年份、环境有可能不同，仅供参考！ 引言今天又碰到一个学弟问我，“我现在比较迷茫，以后想要从事 Java 后端开发的相关工作，却不知如何系统的进行学习，学长能帮忙给一点建议吗？”。 本人目前也是一枚刚毕业的本科应届生，有幸加入美团从事 Java 后端的开发工作，回想起大学时光的学习历程，不得暗自庆幸，像不知道如何进行系统的学习，毕业后到底应该从事计算机领域哪一方面的工作等等这种类似的疑惑，基本都没有产生（准确的来说，就算是产生了，也会有学长或老师及时的帮你解决(*≧▽≦)）。 其实这种疑惑的产生应该是每一个在校生所必经的一个阶段，如果说你没有报班，不打算考研，又没有人系统的指导你学习，大学期间只是按部就班的按照课表进行知识与能力的提升，那么等你面临找工作，想要通过大公司的面试，进入一线互联网大厂进行工作，应该还是比较困难的。 博主在大学期间还算是比较系统的对 Java 后端进行了学习（有很多厉害的学长进行指点），加之在大三春招以及大四秋招期间，面试过不少的大厂（国内一二线互联网公司基本都有涉猎），发觉在 Java 后端这一块，他们提问所对应的知识模块都具有很高的相似性以及复用性，当时就想总结一篇大学期间 Java 后端的一个学习成长路线，解救广大莘莘学子于水火之中，帮助他们尽量不要迷失在庞大且复杂的知识体系中而觉得无从下手。 这里只是结合我大学四年的学习以及最后的面试经历，给大家提供的一份相对还算靠谱的学习计划，实际情况需要你结合自身发展以及未来规划作出相应的补充与完善，相信在你不断学习的过程中，会对 Java 后端的知识体系越来越明了，从而自行调整学习路线。 我将对知识的掌握程度分为三个级别： 了解：认真学习相关知识的基础概念、使用场景以及功能特性即可。 掌握：在了解的基础上需要进行实践，能够将所学知识熟练的运用到代码之中。 熟悉：在了解与掌握的基础上需要深挖其实现原理以及性能优化，需要对知识的理解达到一定的深度。 然后，我将当初觉得面试官最喜欢问的问题所属的领域，分成了如下六个大类，基本上也对应了你在大学期间所要学习并掌握的一些东西。 P.S.： 深挖一个知识点的实现原理以及性能优化等不一定需要通过阅读源码才能实现，学会科学上网，利用网络资源，查询优质技术文章，比直接阅读一堆枯燥的源码要好的多； 计算机方面大部分的书籍都特别的厚，你不能一个字一个字、一页一页的去阅读它，你要学会找重点目录，找重点知识模块，找你现在急需查漏补缺的地方，首先去攻克它，然后再对自己的知识体系进行不断的补充，而不是一上来就逮住一本书，想要把这本书中的每一个字都啃透，那这个过程必然是十分痛苦且耗时的； 在推荐阅读书籍部分，给出的书本顺序是按照学习成本、效果产出以及正确的学习顺序综合考虑的，也就是说，第一个出现的书籍是你首先需要进行学习并且是我强推的。 基础知识计算机网络计算机网络通常被分为 OSI 七层模型或五层模型，说实话，你要问我为什么有了七层模型，还要设计出层级结构与之极其类似的五层模型，这个具体原因我还真说不上来，有兴趣的同学可以自行 Google。总之，我们一般学习计算机网络，都是按照五层模型进行分级学习。 简介 详细 物理层 这一层级的知识，我学习的并不是很多，面试官提问的重点也不在这一层级，姑且定义为了解。 数链层 本人自觉极少数面试官会提问一些这部分的知识，但这并不影响数链层其所包含知识点的重要性：数链层的差错控制、流量控制、滑动窗口协议、交换机、路由器、网关等等…这些概念都要求掌握，也许这对你的面试结果来说并不重要，但对于你个人的提升以及对计算机网络整体的理解都有非常大的帮助。 网络层 重点以及难点，但同样不属于面试官频繁提问的范畴，如果你需要考研，或者进行计算机网络相关的笔试，那么网络层会涉及到 IP 地址、子网掩码、子网网段等等需要你进行计算的相关知识点。除此之外，IP 协议、报文格式、ARP 协议、ICMP 协议等等也都需要你去掌握。这一层还包括一些经典的路由算法，但本人没有进行过学习，面试时也没怎么碰到过，感觉学习成本以及难度较大，暂且定义为了解吧。 传输层 重点！重点！重点！重要的事情一般都会说三遍，属于面试时高频提问范畴、什么拥塞控制、滑动窗口协议、三次握手、四次挥手、TCP、UDP…各种经典的问题，各种不同的问法，都能问烂。要求这一层相关概念必须达到熟悉的程度，并能自行扩展相关问题：比如为何需要三次握手，挥手为何需要四次？三次挥手行不行？建议 Google 一些关于传输层的经典面试题，知悉面试套路。 应用层 重点！重点！重点！没错，同样属于面试官高频提问范畴，也是与用户接触最频繁的网络层级，熟悉 HTTP 协议、DNS 协议。有关 HTTP 协议，同样有大量经典的问题，下去自行了解吧，我就不再赘述了。除此之外，还有 CDN（内容分发）、P2P（对等传输）、HTTPS（对称密钥、公开密钥算法、数字签名）等扩展知识，如果你还掌握了这些（展示自己对知识学习的深度及广度），绝对会给面试官留下不错的印象！ 计算机网络属于 Web 后端的基础知识，打好地基更是作为一名合格程序员所要具备的基本素养，因此不可懈怠有关计算机网络知识的学习。 推荐阅读书籍：计算机网络（第七版，谢希仁著）、计算机网络（第五版，特南鲍姆、韦瑟罗尔著，可做进阶使用）、黑皮书就算了吧，如果不打算成为网络方面的专业人才… P.S.：额外吐槽一句，机械工业出版的黑皮书，虽然一个个大名如雷贯耳，封皮以及书本整体给人十分牛逼以及高大上的感觉，但是！对读者来说，简直就是一块难啃的硬骨头！ 数据结构&amp;算法说到数据结构与算法，很多同学应该都会觉得这是一座阻挠普通程序员通往优秀的大山，也是将程序员这个群体划分为三六九等的标准。 我对数据结构与算法同样抱有恐惧以及排斥心理，数据结构以及算法不同于计算机领域中的其它学科。其理论基础偏数学，又要求程序员具有良好的编码能力以及思维方式，可以说是一门很吃智商的学科，其特有的学习提升途径在计算机领域中又显得独树一帜，且被中国广大莘莘学子所讨厌——刷题。没有再比这无聊、痛苦的事情了，高三一年已经刷吐了，因此再次听到这个词，绝大多数人有排斥心理也是正常反应。 对于数据结构以及算法，怎么说呢？是很重要，并且同样属于计算机科学领域中众多分支学科的基础学科，其重要性可见一斑。但是如果你大学期间不是专业打比赛的，并且未来不会从事人工智能、机器学习、大数据等偏理论、偏科研方向相关的工作，应该不用担心这东西会影响你的面试结果以及未来发展。我初入职场，但依稀记得在哪好像听过一句话：不会数据结构与算法（这里肯定不是指完全不会，应该是说不要求你这方面非常厉害），不会影响你做架构师。 从我以及身边同学春招、秋招所经历的大厂面试情况来看，如果你不满足我上一段所述的条件，想要通过数据结构以及算法部分的面试，只需要掌握大学期间应掌握的、基础的数据结构及算法，并且能够刷掉剑指 Offer 1/3（30 道左右吧，应该） 的题量，问题已经不大了，如果能够刷掉剑指 Offer 2/3 的题量，拿大厂 Offer 应该是完全没有问题的，当然，这里说的刷题，并不是指你对着书本在电脑上敲一遍就完事了，你需要完全理解，并在舍弃书本的情况下，不说每道题能够完美无瑕的写出来，能够完成 80% 左右的正确编码量已经非常不错了。 如果你在大学期间能够达到 10w 行有效代码量，你的编程能力应该会超越 70% 以上与你一同毕业的人，它将成为你强而有力的竞争手段，而刷题（一定要刷值得去刷的题，不要整天刷一些简单的，不会促进自己成长的题，这不会增加你的有效代码量，更不会提高你的编程能力）则是增加有效代码量的重要手段之一。 刷题是巩固数据结构与算法学习最好的方式之一，也是最能提升编程能力的有效途径之一。刷题应该是一个不断积累的过程，考虑到刷题所需要花费的时间，最好的方式便是从现在开始，一周七天，每隔三天，一天留出 6 个小时，专门去牛客网刷题。从现在开始每次刷题就当模拟面试笔试，把控好时间，不断提升写题的速度以及正确率，坚持到你找工作的时候，相信笔试以及现场手写代码这一块，将完全不是问题！ P.S.：关于牛客网的作用及使用，同学们自行了解。总之，基本是各大公司进行笔试的平台，其自身功能也非常强大，很适合有针对性的训练！ 数据结构： 简介 详细 字符串 字符串也算一种数据结构，其相关题型与算法非常多，掌握相关题型。 线性表 掌握数组、链表（双向链表、循环链表等），掌握相关题型。 栈与队列 掌握相关题型。 树 掌握二叉树及相关题型、哈夫曼树、二叉查找树、平衡二叉树、了解 B- 树、B+ 树、红黑树等。 图 如果你面试的岗位不是算法相关，大学期间也没怎么参加过竞赛，图这种数据结构在面试中基本很难被问到，但即使如此，关于图的基本概念，创建图、删除图等这些基本操作还是要掌握的。图主要涉及到一些很经典的算法，例如深搜与广搜，就算在面试中属于低频考点，但在笔试中却属于高频考点。这部分的知识博主掌握的也不是很好，学习成本比较高，总之学习的道路上，合理的时间规划也很重要。 哈希表 掌握哈希（散列）算法、哈希表、了解一致性哈希、分布式哈希表。 算法： 学习算法之前，务必先理解时间、空间复杂度的概念，每个算法在学习的过程中，都需要从这两个维度对性能进行考量。 简介 详细 递归 熟悉递归算法。 排序 熟悉冒泡排序（各种变形）、选择排序、插入排序（希尔排序）、快速排序、归并排序、堆排序。后三个排序属于面试中的高频考点。 查找 熟悉二分查找。 图 熟悉深度优先搜索、广度优先搜索，了解最小生成树、最短路径（Dijkstra 算法）、拓扑排序等。 进阶 贪心算法、动态规划…这两个算法在一些公司的笔试题中会出，已经算是难度偏上的题型了。尤其是动态规划，博主在大学期间学过一段时间，过于抽象是我给它的定义，除了笔试需要，目前感觉，为了找到一份好工作而刻意学习，意义不大（个人看法）。 推荐阅读书籍：一句话，别买算法导论就行，啃不动的，买回来就只能吃灰或用来垫电脑了。本来想强推我大西邮的教材（数据结构与算法，王曙燕著），结果发现京东上没有卖…算法（第四版，Robert Sedgewick, Kevin Wayne 著）。应该还有很多优秀的书籍，但博主并没有真正的阅读过，也就不做推荐了。 Java集合 简介 详细 List 了解 List，了解 List 所属下的所有集合，熟悉 ArrayList、LinkedList。 Set 了解 Set，掌握 TreeSet、熟悉 HashSet。 Map 了解 Map，掌握 TreeMap、熟悉 JDK 1.7 以及 JDK 1.8 中的 HashMap。 在学习好这些基本的集合之后，你还需要了解一些非常经典的能够保证线程安全的集合，如：HashTable、ConcurrentHashMap 等等，不要求你现在就能够使用，但是你要了解一下他们为何能够保证线程安全，知悉部分原理。 推荐阅读书籍：疯狂 Java 讲义（很全，很详细）、Java 学习笔记（如果你在阅读上本书的过程中觉得有困难，那就先从这本书开始，这也是博主学习 Java 时阅读的第一本书，如今觉得作为入门来说还是不错的，讲的很简单）、Effective Java（没有看过，但是书的大名如雷贯耳）。 并发 简介 详细 基础 掌握线程等相关概念，掌握多线程编程，能够编写保证线程安全的并发程序。 进阶 这涉及的模块很多，比如不仅要掌握 synchronized 的使用，还要了解其实现原理以及各种锁优化，还有你是否真的理解了 volatile 关键字… 并发是整个 Java 知识体系中很重要的一个模块，也是面试官很喜欢问的一个模块，更是能够体现你学习深度的一个模块，虽然这个模块的确不太好理解，但是当你得心应手之后，便会感受到并发编程的魅力。 推荐阅读书籍：Java 多线程编程核心技术（高洪岩著，很基础地一本书，适合新手看，速度要快，博主当初用了一周的时间看完的）、Java 并发编程的艺术（进阶书籍，博主也是临近毕业才入手，公司 leader 给推荐，没有细看，过了一下目录，觉得应该是非常好的一本书，广度以及深度都不错）、Java 并发编程实战（大学期间看的书籍，起初感觉也是大名如雷贯耳，但是博主在学习的过程中，觉得这本书实在是太难啃了，书籍目录的编排挺不错，但是内容…可以作为参考书籍阅读，需要较强的自学能力，推荐指数不高）。 IO/NIO 简介 详细 基础 了解 IO/NIO，了解两者之间的区别。 进阶 NIO 主要应用在 Java 网络编程中，在 Web 开发中使用场景不多，但是博主依稀记得当时面试官也喜欢提问 NIO 的相关知识，因此对于 NIO 的学习还是要有一定的深度。感觉不做网络编程的话，没必要系统的进行学习，在网络上多找一些相关的技术文章学习即可。 推荐阅读书籍：同 Java 集合。 反射 简介 详细 基础 掌握反射。 进阶 如果你不了解反射的原理，学习反射的时候，是很懵逼的，并且看完书本之后印象应该也不是很深刻，所以学习反射这块的知识，我倒是建议可以先从原理入手学习，反射的原理并不难理解，在深入理解 Java 虚拟机一书中有详细的解释，具体对应的章节应该是第二章／第六章——“Java 内存区域”／”类文件结构”（记忆可能有误，如果不对的话请自行寻找对应章节） 推荐阅读书籍：同 Java 集合，可以先从基本看起，留有一个大概的印象，如果在看的过程中发现实在懵逼，可以先将这块的知识放下，等了解了反射的原理之后再回过头进行学习。 SpringSSM(Spring + Spring MVC + Mybatis) 是如今最流行的 Web 后端框架，也是进入公司后进行编码时所要掌握的基础技术之一，基本上 70% 的企业级 Java Web 项目都基于 SSM 框架进行搭建。 但是我并不建议在大学期间将主要精力放在如何掌握众多框架的使用上，像博主大学期间就只学过 Spring（Spring MVC 是 Spring 的一个子集）。我一直觉得，框架这种东西，版本迭代日新月异，甚至今天还很流行的框架，在下个月就有可能被更好用的框架所替代，因此对于框架的学习，绝不在多，而在精。除此之外，面试官比较注重你对技术理解的深度，凡事喜欢问原理，如果你对框架的掌握程度都停留在使用层面，那必定是无法让面试官满意的。 基本上同种类框架的设计思想、实现方式背后都能够提炼出众多共性，当你深入理解了一种类型的框架后，就算有新的框架被设计出来，你也能够比别人更迅速的掌握。 简介 详细 IOC 熟悉 IOC，最好可以自己用 Java 实现一个简易的 IOC，不难，也就 1000 行带一点的代码量（可以从网上找找灵感）。推荐阅读优质博客先快速了解 IOC 的基本概念，然后阅读 “Spring 实战” 第 1 章 1.1 节的内容。 Bean 熟悉 Bean、了解 Spring 容器。推荐阅读优质博客快速了解 Bean 的概念，然后阅读 “Spring 实战” 第 2 章，第 3 章的内容。 AOP 熟悉 AOP 及两种实现原理（Java 动态代理以及 CGLIB 动态代理）。推荐阅读优质博客快速了解 AOP 的概念，然后阅读 “Spring 实战” 第 4 章的内容。 Spring MVC 掌握 DispatcherServlet，掌握 Spring MVC 的使用。阅读 “Spring 实战” 第 5 章。 推荐阅读 “Spring 实战” 章节 除了上述提到的章节，推荐阅读第 7 章 7.1 节，7.3 节，第 16 章，第 17 章的内容（第 16 章以及第 17 章的具体内容我并没有读过，只不过是想让大家了解一下 REST 接口以及异步消息等概念，企业级应用中用的挺多的）。 推荐阅读书籍：Spring 实战（第 4 版，Craig Walls 沃尔斯著，看这本书的时候，里面的大量内容，甚至大量章节可以跳过，看完没用）、Spring 技术内幕：深入解析 Spring 架构与设计原理（第 2 版，计文柯著，听说挺不错的，博主没看过）。 P.S.：对于框架的掌握必须要快，并且能够抓住重点（这时候如果有人能带着你入门 Spring，便会得到事半功倍的效果），你才能腾出时间去了解框架背后的原理。博主当初就是自学 Spring 框架，看 Spring 实战（现在觉得这本书里废话简直不要太多）的时候花费了大量的时间，现在回过头来看，当初学到的很多东西已经过时，而 Spring 最核心的三个概念：Bean、IOC、AOP，哪至于用那么久的时间去掌握。 JVM说起 JVM，那应该属于博主在大学期间的强项了，当时对 Java 虚拟机一直抱有敬畏以及探索之心，觉得学好 Java 虚拟机就相当于抓住了 Java 语言的根源，要说整个 Java 体系中的基础，还有什么能比 JVM 更基础的呢？当然这个观点并不正确的，并且非常偏激，博主只是在这里提一下，以回味大学时的美好时光。 事实（面试经历）证明，面试官对于 JVM，并不会问的特别详细，其所问知识的所属范畴也具有非常强的规律性，博主就直接根据大学时阅读过的 “深入理解 Java 虚拟机” 一书中的目录，给大家总结出需要掌握的重要知识点。 阅读 “深入理解 Java 虚拟机” 一书之前，我先帮助大家剔除不需要阅读的章节： 章节 章节名 第 1 章 走近 Java。 第 4 章 虚拟机性能监控与故障处理工具（可以了解一下 JVM 中的一些常用命令：jstack、jmap 等，防止面试时被问到）。 第 5 章 调优案例分析与实战。 第 6 章 第 4、5、6 节 字节码指令简介（可以进行了解，好像在阅读字节码文件时会用到，面试时不问）、公有设计和私有实现、Class 文件结构的发展。 第 8 章 第 4 节 基于栈的字节码解释执行引擎。 第 9 章 类加载及执行子系统的案例与实战。 第 10 章 早期（编译期）优化。 第 11 章 晚期（运行期）优化。 下表便是我觉得大家在本科期间需要掌握并熟悉的 JVM 相关知识，标粗的知识点是面试时可能会问的，需要重点关注下。下表中知识点的出现顺序可以作为学习顺序，感觉会更快的入门 JVM。 简介 详细 类文件结构 对应第 6 章内容，属于面试低频考点，但阅读完后会使你对 Java 类的认识更加深刻。 虚拟机类加载机制 对应第 7 章内容，属于面试低频考点，类加载过程以及双亲委派模型在面试中可能略有涉及，阅读完后会对 Java 类文件被 JVM 加载至内存然后运行这一过程有比较清晰的认识。 Java 内存区域与内存溢出异常 对应第 2 章内容，属于笔试高频考点，Java 内存区域属于面试高频考点，应该就是这一章会使你明白反射的原理，阅读完后会对 Java 变量，Java 对象有全新的认识。 垃圾收集器与内存分配策略 对应第 3 章内容，属于面试高频考点！属于面试高频考点！属于面试高频考点！在这一章中你需要十分熟悉分代收集算法与内存分配与回收策略，考烂了都！除此之外，还需掌握一些垃圾收集器的概念以及功能特性，以防面试时被问到。 虚拟机字节码执行引擎 对应第 8 章内容，属于面试低频考点，阅读后会对方法调用的过程以及多态有全新的认识。 Java 语法糖的味道 对应第 10 章 10.3 小节内容，属于面试低频考点，笔试高频考点，整天都在用 Java 语法糖，你确定不去深入的了解它吗？ Java 内存模型与线程 对应第 12 章内容，Java 内存模型属于面试高频考点，也是难点，学好 Java 内存区域／Java 内存模型，你便可以设计出线程安全的并发程序！ 线程安全与锁优化 对应第 13 章内容，属于面试低频考点，但如果在面试过程中能够讲到锁优化，那绝对会带给面试官不少的惊喜。 推荐阅读书籍：深入理解 Java 虚拟机：JVM 高级特性与最佳实践（第 2 版，周志明著）。 数据库数据库在 Java Web 后端中的地位绝对比你想象中的要高得多得多，Java + 框架 + 数据库是组成每一个 Web 应用的基本单元，数据库对于 Java Web 的重要性，不压于 JVM 对于 Java 的重要性，数据结构与算法对于整个计算机科学的重要性！ 数据库的种类非常多，可以将所有类型的数据库分为两种，一种就是以 MySQL 为代表的关系型（持久化）数据库，另一种便是以 Redis 为代表的内存型（键值对存储）数据库。 MySQL 以及 Redis 也是面试时的高频考点。 MySQL 简介 详细 SQL 语句 属于面试及笔试高频考点，除基本的增删改查语句之外，子查询、联结查询、聚集函数….都是需要掌握的。平时不建议刻意去记忆，能读懂 SQL 语句就行，但是面试及笔试时经常要手写 SQL 语句，这时候你就要做一个充分的准备了。 存储引擎 属于面试高频考点，了解 InnoDB 与 MyISAM 的异同。 索引 属于面试高频考点，熟悉索引的适用场景以及功能特性，了解索引的实现原理。 外键约束 属于面试高频考点，熟悉外键适用场景、功能特性。 事务 属于面试中频考点，但很重要。熟悉事务适用场景、功能特性，熟悉事务的隔离级别（脏读、幻读等）、死锁等。 乐观锁 &amp; 悲观锁 属于面试高频考点，熟悉乐观锁 &amp; 悲观锁的适用场景、功能特性。 范式 属于面试低频考点，但有必要进行掌握。 实践 必须要学会简易数据库的设计！建议做一个 Web 项目，数据库中表的数量不少于 5（如何设计好实体表以及关系表，数据什么时候可以适当冗余？）。 进阶 如果有额外的时间精力，了解 B- 树，B+ 树，B+ 树应该是 MySQL 目前所采用的数据结构，了解一些亿级数据存储所需要掌握的高级技术——主从复制、分库分表…。 推荐阅读书籍：《阿里巴巴 Java 开发手册》– MySQL 数据库 小节，希望能知悉每条规范背后产生的原因。 再推荐一本被称作是 MySQL 数据库的圣经给大家：高性能 MySQL（第 3 版，Baron Schwartz，Peter Zaitsev，Vadim Tkachenko 著），这本书肯定不适合入门，听说读完这本书在 MySQL 领域就能达到专家级别…之前有学长读过，据说只需读完两章内容就可以给面试官吹逼… Redis数据库其实一直是博主的弱项，对于 MySQL 以及 Redis 都没有进行过系统的学习，都是在网络上找的技术博客进行零散的学习，尤其是 Redis，博主也只是稍有了解，当初面试的时候，基本上只要面试官提起 Redis，我都会尴尬的一笑（不会），因此这里目前无法给大家提供有用的建议，大家如果有好的建议，欢迎在评论区留言~ 推荐阅读书籍：Redis 实战（约西亚 L.卡尔森（Josiah，L.，Carlson） 著）、Redis 设计与实现（黄健宏著）。Redis 实战是博主买的一本书，大概翻了一下，类似于 Spring 实战，内容偏实践，基本没有涉及 Redis 的设计思想以及实现原理，至于 Redis 设计与实现身边有好多同学都在看，听说很不错，内容也很有深度，如果有时间有兴趣的话，强烈推荐阅读~ 设计模式有许多种设计模式，以下列出来的是博主目前掌握的，其中前 4 个设计模式是项目中最常用也是面试官最常考的，对付面试掌握前 4 个足矣。 简介 详细 单例模式 熟悉单例模式，熟悉如何在多线程下安全的创建单例，熟悉创建单例的各种方式（目前使用枚举进行单例的创建是最好的方式）。 工厂模式 熟悉工厂模式，熟悉工厂模式在线程池中的应用。 观察者模式 熟悉观察者模式，了解观察者模式与发布-订阅模式的异同。 代理模式 熟悉代理模式，熟悉代理模式在 AOP 实现原理中的应用。 其他模式 熟悉装饰者模式、策略模式、行为模式。 推荐阅读书籍：无。我觉得设计模式的学习是不需要买一本书细细去看的，这东西在了解过后要真正的实践，是需要技术、经验、工程、业务等多方面能力的沉淀。起码，现在我们只需要掌握这些设计模式的思想就行。 项目&amp;实习经历一般来说，想要在本科期间拿到大厂的 Offer，除了有扎实的基本功外，简历上还需要有 2-3 个拿的出手的项目（至少有一个项目是值得去讲给面试官听的）。这些项目不一定要求有多少代码量，更好的选择是这个项目解决了什么问题、或是项目中使用了哪些比较可讲的技术，甚至是我上面所说的，自己实现一个简易的 IOC 等等，都可以作为一个项目。 除了项目之外，如果你在大三下学期参加春招，能够找到一份不错的实习，那么这份经历在秋招的时候也会成为你一个强而有力的竞争点，毕竟在春招的时候就能进入大厂工作，本身就是对你能力的一份证明。不过在实习的时候也别光傻傻的干活了，毕竟咱们的主战场仍然是秋招，自身技术上的提升可不能因为实习而耽搁了。 总结说了这么多，事实上只要你大学期间好好学习，在追求技术广度的同时不放过深度，最后的结果都不会太差。然后，如果经济条件允许的话，报个好的培训班吧（差的就别去了），真的会事半功倍，也不用愁没项目可做。 最后，欢迎大家对本篇博客作出补充及提出批评~ 大四秋招应聘简历附：]]></content>
      <categories>
        <category>漫谈</category>
      </categories>
      <tags>
        <tag>Java Web</tag>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Geek--使用Hexo下的NexT.Mist主题进行博客搭建]]></title>
    <url>%2F2019%2F07%2F31%2FGeek-%E4%BD%BF%E7%94%A8Hexo%E4%B8%8B%E7%9A%84NexT-Mist%E4%B8%BB%E9%A2%98%E8%BF%9B%E8%A1%8C%E5%8D%9A%E5%AE%A2%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[]]></content>
  </entry>
  <entry>
    <title><![CDATA[Java并发--原子变量类的使用]]></title>
    <url>%2F2018%2F10%2F03%2FJava%E5%B9%B6%E5%8F%91-%E5%8E%9F%E5%AD%90%E5%8F%98%E9%87%8F%E7%B1%BB%E7%9A%84%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[注：本篇博客主要内容来源于网络，侵删~ 引言我们假设你已经熟练掌握了CAS，原子变量类等的相关概念。这篇博客中，我们主要讨论原子变量类的使用。 原子变量类原子变量类共12个，分4组： 计数器：AtomicInteger，AtomicLong，AtomicBoolean，AtomicReference。 域更新器：AtomicIntegerFieldUpdater，AtomicLongFieldUpdater，AtomicReferenceFieldUpdater。 数组：AtomicIntegerArray，AtomicLongArray，AtomicReferenceArray。 复合变量：AtomicMarkableReference，AtomicStampedReference。 在每组中我会选择其中一个较有代表性的进行分析与举例。 AtomicReference使用说明AtomicReference的作用是对”对象”进行原子操作。 源码分析123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657public class AtomicReference&lt;V&gt; implements java.io.Serializable &#123; private static final long serialVersionUID = -1848883965231344442L; // 获取Unsafe对象，Unsafe的作用是提供CAS操作 private static final Unsafe unsafe = Unsafe.getUnsafe(); private static final long valueOffset; static &#123; try &#123; // 获取相应字段相对Java对象的“起始地址”的偏移量 valueOffset = unsafe.objectFieldOffset (AtomicReference.class.getDeclaredField("value")); &#125; catch (Exception ex) &#123; throw new Error(ex); &#125; &#125; // volatile类型 private volatile V value; public AtomicReference(V initialValue) &#123; value = initialValue; &#125; public AtomicReference() &#123; &#125; public final V get() &#123; return value; &#125; public final void set(V newValue) &#123; value = newValue; &#125; public final void lazySet(V newValue) &#123; unsafe.putOrderedObject(this, valueOffset, newValue); &#125; public final boolean compareAndSet(V expect, V update) &#123; return unsafe.compareAndSwapObject(this, valueOffset, expect, update); &#125; public final boolean weakCompareAndSet(V expect, V update) &#123; return unsafe.compareAndSwapObject(this, valueOffset, expect, update); &#125; public final V getAndSet(V newValue) &#123; while (true) &#123; V x = get(); if (compareAndSet(x, newValue)) return x; &#125; &#125; public String toString() &#123; return String.valueOf(get()); &#125;&#125; 关于上述代码只有两点需要强调： valueOffset = unsafe.objectFieldOffset(AtomicReference.class.getDeclaredField(&quot;value&quot;)) 通过相关字段的偏移量获取值比直接使用反射获取相应字段的值性能要好许多； 关于lazySet，推荐阅读这一篇博客：JUC中Atomic Class之lazySet的一点疑惑。 使用举例12345678910111213141516171819202122232425262728class Person &#123; volatile long id; public Person(long id) &#123; this.id = id; &#125; public String toString() &#123; return "id:" + id; &#125;&#125;public class AtomicReferenceTest &#123; public static void main(String[] args) &#123; // 创建两个Person对象，它们的id分别是101和102。 Person p1 = new Person(101); Person p2 = new Person(102); // 新建AtomicReference对象，初始化它的值为p1对象 AtomicReference ar = new AtomicReference(p1); // 通过CAS设置ar。如果ar的值为p1的话，则将其设置为p2。 ar.compareAndSet(p1, p2); Person p3 = (Person)ar.get(); System.out.println("p3 is "+p3); System.out.println("p3.equals(p1)="+p3.equals(p1)); &#125;&#125; AtomicReferenceFieldUpdater接下来所有的原子变量类不再进行源码分析。事实上所有原子变量类的实现都大同小异，感兴趣的同学可以阅读源码。 使用说明一个基于反射的工具类，它能对指定类的指定的volatile引用字段进行原子更新。(注意这个字段不能是private的) 通过调用AtomicReferenceFieldUpdater的静态方法newUpdater就能创建它的实例，该方法要接收三个参数： 包含该字段的对象的类; 将被更新的对象的类; 将被更新的字段的名称。 使用举例12345678910111213class Dog &#123; volatile String name = "dog1";&#125; public class App &#123; public static void main(String[] args) throws Exception &#123; AtomicReferenceFieldUpdater updater = AtomicReferenceFieldUpdater.newUpdater(Dog.class, String.class, "name"); Dog dog1 = new Dog(); updater.compareAndSet(dog1, dog1.name, "test"); System.out.println(dog1.name); &#125;&#125; AtomicReferenceArray使用说明可以用原子方式更新其元素的对象引用数组。 以下是AtomicReferenceArray类中可用的重要方法的列表： 序列 方法 描述 1 public AtomicReferenceArray(int length) 构造函数，创建给定长度的新 AtomicReferenceArray。 2 public AtomicReferenceArray(E[] array) 构造函数，创建与给定数组具有相同长度的新 AtomicReferenceArray，并从给定数组复制其所有元素。 3 public boolean compareAndSet(int i, E expect, E update) 如果当前值==期望值，则将位置i处的元素原子设置为给定的更新值。 4 public E get(int i) 获取位置i的当前值。 5 public E getAndSet(int i, E newValue) 将位置i处的元素原子设置为给定值，并返回旧值。 6 public void set(int i, E newValue) 将位置i处的元素设置为给定值。 使用举例1234567891011121314151617181920212223242526272829303132333435363738394041424344public class TestThread &#123; // 创建原子引用数组 private static String[] source = new String[10]; private static AtomicReferenceArray&lt;String&gt; atomicReferenceArray = new AtomicReferenceArray&lt;String&gt;(source); public static void main(String[] args) throws InterruptedException &#123; for (int i = 0; i &lt; atomicReferenceArray.length(); i++) &#123; atomicReferenceArray.set(i, "item-2"); &#125; Thread t1 = new Thread(new Increment()); Thread t2 = new Thread(new Compare()); t1.start(); t2.start(); t1.join(); t2.join(); &#125; static class Increment implements Runnable &#123; public void run() &#123; for(int i = 0; i &lt; atomicReferenceArray.length(); i++) &#123; String add = atomicReferenceArray.getAndSet(i, "item-" + (i+1)); System.out.println("Thread " + Thread.currentThread().getId() + ", index " + i + ", value: " + add); &#125; &#125; &#125; static class Compare implements Runnable &#123; public void run() &#123; for(int i = 0; i&lt; atomicReferenceArray.length(); i++) &#123; System.out.println("Thread " + Thread.currentThread().getId() + ", index " + i + ", value: " + atomicReferenceArray.get(i)); boolean swapped = atomicReferenceArray.compareAndSet(i, "item-2", "updated-item-2"); System.out.println("Item swapped: " + swapped); if(swapped) &#123; System.out.println("Thread " + Thread.currentThread().getId() + ", index " + i + ", updated-item-2"); &#125; &#125; &#125; &#125;&#125; AtomicStampedReference使用说明AtomicStampedReference主要用来解决在使用CAS算法的过程中，可能会产生的ABA问题。一般我们会使用带有版本戳version的记录或对象标记来解决ABA问题，AtomicStampedReference实现了这个作用，它通过包装[E, Integer]的元组来对对象标记版本戳stamp。 以下是AtomicStampedReference类中可用的重要方法的列表： 序列 方法 描述 1 public AtomicStampedReference(V initialRef, int initialStamp) 构造方法，传入引用和戳。 2 public boolean compareAndSet(V expectedReference, V newReference, int expectedStamp, int newStamp) 如果当前引用 == 预期值并且当前版本戳 == 预期版本戳，将更新新的引用和新的版本戳到内存。 3 public void set(V newReference, int newStamp) 设置当前引用的新引用和版本戳。 4 public boolean attemptStamp(V expectedReference, int newStamp) 如果当前引用 == 预期引用，将更新新的版本戳到内存。 使用举例下面的代码分别用AtomicInteger和AtomicStampedReference来对初始值为100的原子整型变量进行更新，AtomicInteger会成功执行CAS操作，而加上版本戳的AtomicStampedReference对于ABA问题会执行CAS失败： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import java.util.concurrent.TimeUnit;import java.util.concurrent.atomic.AtomicInteger;import java.util.concurrent.atomic.AtomicStampedReference;public class ABA &#123; private static AtomicInteger atomicInt = new AtomicInteger(100); private static AtomicStampedReference atomicStampedRef = new AtomicStampedReference(100, 0); public static void main(String[] args) throws InterruptedException &#123; Thread intT1 = new Thread(new Runnable() &#123; @Override public void run() &#123; atomicInt.compareAndSet(100, 101); atomicInt.compareAndSet(101, 100); &#125; &#125;); Thread intT2 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; &#125; boolean c3 = atomicInt.compareAndSet(100, 101); System.out.println(c3); // true &#125; &#125;); intT1.start(); intT2.start(); intT1.join(); intT2.join(); Thread refT1 = new Thread(new Runnable() &#123; @Override public void run() &#123; try &#123; TimeUnit.SECONDS.sleep(1); &#125; catch (InterruptedException e) &#123; &#125; atomicStampedRef.compareAndSet(100, 101, atomicStampedRef.getStamp(), atomicStampedRef.getStamp() + 1); atomicStampedRef.compareAndSet(101, 100, atomicStampedRef.getStamp(), atomicStampedRef.getStamp() + 1); &#125; &#125;); Thread refT2 = new Thread(new Runnable() &#123; @Override public void run() &#123; int stamp = atomicStampedRef.getStamp(); try &#123; TimeUnit.SECONDS.sleep(2); &#125; catch (InterruptedException e) &#123; &#125; boolean c3 = atomicStampedRef.compareAndSet(100, 101, stamp, stamp + 1); System.out.println(c3); // false &#125; &#125;); refT1.start(); refT2.start(); &#125;&#125; 性能比较：锁与原子变量事实上，CAS的性能总是优于锁。我们分两种情况进行讨论。 1. 线程间竞争程度较高 对于锁来说，激烈的竞争意味着线程频繁的挂起与恢复，频繁的上下文切换，这些操作都是非常耗费系统资源的；对于CAS算法来说，激烈的竞争意味着线程将对竞争进行频繁的处理（重试，回退，放弃等策略）。 即使如此，一般来说，CAS算法的性能依旧优于锁。 2. 线程间竞争程度较低 较低的竞争程度意味着CAS操作总是能够成功；对于锁来说，虽然锁之间的竞争度也随之下降，但由于获取锁与释放锁的操作不但耗费系统资源，并且其中本身就包含着CAS操作，因此在这种情况下，CAS操作的性能依旧优于锁。 总结 这篇博客并没有讲述CAS操作以及可能产生的ABA问题，但是我们必须熟悉这两个知识点； 这篇博客的主要目的是构建起大家对原子变量类的一个认识，以至于在以后的项目开发中，可以去思考如何使用这些原子变量类； 对于原子变量与锁之间的优势与劣势，性能间的比较，有一个较为清晰的认识。 参考阅读Java并发编程实战 Java并发AtomicReferenceArray类 用AtomicStampedReference解决ABA问题]]></content>
      <categories>
        <category>Java并发</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>原子变量类</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发--深入理解显式锁]]></title>
    <url>%2F2018%2F09%2F27%2FJava%E5%B9%B6%E5%8F%91-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E6%98%BE%E5%BC%8F%E9%94%81%2F</url>
    <content type="text"><![CDATA[注：本篇博客部分内容引用自：Java并发编程：Lock 引言在Java 5.0之前，协调对共享对象的访问可以使用到的机制只有synchronized和volatile。在Java 5.0之后，增加了一种新的机制：ReentrantLock。ReentrantLock并不是一种替代内置锁的方法，而是在内置锁不再适用的情况下，作为一种可选择的高级功能。 既生synchronized，何生Locksynchronized主要在功能上存在一些局限性。 如果获取锁的线程由于要等待IO或者其他原因（比如调用sleep方法）被阻塞了，但是又没有释放锁，其他线程便只能干巴巴地等待，试想一下，这多么影响程序执行效率。因此就需要有一种机制可以不让等待的线程一直无期限地等待下去（比如只等待一定的时间或者能够响应中断），通过Lock就可以办到。 再举个例子：当有多个线程读写文件时，读操作和写操作会发生冲突现象，写操作和写操作会发生冲突现象，但是读操作和读操作不会发生冲突现象。 如果采用synchronized关键字来实现同步的话，就会导致一个问题：如果多个线程都只是进行读操作，那么当一个线程在进行读操作时，其他线程只能等待无法进行读操作。因此就需要一种机制来使得多个线程都只是进行读操作时，线程之间不会发生冲突，通过Lock就可以办到。 另外，通过Lock可以知道线程有没有成功获取到锁。这个是synchronized无法办到的。 值得注意的是：在使用Lock时，我们必须在finally块中释放锁！ 如果在被保护的代码块中抛出了异常，那么这个锁永远都无法被释放。如果没有使用finally来释放锁，当出现问题时，将很难追踪到最初发生错误的位置，因为我们没有记录应该释放锁的位置与时间。 这就是ReentrantLock不能完全替代synchronized的原因：它更加危险，因为当程序的执行控制离开被保护的代码块时，不会自动清除锁。 注：FindBugs可以帮助你找到未释放的锁。 Lock接口认识Lock我们先来看一下Lock接口的实现： 12345678910111213public interface Lock &#123; // 加锁 void lock(); // 可中断的锁 void lockInterruptibly() throws InterruptedException; // 轮询锁与定时锁 boolean tryLock(); boolean tryLock(long time, TimeUnit unit) throws InterruptedException; // 解锁 void unlock(); // 本节并不需要关注 Condition newCondition();&#125; ReentrantLock是唯一实现了Lock接口的类。在获取（释放）ReentrantLock时，有着与进入（退出）同步代码块相同的内存语义，与synchronized一样，ReentrantLock还提供了可重入的加锁语义。 tryLock方法tryLock只有在成功获取了锁的情况下才会返回true，如果别的线程当前正持有锁，则会立即返回false！如果为这个方法加上timeout参数，则会等待timeout的时间才会返回false或者在获取到锁的时候返回true。 在内置锁中，死锁是一个严重的问题，恢复程序的唯一方法是重启程序，而防止死锁的唯一方法就是在构造程序时避免出现不一致的锁顺序。可定时与可轮询的锁提供了另一种方式：避免死锁的发生。 如果不能获取所有需要的锁，那么可以使用可定时或可轮询的锁获取方式，从而使你重新获得控制权，它会释放已经获得的锁，然后重新尝试获取所有锁。无参数的tryLock一般用作轮询锁，而带有TimeUnit参数的一般用作定时锁。 考虑如下程序，它将资金从一个账户转入另一个账户。在开始转账之前，首先要获得这两个Account对象的锁，以确保通过原子方式来更新两个账户中的余额，同时又不破坏一些不变性条件，如：“账户的余额不能为负数”。 12345678910111213public void transferMoney(Account fromAccount, Account toAccount, DollarAmount amount) throws InsufficientResourcesException &#123; synchronized (fromAccount) &#123; synchronized (toAccount) &#123; if (fromAccount.getBalance().compareTo(amount) &lt; 0) &#123; throw new InsufficientResourcesException(); &#125; else &#123; fromAccount.debit(amount); toAccount.credit(amount); &#125; &#125; &#125;&#125; 这个程序看似无害，实则会发生死锁。如果两个线程同时调用transferMoney，其中一个线程从X向Y转账，另一个线程从Y向X转账，那么就会发生死锁： 12A: transferMoney(myAccount, yourAccount, 10);B: transferMoney(yourAccount, myAccount, 20); 如果执行顺序不当，那么A可能获得myAccount的锁并等待yourAccount的锁，然而B此时持有yourAccount的锁并等待myAccount的锁，就会发生死锁。 我们可以使用tryLock用作轮询锁来解决这样的问题，使用tryLock来获取两个锁，如果不能同时获得，则退回并重新尝试。程序中锁获取的休眠时间包括固定部分和随机部分，从而降低了发生活锁的可能性。如果在指定时间内不能获得所有需要的锁，那么transferMoney将返回一个失败状态，从而使该操作平缓的失败。 123456789101112131415161718192021222324252627282930313233public boolean transferMoney(Account fromAcct, Account toAcct, DollarAmount amount, long timeout, TimeUnit unit) throws InsufficientResourcesException, InterruptedException &#123; long fixedDelay = getFixedDelayComponentNanos(timeout, unit); long randMod = getRandomDelayModulusNanos(timeout, unit); long stopTime = System.nanoTime() + unit.toNanos(timeout); while (true) &#123; if (fromAcct.lock.tryLock()) &#123; try &#123; if (toAcct.lock.tryLock()) &#123; try &#123; if (fromAccount.getBalance().compareTo(amount) &lt; 0) &#123; throw new InsufficientResourcesException(); &#125; else &#123; fromAccount.debit(amount); toAccount.credit(amount); return true; &#125; &#125; finally &#123; toAcct.lock.unlock(); &#125; &#125; &#125; finally &#123; fromAcct.lock.unlock(); &#125; &#125; if (System.nanoTime() &lt; stopTime) &#123; return false; &#125; NANOSECONDS.sleep(fixedDelay + rnd.nextLong() % randMod); &#125;&#125; tryLock用作定时锁的程序如下： 12345678910111213public boolean trySendOnSharedLine(String message, long timeout, TimeUnit unit) throws InterruptedException &#123; long nanosToLock = unit.toNanos(timeout) - estimatedNanosToSend(message); if (!lock.tryLock(nanosToLock, NANOSECONDS)) &#123; return false; &#125; try &#123; return trySendOnSharedLine(message); &#125; finally &#123; lock.unlock(); &#125;&#125; 上述程序试图在Lock保护的共享通信线路上发送一条消息，如果不能在指定的时间内完成，代码就会失败。定时的tryLock能够在这种带有时间限制的操作中实现独占加锁的行为。 lockInterruptibly方法lockInterruptibly方法比较特殊，当通过这个方法去获取锁时，如果线程正在等待获取锁，则这个线程能够响应中断，即中断线程的等待状态。也就使说，当两个线程同时通过lock.lockInterruptibly()想获取某个锁时，假若此时线程A获取到了锁，而线程B只有在等待，那么对线程B调用threadB.interrupt()方法能够中断线程B的等待过程。 由于在lockInterruptibly方法的声明中抛出了异常，所以lock.lockInterruptibly()必须放在try块中或者在调用lockInterruptibly的方法外声明抛出InterruptedException。 因此lockInterruptibly一般的使用形式如下： 12345678public void method() throws InterruptedException &#123; lock.lockInterruptibly(); try &#123; // ..... &#125; finally &#123; lock.unlock(); &#125; &#125; 注意，当一个线程获取了锁之后，是不会被interrupt方法中断的。因为单独调用interrupt方法不能中断正在运行过程中的线程，只能中断阻塞过程中的线程。因此当通过lockInterruptibly方法获取某个锁时，如果不能获取到，只有在进行等待的情况下，是可以响应中断的。 定时的tryLock同样能够响应中断，因此当需要实现一个定时的和可中断的锁获取操作时，可以使用tryLock方法。 公平锁公平锁即尽量以请求锁的顺序来获取锁。比如同是有多个线程在等待一个锁，当这个锁被释放时，等待时间最久的线程（最先请求的线程）会获得该锁，这种就是公平锁。 非公平锁即无法保证锁的获取是按照请求锁的顺序进行的。这样就可能导致某个或者一些线程永远获取不到锁。 在Java中，synchronized就是非公平锁，它无法保证等待的线程获取锁的顺序。而对于ReentrantLock和ReentrantReadWriteLock，它默认情况下是非公平锁，但是可以设置为公平锁。 在ReentrantLock中定义了2个静态内部类，一个是NotFairSync，一个是FairSync，分别用来实现非公平锁和公平锁。我们可以在创建ReentrantLock对象时，通过以下方式来设置锁的公平性： ReentrantLock lock = new ReentrantLock(true); 参数为true表示为公平锁，为fasle为非公平锁。默认情况下，如果使用无参构造器，则是非公平锁。 另外在ReentrantLock类中定义了很多方法，比如： 1234567isFair() //判断锁是否是公平锁isLocked() //判断锁是否被任何线程获取了isHeldByCurrentThread() //判断锁是否被当前线程获取了hasQueuedThreads() //判断是否有线程在等待该锁 在ReentrantReadWriteLock中也有类似的方法，同样也可以设置为公平锁和非公平锁。不过要记住，ReentrantReadWriteLock并未实现Lock接口，它实现的是ReadWriteLock接口。 在synchronized与ReentrantLock之间进行抉择在性能上，Java 5.0中ReentrantLock远远优于内置锁，而在Java 6.0中则是略有胜出。 我们建议，仅当内置锁不能满足需求时，才可以考虑使用ReentrantLock。 在Java 8.0中，内置锁的性能已经不压于ReentrantLock，并且未来更可能会继续提升synchronized的性能，毕竟synchronized是JVM的内置属性。 总结 清楚为什么有Lock接口； 清楚使用ReentrantLock有什么优缺点； 掌握如何使用ReentrantLock（定时锁，轮询锁，中断锁以及一些其他功能）； 能够在synchronized与Lock中做出选择。 参考阅读Java并发编程实战 Java并发编程：Lock]]></content>
      <categories>
        <category>Java并发</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>显式锁</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发--深入理解线程池]]></title>
    <url>%2F2018%2F09%2F13%2FJava%E5%B9%B6%E5%8F%91-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E7%BA%BF%E7%A8%8B%E6%B1%A0%2F</url>
    <content type="text"><![CDATA[为什么需要线程池在生产环境中，我们不能无限制的创建线程，主要原因如下： 线程创建与销毁的代价并不低； 如果可运行的线程数量多于可用处理器的数量，有些线程将会闲置，大量闲置的线程会消耗系统资源（内存）并给垃圾收集器带来压力； 大量线程竞争CPU也会造成不小的性能开销。 Executor框架Executor框架在Java 5中被引入，其内部使用了线程池机制。它在java.util.cocurrent包下，通过该框架来控制线程的启动、执行和关闭，可以简化并发编程的操作。 Executor框架包括：线程池，Executor，Executors，ExecutorService等（Callable与Future本篇不进行讨论）。 Executor接口我们先来了解一下其中的Executor接口： 123public interface Executor &#123; void execute(Runnable command);&#125; Executor接口的定义非常简单，但它却为灵活且强大的异步任务执行框架提供了能够支持多种不同类型任务的执行策略。它提供一种标准的方法将任务的提交过程与执行过程进行了解耦。 Executor接口基于生产者 — 消费者模型，提交任务的操作相当与生产者，执行任务的线程相当于消费者。 线程池的实现操作了Executor接口，但现在，我们只关心它是如何将任务提交与任务执行进行解耦的。来看一个例子： 12345678910111213141516171819202122232425262728293031import java.io.IOException;import java.net.ServerSocket;import java.net.Socket;import java.util.concurrent.Executor;import java.util.concurrent.Executors;/** * @author dhengyi * @since 2018/9/4 23:29 **/public class TaskExecutionWebServer &#123; private static final int NTHREADS = 100; private static final Executor executor = Executors.newFixedThreadPool(NTHREADS); public static void main(String[] args) throws IOException &#123; ServerSocket serverSocket = new ServerSocket(80); while (true) &#123; final Socket connection = serverSocket.accept(); // 任务创建 Runnable task = new Runnable() &#123; @Override public void run() &#123; &#125; &#125;; // 任务提交与执行 executor.execute(task); &#125; &#125;&#125; 对上述代码进行修改，将任务的执行改为为每个任务都创建新的线程： 123456789101112import java.util.concurrent.Executor;/** * @author dhengyi * @since 2018/9/5 9:37 **/public class ThreadPerTaskExecutor implements Executor &#123; @Override public void execute(Runnable command) &#123; new Thread(command).start(); &#125;&#125; 我们还可以对其进行修改，使TaskExecutionWebServer的行为类似于单线程的行为，即以同步的方式执行每个任务： 123456789101112import java.util.concurrent.Executor;/** * @author dhengyi * @since 2018/9/5 9:43 **/public class WithinThreadExecutor implements Executor &#123; @Override public void execute(Runnable command) &#123; command.run(); &#125;&#125; 通过使用Executor，我们将任务的提交与执行进行了解耦，我们只需采用另一种不同的Executor实现，就完全可以改变应用程序的行为。改变Executor实现或配置所带来的影响要远远小于改变任务提交方式带来的影响。 Executor的生命周期—ExecutorService接口我们已经知道了如何创建一个Executor，但JVM只有在所有非守护线程全部终止后才会退出，因此我们还需讨论一下Executor如何关闭。 关闭应用程序的方式我们通常分为两种： 平缓的关闭：完成所有已启动的任务，拒绝接受新任务。 粗暴的关闭：直接取消所有任务，拒绝接受新任务。 为了便于管理执行服务的生命周期，Executor扩展了ExecutorService接口，如下： 123456789101112public interface ExecutorService extends Executor &#123; // 平缓的关闭 void shutdown(); // 粗暴的关闭 List&lt;Runnable&gt; shutdownNow(); boolean isShutdown(); boolean isTerminated(); // 等待终止，通常在调用此方法后会立即调用shutdown，从而产生同步关闭ExecutorService的效果 boolean awaitTermination(long timeout, TimeUnit unit) throws InterruptedException; // ... ... 其他用于任务提交的便利方法&#125; ExecutorService的生命周期有三种状态：运行，关闭，终止。 创建ExecutorService时处于运行态（RUNNING）。我们可以通过调用线程池的shutdown或shutdownNow方法来关闭线程池，在调用这两个方法之一后，线程池将不再接收新任务。它们的原理是遍历线程池中的工作线程，然后逐个调用线程的interrupt方法来中断线程，所以无法响应中断的任务可能永远无法终止。但是它们存在一定的区别，shutdownNow首先将线程池的状态设置成STOP，然后尝试停止所有的正在执行或暂停任务的线程，并返回等待执行任务的列表，而shutdown只是将线程池的状态设置成SHUTDOWN状态，然后中断所有没有正在执行任务的线程。 只要调用了这两个关闭方法的其中一个，isShutdown方法就会返回true。当所有的任务都已关闭，且任务缓存队列已经清空或执行结束后才表示线程池关闭成功，进入终止态（TERMINATED），这时调用isTerminaed方法会返回true。 线程池的创建—Executors关于线程池的优势不再多说。我们可以通过Executors中的静态工厂方法创建一个线程池： newFixedThreadPool：创建固定大小的线程池，每当提交一个任务就创建一个线程，直到达到线程池最大数量，如果某个线程发生了Exception异常，线程池会补充一个新线程； newCachedThreadPool：可缓存线程池，如果线程池中有空闲的线程，那么将会回收空闲线程，当任务数量增加时，则添加新的线程，线程池的规模不受限制； newSingleThreadExecutor：单线程Executor，如果此线程出现异常，会创建另一个线程进行替代。它会确保依照任务在队列中的顺序来串行执行； newScheduledThreadPool：创建固定大小线程池并以延迟或定时的方式来执行任务。 线程池的使用认识ThreadPoolExecutor如果Executors提供默认的静态工厂方法创建的线程池不能满足需求，我们可以通过ThreadPoolExecutor的构造函数实例化一个对象，根据自己的需求定制相应线程池，ThreadPoolExecutor定义了许多构造函数，我们给出最常见的形式： 123456789public ThreadPoolExecutor(int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, BlockingQueue&lt;Runnable&gt; workQueue, ThreadFactory threadFactory, RejectedExecutionHandler handler) &#123; ... ...&#125; 其实通过源码我们可以知道：ThreadPoolExecutor继承了类AbstractExecutorService，抽象类AbstractExecutorService实现了ExecutorService接口，基本实现了ExecutorService中声明的所有方法，ExecutorService接口继承了Executor接口，因此ThreadPoolExecutor也基于Executor接口。 我们分模块对上述参数进行描述。 线程的创建与销毁corePoolSize（基本大小），maximumPoolSize（最大大小），keepAliveTime（存活时间）等因素共同负责线程的创建与销毁。 基本大小：当提交一个任务到线程池时，线程池会创建一个线程来执行任务，即使其他空闲的基本线程能够执行新任务也会创建线程，如果调用了线程池的prestartAllCoreThreads方法，线程池会提前创建并启动所有基本线程。只有在工作队列满时才会创建超出这个数量的线程；最大大小：线程池中可同时活动的线程数量的上限。若某个线程的空闲时间超过存活时间，则此线程被标记为可回收，并当线程池当前大小超过基本大小时，此线程将被终止。 newFixedThreadPool工厂方法将线程池的基本大小与最大大小设置为参数中指定的值并且两者相等，且创建的线程池不会超时；newCachedThreadPool工厂方法将线程池的最大大小设为Integer.MAX_VALUE，而将基本大小设置为0，超时设置为1分钟。其他形式的线程池可以通过显式的ThreadPoolExecutor构造函数进行构造。 注：在将基本大小设置为0之后，有一些值得注意的事项。只有当线程池中的线程数量等于线程池的基本大小并且工作队列已满的情况下，ThreadPoolExecutor才会创建新的线程。因此，如果线程池的基本大小为0，但工作队列还有容量，那么把任务交给线程池时，只有当线程池的工作队列被填满之后，才会执行任务。这通常不是我们所期望的。（将基本大小设置为0的主要目的为当没有任务执行时，销毁工作线程以免阻碍JVM的退出） 任务队列当任务到达线程池的速率超过了线程池的处理速率，那么新到来的任务将会累积起来，我们在线程池中使用一个由Executor管理的Runnable队列来保存等待执行的任务。使用任务队列的好处在于降低了这些任务对CPU资源的竞争，任务队列可以缓解任务的突增问题，但如果任务持续高速的到来，依旧可能耗尽内存资源（阻塞队列没有边界）。 任务队列分为三种：有界队列，无界队列，同步移交。 newFixedThreadPool和newSingleThreadExecutor在默认情况下都使用无界队列：LinkedBlockingQueue。 我们建议使用有界队列，例如：ArrayBlockingQueue，有界的LinkedBlockingQueue，PriorityBlockingQueue。有界队列有助于避免资源的耗尽。 在newCachedThreadPool中则使用了SynchronousQueue（Java 5，在Java 6中提供了一个新的非阻塞算法来替代了SynchronousQueue）。SynchronousQueue并不是一个真正的队列，而是一种在线程之间进行移交的机制。要将一个元素放入SynchronousQueue中，必须要有另一个线程正在等待接收这个元素。如果没有线程正在等待，并且线程池的当前大小小于最大值，则ThreadPoolExecutor将创建一个新的线程，否则将拒绝这个任务。只有当线程池是无界的或是可以拒绝任务时，SynchronousQueue才具有实际价值。使用这种方式的优势很明显：任务会直接移交给执行它的线程，而不是被首先放在队列中。 对于Executor，newCachedThreadPool是一种很好的默认选择。 只有任务独立，为线程池或工作队列设置界限才是合理的。如果任务之间存在依赖性，那么有界的线程池或工作队列则可能导致线程出现“饥饿”死锁问题，此时应使用无界线程池如：newCachedThreadPool。 线程工厂线程池中的线程都是由线程工厂进行创建的。默认的线程工厂创建一个新的，非守护的线程。我们可以通过指定一个线程工厂方法，来定制线程池的配置信息。ThreadFactory接口如下： 123public interface ThreadFactory &#123; Thread newThread(Runnable r);&#125; 每当线程池中创建一个新线程，都会调用这个newThread方法。 通常，我们都会使用定制的线程工厂方法，我们可能希望实例化一个定制的Thread类用于执行调试信息的记录，可能希望修改线程优先级，或者只是为了给线程取一个更有意义的名字。在如下程序中，我们给出一个自定义的线程工厂： 123456789101112131415161718import java.util.concurrent.ThreadFactory;/** * @author dhengyi * @since 2018/9/12 22:48 **/public class MyThreadFactory implements ThreadFactory &#123; private final String poolName; public MyThreadFactory(String poolName) &#123; this.poolName = poolName; &#125; @Override public Thread newThread(Runnable r) &#123; return new MyAppThread(r, poolName); &#125;&#125; 我们还可以在MyAppThread中定制其他行为，包括为线程指定名字，设置自定义的UncaughtExceptionHandler向Logger中写入信息，维护一些统计信息（多少个线程被创建与销毁），在线程被创建或终止时把调试信息写入日志。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758import java.util.concurrent.atomic.AtomicInteger;import java.util.logging.Level;import java.util.logging.Logger;/** * @author dhengyi * @since 2018/9/12 21:51 **/public class MyAppThread extends Thread &#123; public static final String DEFAULT_NAME = "MyAppThread"; private static volatile boolean debugLifecycle = false; private static final AtomicInteger created = new AtomicInteger(); private static final AtomicInteger alive = new AtomicInteger(); private static final Logger log = Logger.getAnonymousLogger(); public MyAppThread(Runnable runnable) &#123; this(runnable, DEFAULT_NAME); &#125; public MyAppThread(Runnable runnable, String name) &#123; super(runnable, name + "-" + created.incrementAndGet()); setUncaughtExceptionHandler( new Thread.UncaughtExceptionHandler() &#123; public void uncaughtException(Thread t, Throwable e) &#123; log.log(Level.SEVERE, "UNCAUGHT in thread" + t.getName(), e); &#125; &#125;); &#125; @Override public void run() &#123; boolean debug = debugLifecycle; if (debug) log.log(Level.FINE, "Created " + getName()); try &#123; alive.incrementAndGet(); super.run(); &#125; finally &#123; alive.decrementAndGet(); if (debug) log.log(Level.FINE, "Exiting " + getName()); &#125; &#125; public static int getThreadsCreated() &#123; return created.get(); &#125; public static int getThreadsAlive() &#123; return alive.get(); &#125; public static boolean getBug() &#123; return debugLifecycle; &#125; public static void setDebug(boolean b) &#123; debugLifecycle = b; &#125;&#125; 饱和策略当有界队列被填满之后，饱和策略开始发挥作用。我们可以通过ThreadPoolExecutor的setRejectedExecutionHandler方法来选择不同的饱和策略。JDK主要提供了以下几种不同的饱和策略： AbortPolicy（中止策略）：默认的饱和策略，会抛出未检查的RejectedExecutionException。我们可以捕获这个异常，并按需编写自己的处理代码。 DiscardPolicy（抛弃策略）：当任务队列已满，抛弃策略会抛弃该任务。 DiscardOldestPolicy（抛弃最旧策略）：会抛弃下一个将要执行的任务（入队最早的任务，可以理解为最旧的任务），然后尝试重新提交新任务。 CallerRunsPolicy（调用者运行策略）：该策略不会抛弃任务，也不会抛出异常，而是将某些任务回退到调用者。它不会在线程池中的某个线程中执行任务，而是在调用了execute的线程中执行该任务。因此当工作队列已满，并且线程池中线程数量已达maximumPoolSize时，下一个任务会在调用execute的主线程中执行。由于任务执行需要一定的时间，因此主线程在这段时间内不会调用accept，因此到达的请求将被保存在TCP层的队列中而不是应用程序的队列中。如果持续过载，那么TCP层的缓冲队列也将会被填满，因此同样会抛弃请求。但对于服务器来说，这种过载情况是逐渐向外蔓延开的 — 从线程池队列到应用程序再到TCP层，最终到达客户端，这是一种平缓的性能降低。 如下，我们使用了“调用者运行”饱和策略： 123ThreadPoolExecutor executor = new ThreadPoolExecutor(N_THREADS, N_THREADS, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue&lt;Runnable&gt;(CAPACITY));executor.setRejectedExecutionHandler(new ThreadPoolExecutor.CallerRunsPolicy()); 总结1. 熟悉Executor框架； 注：所有的线程池都基于Executor接口，ExecutorService接口继承于Executor，提供了对线程池生命周期的管理，Executors提供了默认的几种创建线程池的工厂方法。 2. 熟练掌握ThreadPoolExecutor的使用，熟悉ThreadPoolExecutor中各个参数使用及含义； 注：当默认提供的线程池不能满足自己的需求，我们就需要通过ThreadPoolExecutor定制线程池。 3. 线程池还有诸多细节； 注：如何合理配置线程池的大小，继承ThreadPoolExecutor对其进行扩展（beforeExecutor，afterExecutor，terminated） 参考阅读Java并发编程实战 聊聊并发 — Java线程池的分析和使用 Java并发编程：线程池的使用 — 很详细的一篇博客，其中还讨论了线程池的实现细节]]></content>
      <categories>
        <category>Java并发</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>线程池</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络--详解P2P对等网络（二）—Chord算法研究与分析]]></title>
    <url>%2F2018%2F06%2F28%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-%E8%AF%A6%E8%A7%A3P2P%E5%AF%B9%E7%AD%89%E7%BD%91%E7%BB%9C%EF%BC%88%E4%BA%8C%EF%BC%89%E2%80%94Chord%E7%AE%97%E6%B3%95%E7%A0%94%E7%A9%B6%E4%B8%8E%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[结构化与非结构化网络非结构化的P2P网络是指网络节点之间不存在组织关系，节点之间完全是对等的，比如第一代P2P网络Napster。 结构化的P2P网络与非结构化恰好相反，我们认为网络在逻辑上存在一个人为设计的结构，比如Chord假定网络是一个环，Kadelima则假定为一颗二叉树。有了这些逻辑结构，就给我们资源查找引入了更多的算法和思路。 引言我们在 计算机网络–详解P2P对等网络（一）—BitTorrent协议 这一篇博客中讲述了BT下载的过程：在对等用户拿到种子文件的时候，首先会联系tracker服务器，然后加入用户集群，并在用户集群中寻找自己所需的内容，最后与拥有内容的对等用户进行联系。 从BT下载的过程中引出本节所要讨论的问题：如何高效的从用户集群中找出哪些对等用户拥有你正在寻求的具体内容？ 在历史中有三种比较典型的模型来解决这个问题： Napster：使用一个中心服务器接收所有的查询，服务器告知去哪下载其所需要的数据。存在的问题是中心服务器单点失效导致整个网络瘫痪。 Gnutella：使用消息洪泛（message flooding）来定位数据。一个消息被发到用户集群内每一个节点，直到找到其需要的数据为止。存在的问题是消息数与节点数成线性关系，导致网络负载较重。 SN型：超级节点（Super Node），SN保存网络中节点的索引信息，这一点和中心服务器类型一样，但是网内有多个SN，其索引信息会在这些SN中进行传播，所以整个系统的崩溃几率就会小很多。尽管如此，网络还是有崩溃的可能。 关于P2P网络拓扑结构更详细的内容，请参考：P2P网络的拓扑结构。 现在的研究结果中，Chord、Pastry、CAN和Tapestry等常用于构建结构化P2P的分布式哈希表系统。 Chord算法是麻省理工学院（MIT）提出的一种基于DHT技术的结构化P2P路由协议，具有完全分布式、负载均衡、可用性及可扩展性好、命名方式灵活等特点。本文主要对Chord算法展开分析。 分布式哈希表（DHT）对于本节问题的思考，我们可以给出一种基本的解决方案：每个对等节点维护了一张路由表（索引），这张路由表只保存了少量有关其他节点的信息，这个特点意味着它保持最新索引的代价不会很昂贵。其次，每个节点可以快速的查看索引中的表项，否则，它就不是个有效的索引。最后，每个节点可以同时使用索引，即使其他节点来来去去，这个属性意味着索引的性能随着节点数量的增长反而越来越好~ 该解决方案就被称为分布式哈希表，因为对等节点所维护的路由表就是一张索引表，而索引的基本功能就是将一个关键字映射到一个值。这简直就是一张哈希表，但是我们的解决方案是分布式版本。我们可以再看一下维基对于DHT的定义： 分布式哈希表（distributed hash table，缩写DHT）：分布式计算系统中的一类，用来将一个关键值（key）的集合分散到所有在分布式系统中的节点，并且可以有效地将消息转送到唯一一个拥有查询者提供的关键值的节点（Peers）。这里的节点类似散列表中的存储位置。分布式散列表通常是为了拥有极大节点数量的系统，而且在系统的节点常常会加入或离开（例如网络断线）而设计的。在一个结构性的覆盖网络（overlay network）中，参加的节点需要与系统中一小部分的节点沟通，这也需要使用分布式散列表。 上述我特意加粗的语句，正是对P2P网络架构的描述。 如果对于DHT的概念还抱有一定的疑惑，可以在网上搜寻更白话的说明，博主不再进行贴出。 DHT与一致性哈希如上所述，DHT的主要想法是把网络上资源的存取像哈希表一样，可以简单而快速地进行put、get。与一致性哈希相比，DHT更强调的是资源的存取，而不管添加删除节点时产生的资源震荡的问题。与一致性哈希相同的是，DHT也只是一个概念，具体细节留给各实现。 当前这些P2P实现可以被作为DHT的具体实现，再次列举一些有代表性的实现： Chord、CAN、Tapestry、Pastry、Apache Cassandra、Kadelima、P-Grid、BitTorrent DHT Chord算法Chord是什么？Chord是一个算法，也是一个协议。作为一个算法，Chord可以从数学的角度严格证明其正确性和收敛性；作为一个协议，Chord详细定义了每个环节的消息类型。当然，Chord之所以受追捧，还有一个主要原因就是Chord足够简单，3000行的代码就足以实现一个完整的Chord。 Chord概述Chord的实现方式如下：给定一个关键字Key，将其映射到某个节点。为此，采用相同哈希函数（SHA-1）为每个节点和关键字产生一个m bit的ID，并按照ID大小构成环形拓扑。节点所产生的ID被称为节点标识符，关键字所产生的ID我们称它为关键字ID。运行Chord的主机相互连接构成Chord网络，这是一个建立在IP网络之上的覆盖（overlay）网络。每个节点N有2个邻居：以顺时针为正方向排列在N之前的第1个节点称为N的前继（predecessor），在N之后的第1个节点称为N的后继（successor）。如下图（蓝色节点为节点ID，白色节点为关键字ID）： 同一致性哈希一样，资源放置在关键字ID的后继节点上，如上图，资源2被放置在节点3中。 Finger表我们在本篇博客分布式哈希表（DHT）一节中已经讲过，每个对等节点都会维护一张路由表，以便在用户集群中寻找拥有所需资源的其他对等节点。这张路由表就被称为Finger表，Finger表的表项大小为m，由两列数据项组成，如下： ID+2的i次方 successor 其中ID就代表节点标识符，i表示Finger表中表项的下标，从0开始，successor则表示存储资源的后继节点。 举个例子：我们现在有一个m = 3的Chord环，它可以容纳2的3次方，也就是8个节点。现在有4台机器，假设它们经过哈希之后所产生的ID为0，1，2，6，那么机器1中将要维护的Finger表如下： i ID+2的i次方 successor 0 2 2 1 3 6 2 5 6 其中ID+2的i次方表示的是关键字ID。 对于上表的解释，由一致性哈希可知： 机器1本地存储着关键字ID为1的数据，机器2本地存储着关键字ID为2的数据，机器6本地存储着关键字ID为3，4，5，6的数据，机器0本地存储着关键字ID为7，0 的数据。 与此同时，如上表，机器1上，还存储着关键字ID为2，3，5的数据所在的机器地址。比如，机器1知道，关键字ID为5的数据存储在机器6上面。 Chord的查找Chord采取幂次逼近查询法。任何一个节点收到查询关键字ID为“K”的请求时，首先检查K是否落在该节点标识和它的后继节点标识之间，如果是的话，这个后继节点就是存储目标(K, V)对的节点。否则，节点将查找它的Finger表，找到表中节点标识符最大但不超过K的节点，并将这个查询请求转发给该节点。通过重复这个过程，最终可以定位到K的后继节点，即存储有目标(K, V)对的节点。 比如，当机器1接收到查询关键字ID为7的数据在哪台机器上时，它发现关键字ID“7”并不在该节点标识符和它的后继节点标识符之间，因此它查找节点标识符最大但没有超过7的节点，为6，于是将查询请求转发到机器6上。 机器6的路由表按照上述规则进行生成，如下（环形拓扑）： i ID+2的i次方 successor 0 7 0 1 0 0 2 2 2 机器6上的路由表指出：关键字ID为7的数据在机器0上… …重复这个过程，最终可找到保存关键字ID为7的资源的节点。 通过在每台机器上保存m项的路由信息，上面的方式可以做到O(logN)的查询时间复杂度。另外，比如Amazon Dynamo在论文中所说：通过在每台机子上保存足够多的路由信息，理论上可以做到O(1)时间的查询（相应的，节点间冗余信息也会更多）。 节点的加入新节点的加入需要一个称为向导的已知节点（n0）进行协助，任何一个运行在Chord网络中的节点都可以充当这个角色。加入过程包括新节点本身的Join操作和被其他节点发现2个阶段。如下图所示，假设np和ns是Chord网络中相邻两节点，n为新节点，它加入网络后应该位于np和ns节点之间。 新节点的加入有三个操作： Join() ：n加入一个Chord环，已知其中有一个向导节点n0； Stabilize(): 每个节点在后台周期性的进行此项操作，查询自身节点的后继节点的前序节点是否是自身，如果不是自身，说明有新加入的节点，此时将自身的后继节点修改为新加入的节点； Notify(n): n通知其他节点它的存在，若此时其他节点没有前序节点或n比其现有的前序节点更加靠近自身，则将n设置为前序节点。 在了解了上述三个操作之后，我们讨论一下n节点加入的具体过程： n请求向导为它查找后继 (即ns)，并初始化自身Finger表，按照Finger表的定义，此时只有n对自身属性进行了设置，其他节点并不知道新节点的加入（如上图a）； 在n节点将自身的后继节点修改为ns后，会对ns进行Notify(n)操作，即n节点通知ns它的存在，此时ns标记n成为自己的前序节点； 所有节点会在后台周期性的进行Stabilize操作，此时np发现ns的前序结点已不是自身，则np将自己的后继节点修改为n； np对n进行Notify(np)操作，n接到通知，将np修改为自己的前序节点。 在这里有一个问题：向导节点如何帮助新加入的节点寻找它的后继节点以及新加入的节点如何初始化其Finger表？ 第一点：对于新节点n，通过向向导节点提交查找n自身节点标识符的请求，向导节点检索其后继；第二点：新节点通过向向导节点请求ID + 2的m次方从而构建Finger表。 节点的失效节点的失效是节点没有通知其他节点而突然离开网络，这通常由主机崩溃或IP网络断开等意外原因造成，此时失效节点的前继保存的后继信息变得不可用，从而造成Chord环的断裂。为了处理这个问题，需要周期性的对节点的前序和后继进行探测。如果节点n发现其后继或前序已经失效，则从Finger表中顺序查找第1个可用节点进行替换，并重建Finger表。对前序节点失效的处理仍需要借助于Notify消息。考虑上图中的例子，ns虽然能够感知n的失效却无法进行修复。由于上述对后继失效的处理过程能够保证Chord环后继链的正确性，因此np通过在Stabilize中向新后继ns发送Notify，把ns的前继改成np。值得注意的是，其他节点也可能在Finger表项中保存有失效节点的记录，因此需要多次Stabilize，把失效信息扩散到Chord网络中。虽然这种方法最终能够保证Chord网络的完整性，但在节点频繁进出的情况下，其效率仍须更深入地研究。 节点的退出由于节点失效处理方法是稳定的，因此节点的退出可看作为失效而不采取其他附加措施。但基于效率的考虑，节点n退出时进行如下操作：1. 把n后继节点的前继改成n的前继；2. 把n前继节点的后继改成n的后继；3. 从n前继的Finger表中删除n。 总结 熟悉DHT、一致性哈希、Chord算法之间的概念及联系； 熟悉Chord算法的思想（Finger表的构建、Chord查询、节点的加入等）； 了解P2P网络的一些其它拓扑结构。 PS：关于Chord算法数学角度上的证明与分析，有兴趣的同学可以自行查阅相关资料~ 参考阅读计算机网络（第五版） — Andrew S. TanenBaum/David J. Wetherall 结构化P2P网络chord算法研究与分析 分布式哈希算法 【学术之门之P2P算法研读】P2P中的Chord算法 Chord算法（原理）]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>P2P</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查找--深入理解一致性哈希算法]]></title>
    <url>%2F2018%2F06%2F26%2F%E6%9F%A5%E6%89%BE-%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3%E4%B8%80%E8%87%B4%E6%80%A7%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95%2F</url>
    <content type="text"><![CDATA[注：本篇博客只是讲述了一致性哈希的思想，我们会在之后讲述分布式哈希表以及一致性哈希的一种实现（Chord算法）。 什么是一致性哈希算法？引用自维基百科： 一致性哈希是一种特殊的哈希算法。在使用一致哈希算法后，哈希表槽位数（大小）的改变平均只需要对 K/n个关键字重新映射，其中K是关键字的数量，n是槽位数量。然而在传统的哈希表中，添加或删除一个槽位几乎需要对所有关键字进行重新映射。 总结：一致性哈希算法主要关注的是在分布式架构中，当节点数目发生变化的时候（增加/删除），怎样使再哈希的数据量最少。 一致性哈希的引出在分布式系统中，节点的宕机、某个节点加入或者移出集群是常事。对于分布式存储而言，假设存储集群中有10台机器（node），如果采用传统Hash方式对数据分片（item）(即数据根据哈希函数映射到某台机器上存储)，哈希函数应该是这样的：hash(item) % 10。根据上面的介绍，当node数发生变化（增加、移除）后，数据会被重新“打散”，导致大部分数据不能落到原来的节点上，从而导致大量数据需要迁移，而这种移动会造成网络的负载。 那么，一个亟待解决的问题就变成了：当node数发生变化时，如何保证尽量少引起迁移呢？即当增加或者删除节点时，对于大多数item，保证原来分配到的某个node，现在仍然应该分配到那个node，将数据迁移量降到最低。 一致性哈希的原理及优劣 传统的Hash算法将对应的key哈希到一个具有2^32次方（int）个桶的空间中，即0~(2^32)-1的数字空间中。现在我们可以将这些数字头尾相连，想象成一个闭合的环形。也就是将传统的线性哈希表构造成环形哈希表。 我们来看一下如何将数据映射到环形哈希表上： 如上图，有三台机器（node），四个数据项（item），每台机器对应着一个n位的ID（采用机器的IP或者机器唯一的别名作为哈希函数的输入值），并且映射到环中，每个查询键，也是一个n位的ID，节点的ID和查询键对应着相同的映射空间。三台机器将整个环分割成了三部分，分别是(1,3)，(3,2)，(2,1)。 机器1负责存储落在(2,1)范围内的数据，机器2负责存储落在(3,2)范围内的数据….. 也就是说，对数据进行Hash时，数据的地址会落在环上的某个点上，数据就存储在该点的顺时针方向上的那台机器上。 这种数据存储的方式，相比于普通哈希方式，有明显的优势：当添加新机器或者删除机器时，不会影响到全部数据的存储，而只是影响到这台机器上所存储的数据(落在这台机器所负责的环上的数据)。 而这种思想，也就是一致性哈希。我们举个例子来感受一下使用一致性哈希的好处： 比如，机器1被移除了，那落在(2,1)范围内的数据全部需要由机器3来存储，也就只影响到落在(2,1)这个范围内的数据。同时，扩容也很方便，比如在(1,3)这段环上再添加一台机器4，只需要将3上的一部分数据拷贝到机器4上即可。 虽然一致性Hash算法解决了节点变化导致的数据迁移问题，但是，我们回过头来再看看数据分布的均匀性： 数据被传统的哈希算法“打散”后，是可以比较均匀分布的。但是引入一致性哈希算法后，为什么就不均匀呢？数据本身的哈希值并未发生变化，变化的是判断数据哈希后应该落到哪个节点的算法变了。 这三个节点Hash后，在环上分布不均匀，导致了每个节点实际占据环上的区间大小不一，数据随机映射到每个节点的概率就有较大差别，换个说法，也就是不能实现较好的负载均衡。 举个例子：机器1的配置很高，性能很好，而机器3的配置很低。但是，如上图，大部分数据由于某些特征都哈希到(1,3)这段环上，直接就导致了机器3的存储压力很大。 虚拟节点的引入当我们将node进行哈希后，这些值并没有均匀地落在环上，因此，最终会导致，这些节点所管辖的范围并不均匀，最终导致了数据分布的不均匀。 为了解决一致性哈希的不足，从而引入了虚拟节点的概念。 引入虚拟节点，可以有效地防止物理节点（机器）映射到哈希环中出现不均匀的情况。比如上图中的机器1、2、3都映射在环的左半边上。 一般，虚拟节点会比物理节点多很多，并可均衡分布在环上，从而提高负载均衡的能力。 如果虚拟节点与物理机器映射得好，某一台物理机器宕机后，其上的数据可由其他若干台物理机器共同分担； 如果新添加一台机器，它可以对应多个不相邻环段上的虚拟节点，从而使得Hash的数据存储得更分散。 如何判定一致性哈希算法的好坏？ 平衡性(Balance)：平衡性是指哈希的结果能够尽可能分布到所有的缓冲（服务器节点）中去，这样可以使得所有的缓冲空间都得到利用，很多哈希算法都能够满足这一条件； 单调性(Monotonicity)：单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲加入到系统中，哈希的结果应能够保证原有已分配的内容可以被映射到原有的或者新的缓冲中去，而不会被映射到旧的缓冲集合中的其他缓冲区； 分散性(Spread)：在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性； 负载(Load)：负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射为不同的内容。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷。 注：对于第四点来说，好的一致性哈希算法，应该将同一用户的请求映射到相同的机器上，因为一个缓冲区始终处理同一个用户的请求是比较容易实现的（Cookie），如果将不同用户的不同请求映射到相同的缓冲区中，既有可能增大机器的负载，而且不容易实现。 总结 熟悉一致性哈希算法的特点及原理； 熟悉一致性哈希算法的优劣与改进（虚拟节点）； 掌握为什么会出现一致性哈希算法； 了解一致性哈希算法的另一种改进（线性空间的引入）—详见一致性哈希算法的理解与实践。 参考阅读一致性哈希算法的理解与实践 一致性哈希算法学习及JAVA代码实现分析 每天进步一点点—五分钟理解一致性哈希算法(consistent hashing) 分布式哈希算法]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>一致性哈希</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络--详解P2P对等网络（一）—BitTorrent协议]]></title>
    <url>%2F2018%2F06%2F21%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-%E8%AF%A6%E8%A7%A3P2P%E5%AF%B9%E7%AD%89%E7%BD%91%E7%BB%9C%EF%BC%88%E4%B8%80%EF%BC%89%E2%80%94BitTorrent%E5%8D%8F%E8%AE%AE%2F</url>
    <content type="text"><![CDATA[注：本篇文章内容借鉴自：浅入浅出BitTorrent协议，其中加入了博主的部分原创，感谢，侵删~ 对等网络的概念以下定义主要摘抄自维基百科与中科院计算技术研究所： 对等式网络（peer-to-peer，简称P2P），又称点对点技术，是无中心服务器、依靠用户群（peers）交换信息的互联网体系。它的作用在于，网络的参与者共享他们所拥有的一部分硬件资源（处理能力、存储能力、网络连接能力、打印机等），这些共享资源通过网络提供服务和内容，能被其它对等节点(Peer)直接访问而无需经过中间实体，减低以往网路传输中的节点，以降低资料遗失的风险。与有中心服务器的中央网络系统不同，对等网络中的每个用户端既是资源（服务和内容）提供者（Server），又是资源获取者（Client），任何一个节点无法直接找到其他节点，必须依靠其用户群进行信息交流。 对等网络的模型P2P网络的简易模型图如下： 如图，我们可以总结出P2P网络的一些特征： 非中心化：P2P是全分布式系统，网络中的资源和服务分散在所有的节点上，信息的传输和服务的实现都直接在节点之间进行，可以无需中间环节和服务器介入。 可扩展性：用户可以随时加入该网络，系统的资源和服务能力随之同步扩充。理论上其可扩展性几乎可以认为是无限的。 健壮性：因为服务是分散在各个节点之间的，部分节点或网络遭到破坏对其他部分的影响很小，故P2P具有耐攻击、高容错的特点。P2P网络一般在部分结点失效时能够自动调整整体拓扑，保持其它结点的连通性。P2P网络通常都是以自组织的方式建立起来的，并允许结点自由地加入和离开。 高性价比：P2P架构可以有效地利用互联网中散布的大量普通结点，将计算任务或存储资料分布到所有结点上。利用其中闲置的计算能力或存储空间，达到高性能计算和海量存储的目的。 隐私保护：在P2P网络中，由于信息的传输分散在各节点之间进行而无需经过某个集中环节，用户的隐私信息被窃听和泄漏的可能性大大缩小。 负载均衡：由于每个节点既是服务器又是客户端，减少了传统C/S模型中对服务器计算能力、存储的要求，同时因为资源分布在多个节点，更好的实现了整个网络的负载均衡。 BitTorrent协议BitTorrent协议，简称BT协议，是一种互联网上新兴的P2P传输协议（即BT是一种特殊的P2P）。 出现的历史原因随着全球接入互联网人数的增长，对互联网应用的服务端要求越来越高，吞吐量和并发量逐步加大。像国外的Google、Amazon，国内的阿里、百度等大型互联网企业十分关注如何使用集群、负载均衡等技术来提高网站的并发和吞吐量。但对于保存大文件给用户下载的中小型网站，在互联网爆发式增长下，用户数量越来越多，传输的文件越来越大，使用一台或几台服务器和传统的FTP、HTTP协议难以满足用户对下载速度的需求（受限于庞大的用户请求量、硬盘读取速度、带宽等）。BitTorrent协议解决了这些问题。 BT下载流程 无论是BT还是PT，还是FTP，都是一种下载方式，或者学术点的讲法，叫文件传输。 BT下载通过一个P2P下载软件（点对点下载软件）来实现，克服了传统下载方式的局限性，具有下载的人越多，文件下载速度就越快的特点。其好处是不需要资源发布者拥有高性能服务器就能迅速有效地把发布的资源传向其他的BT客户软件使用者，而且大多数的BT软件都是免费的。 BT下载架构模型图： 现在，我们已经清楚了BT下载的架构模型，对于BT下载流程的讨论，我们跟随BT协议所引出的三个问题来进行： 一个对等用户如何找到具有自己想下载内容的其他对等用户？ 对等用户们如何复制内容以便为大家提供高速下载？ 对等用户们如何相互鼓励上传内容给他人同时为自己下载内容？ B encode在解决这三个问题之前，先简单介绍下B encode，因为在BitTorrent协议中的数据几乎都是用B encode进行编码的。它是一种作用类似于XML和JSON的数据组织格式，可以表达字符串、整数两种基本类型，列表、字典两种数据结构，它的语法规则十分简单。 字节串按如下方式编码： &lt;以十进制ASCII编码的串长度&gt;:&lt;串数据&gt;例：“4:spam”表示字节串“spam” 整数按如下方式编码： i&lt;以十进制ASCII编码的整数&gt;e例：“i3e”表示整数“3” 列表按如下方式编码： l&lt;内容&gt;e开始的“l”与结尾的“e”分别是开始和结束分隔符。lists可以包含任何B编码的类型，包括整数、串、dictionaries和其他的lists。例：l4:spam4:eggse 表示含有两个串的lists:[“spam”、“eggs”] 字典按如下方式编码： d&lt;内容&gt;e开始的“d”与结尾的“e”分别是开始和结束分隔符。注意键（key）必须被B编码为串。值可以是任何B编码的类型，包括整数、串、lists和其他的dictionaries。键（key）必须是串，并且以排序的顺序出现（以原始串排列，而不是以字母数字顺序）。例：d3:cow3:moo4:spam4:eggse表示dictionary { “cow” =&gt; “moo”, “spam” =&gt; “eggs” } torrent文件与tracker服务器现在，我们考虑上述中的第一个问题：一个对等用户如何找到具有自己想下载内容的其他对等用户？ 在P2P网络系统的最初阶段，并不是所有的用户都拥有所有的内容，首先需要一个内容的发布者。作为内容的发布者，BitTorrent可以为每个发布者（内容提供商）创建一个内容描述文件，即.torrent文件，也就是我们平常所说的种子文件。 种子文件是一种指定格式的文件，包含了两类关键信息，一类信息是tracker服务器的URL，一类信息是一个大小相等的块的清单。tracker服务器可以将用户引导至种子文件的内容，也就是将用户引导至拥有当前用户所需下载内容的其他对等节点处，块清单则组成了内容。 关于种子文件具体的组成结构，它使用B encode表示，整个是一个字典数据结构，它有多个key值，包括一些是可选的，这里介绍最关键的上述两类信息的键值对： info：存储资源文件的元信息 piece length pieces name/path announce：描述tracker服务器的URL info：info键对应的值又是一个字典结构，BT协议将一个文件分成若干块，便于客户端从各个主机下载各个块。其中的piece length键值对表示一个块的长度，通畅情况下是2的n次方，根据文件大小有所权衡，通长越大的文件piece length越大以减少piece的数量，降低piece数量一方面降低了种子文件保存块信息数目的大小，一方面也减少了下载时需要对块做的确认操作，加快下载速度。目前块的大小通常是256KB，512KB或者1MB。 pieces：表示每个块的正确性验证信息，每一块均对应一个唯一的SHA1散列值。该键对应的值是所有块的SHA1散列值（每个块所对应的散列值大小为20字节）连接而成的字符串。 name/path：表示具体文件的信息。因为BitTorrent协议允许将数个文件和文件夹作为一个BT下载进行发布，因此下载方可以根据需要勾选某一些下载文件。注意，这里将数个文件也砍成一个数据流，因此一个piece如果在文件边界上，可能包含不同文件的信息。 announce：保存的是tracker服务器的URL，在一些扩展协议中，announce可以保存多个tracker服务器作为备选。 生成好种子文件之后，为了下载由种子文件所描述的内容，一个对等用户首先需要和此种子文件取得联系。这个种子文件通常被放在服务器上，可以通过HTTP或者FTP协议供用户下载这个种子文件。相比于直接将整个资源文件提供给用户下载，只传输一个种子文件大大降低了服务器的负荷。 在取得种子文件后，我们可以通过BitTorrent协议提供的一些工具（BitTorrent软件客户端）来打开这个种子文件，客户端会根据种子文件的name/path元信息告诉我们这个种子文件可以下载到（假设）一个.mkv文件，一个字幕文件，在这个阶段我可以进行一些勾选，选择下载某些而不是全部的资源。 资源选择确定后，客户端就开始了下载。客户端的第一步任务根据种子文件上的tracker服务器的URL使用HTTP进行GET请求——tracker服务器维护着一个正在主动上传和下载该内容的所有其他对等用户列表，我们将这一组对等用户称为用户群，对等用户可以随时离开群（以及返回），只要他们及时向tracker服务器进行报告即可——这个请求包含了很多参数，这里只介绍从客户端发送到tracker的请求中最关键的几个参数。 info_hash peer_id ip port info_hash：种子文件中info键所对应的值的SHA1散列，可以被tracker服务器用来索引唯一的对应资源。 peer_id：20Byte的串，没有任何要求，被tracker服务器用于记录客户端的名字。 ip：可以从HTTP GET请求中直接获取，放在参数中可以解决使用代理进行HTTP GET的情况，tracker服务器可以记录客户端的IP地址。 port：客户端监听的端口号，用于接收Response。一般情况下为BitTorrent协议保留的端口号：6881-6889，tracker服务器会记录下端口号用于通知其他客户端。 在tracker服务器收到客户端的HTTP GET请求后，会返回B encode形式的text/plain文本，同样是一个字典数据结构，其中最关键的一个键值对是peers，它的值是个字典列表结构，列表中的每一项都是如下的字典结构： peers peer_id ip port 这些信息在每个客户端连接tracker服务器的时候都发送过，并且被Tracker服务器保存了下来。新来的客户端自然要获取到这些下载中或者已下载完的客户端的ip，port等信息，有了这些信息，客户端就不需要像FTP或者HTTP协议一样持续找服务器获取资源，可以从这些其他客户端上请求获取资源。 peer to peer如上所述，如果对第一个问题进行简单的总结，那么自己想要下载的内容就交由种子文件进行确定，而如何寻找拥有这些资源的其他对等用户，则需要tracker服务器的帮助。 那么，我们现在思考第二个问题：对等用户们如何复制内容以便为大家提供高速下载？ 客户端从tracker服务器获取到若干其他下载者(peer)的ip和port信息，会进行请求并维持跟每一个peer的连接状态。一个客户端和每一个peer的状态主要有下列状态信息： choke：远程客户端拒绝响应本客户端的任何请求。 interested：远程客户端对本客户端的数据感兴趣，当本客户端unchoked远程客户端后，远程客户端会请求数据。 所以应该有4个参数，分别表示本客户端对远程客户端是否choke，是否interested，远程客户端对本客户端是否choke，是否interested。当一个客户端对一个远程peer感兴趣并且那个远程peer没有choke这个客户端，那么这个客户端就可以从远程peer下载块(block)。当一个客户端没有choke一个peer，并且那个peer对这个客户端感兴趣时，这个客户端就会上传块(block)。 补充一点：由于内容发布者本来就拥有所有的块，因此在生成种子文件的时候，内容发布者首先会根据种子文件将内容重新下载一遍，在这个过程中就会联系tracker服务器，然后将自己的信息记录在其所维护的用户列表上。 其实上述所讲的概念已经解答了第二个问题，每一个对等节点在参与一个用户群期间，都可同时从其他对等节点处下载缺少的块，并给其他对等节点上传本身拥有并且他们所需要的块。如果每个对等节点都这样做，那么经过短暂的一段时间后，所有的块都将成为广泛可用——加入用户群的用户越多，块被交易的越频繁，下载速度越快。 对第二个问题的答案进行一个简单的总结：对等用户们通过在下载块的同时也会上传块，从而为大家提供高速下载。 我们再进行一些补充，上面也提到了，在交易块的过程中，端与端之间会进行通信： 首先会发送握手报文，告诉远程客户端本客户端的一些信息，包括info_hash和peer_id。 接下来的所有报文有如下几种类型： keep-alive：告诉远程客户端这个通信还在维持，否则超过2分钟没有任何报文远程客户端会将通信关闭 choke unchoke interested not interested bitfield：告诉对方我已经有的piece have：告诉对方某个piece已经成功下载并且通过hash校验 request：请求某个块(block) index: 整数，指定从零开始的piece索引 begin: 整数，指定piece中从零开始的字节偏移 length: 整数，指定请求的长度 piece：返回请求的块(block)的数据，是真正的资源信息 index: 整数，指定从零开始的piece索引 begin: 整数，指定piece中从零开始的字节偏移 block: 数据块 经过这些报文在本地客户端和若干个远程客户端之间的来回传递，就能够获取到资源文件。 PT下载在解决了前两个问题之后，我们来考虑第三个问题：对等用户们如何相互鼓励上传内容给他人同时为自己下载内容？ 我们也可以看到，在P2P网络中，起初必须有一个内容的提供者，并且在健康的P2P网络中，每个peer都应当同时扮演客户端与服务器两个角色，那些只想从系统中获取资源而没有实物贡献的节点我们称之为“搭便车”或“吸血鬼”，如果这样的用户太多，那系统将无法正常工作。 PT全称Private Tracker，与BT最大的不同点分别为可进行私密范围下载，及可统计每个用户的上传及下载量。从技术上可以简单的看作有一个tracker服务器会对用户的下载上传进行统计，分享率不够就禁止用户下载，在一定程度上可以防止只下载而不上传的用户存在。关于PT下载更详细的内容，博主不在这里进行讨论，有兴趣的同学可以自行查阅相关资料。 一个有趣的小问题迅雷作为国内首屈一指的BT下载工具，为什么有时在下载接近完成的最后，一些数据总是传输的非常慢呢？ 基于现实情况分析，有些人下载完成后关掉下载任务，提供较少量数据给其他用户，为尽量避免这种行为，在非官方BitTorrent协议中存在超级种子的算法。这种算法允许文件发布者分几步发布文件，发布者不需要一次提供文件所有内容，而是慢慢开放下载内容的比例，延长下载时间。此时，速度快的人由于未下载完必须提供给他人数据，速度慢的人有更多机会得到数据。由此往往造成用户卡在任务的99%，下载1G的任务要上传3G之多的数据。 总结 熟悉P2P网络的特点及优势； 熟悉BitTorrent协议的基本内容； 熟悉BT下载的过程（种子文件以及tracker服务器）； 掌握诸如用户群、吸血鬼等名词含义； 了解种子文件的组成格式——B编码； 了解peer与peer之间的通信报文格式； 能独立回答出关于BitTorrent协议的三个问题。 参考阅读计算机网络（第五版） — Andrew S. TanenBaum/David J. Wetherall 浅入浅出BitTorrent协议 P2P（对等网络）、PT下载与BT下载]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>P2P</tag>
        <tag>BitTorrent协议</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络--详解CDN]]></title>
    <url>%2F2018%2F06%2F18%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-%E8%AF%A6%E8%A7%A3CDN%2F</url>
    <content type="text"><![CDATA[注：本文内容摘抄自CDN详解 — 不挑食的程序员，博主在其基础上进行了少量修改并加入了部分原创，感谢，侵删~ 之前我们已经学习过缓存机制以及Web代理，但对于大型网站的构建，这些还远远不够。要想真正的构建一个流行的网站，内容分发网络（Content Delivery Network）简称CDN，是一项不可或缺的技术。 什么是CDN？以下内容摘自维基百科： 内容分发网络（CDN）是指一种通过互联网互相连接的计算机网络系统，利用最靠近每位用户的服务器，更快、更可靠地将音乐、图片、视频、应用程序及其他文件发送给用户。 如果对整个CDN系统做一个简单的描述： CDN系统主要由4大部分组成，每部分都由集群所构成。这4部分分别由CDN专属DNS服务器、全局负载均衡设备、区域负载均衡设备、CDN缓存服务器（边缘节点）构成。除过CDN专属DNS服务器，其他3部分集群都有源服务器上对应资源的全部或部分副本。CDN系统通过各部分的负载均衡算法，最终指示客户端使用附近最优的边缘节点中的一台缓存服务器作为服务端，从而提高Web应用的性能。 CDN的基本工作过程在传统的Web模型中，发出请求后一般要经过如下几个步骤： 用户在自己的浏览器中输入要访问的网站域名。 浏览器向本地DNS服务器请求对该域名的解析。 本地DNS服务器中如果缓存有这个域名的解析结果，则直接响应用户的解析请求。 本地DNS服务器中如果没有关于这个域名的解析结果的缓存，则以迭代方式向整个DNS系统请求解析，获得应答后将结果反馈给浏览器。 浏览器得到域名解析结果，就是该域名相应的服务设备的IP地址 。 浏览器获取IP地址之后，经过标准的TCP握手流程，建立TCP连接。 浏览器向服务器发起HTTP请求。 服务器将用户请求内容传送给浏览器。 经过标准的TCP挥手流程，断开TCP连接。 在网站和用户之间加入CDN以后，用户不会有任何与原来不同的感觉。从宏观上来看，一个典型的CDN用户访问调度流程如下： 当用户点击网站页面上的内容URL，先经过本地DNS系统解析，如果本地DNS服务器没有相应域名的缓存，则本地DNS系统会将域名的解析权交给CNAME指向的CDN专用DNS服务器。 CDN的DNS服务器将CDN的 全局负载均衡设备 IP地址返回给用户。 用户向CDN的全局负载均衡设备发起URL访问请求。 CDN全局负载均衡设备根据用户IP地址，以及用户请求的URL，选择一台用户所属区域的区域负载均衡设备，并将请求转发到此设备上。 基于以下这些条件的综合分析之后，区域负载均衡设备会选择一个最优的缓存服务器节点，并从缓存服务器节点处得到缓存服务器的IP地址，最终将得到的IP地址返回给全局负载均衡设备： 根据用户IP地址，判断哪一个边缘节点距用户最近； 根据用户所请求的URL中携带的内容名称，判断哪一个边缘节点上有用户所需内容； 查询各个边缘节点当前的负载情况，判断哪一个边缘节点尚有服务能力。 全局负载均衡设备把服务器的IP地址返回给用户。 用户向缓存服务器发起请求，缓存服务器响应用户请求，将用户所需内容传送到用户终端。如果这台缓存服务器上并没有用户想要的内容，而区域均衡设备依然将它分配给了用户，那么这台服务器就要向它的上一级缓存服务器请求内容，直至追溯到网站的源服务器将内容拉到本地。 CDN全局负载均衡设备与CDN区域负载均衡设备根据用户IP地址，将域名解析成相应节点中缓存服务器的IP地址，实现用户就近访问，从而提高服务端响应内容的速度。 理论上，最简单的CDN网络只有一个CDN专用DNS服务器，一个全局负载均衡设备，然后各节点一台缓存服务器，即可运行。 CDN的部署架构CDN系统设计的首要目标是尽量减少用户的访问响应时间，为达到这一目标，CDN系统应该尽量将用户所需要的内容存放在距离用户最近的位置。也就是说，负责为用户提供内容服务的缓存设备应部署在物理上的网络边缘位置，我们称这一层为CDN边缘层。CDN系统中负责全局性管理和控制的设备组成中心层，中心层同时保存着基本上最完善的内容副本，当边缘层设备未命中时，会向中心层请求，如果在中心层仍未命中，则需要中心层向源站回源。 不同CDN系统设计之间存在差异，中心层可能具备用户服务能力，也可能不直接提供服务，只向下级节点提供内容。如果CDN网络规模较大，边缘层设备直接向中心层请求内容或服务会造成中心层设备压力过大，就要考虑在边缘层和中心层之间部署一个区域层，负责一个区域的管理和控制，也保存部分内容副本供边缘层访问。 实际上，边缘层、区域层、中心层分别对应于边缘节点、区域负载均衡设备、全局负载均衡设备。 节点是CDN系统中最基本的部署单元，每个节点都是由服务器集群组成。一个CDN系统由大量的、地理位置上分散的POP（point-of-presence）节点组成，为用户提供就近的内容访问服务。 CDN节点网络主要包含CDN骨干点和POP点。CDN骨干点和CDN POP点在功能上不同。 中心和区域节点一般称为骨干点，主要作为内容分发和边缘未命中时的服务点； 边缘节点又被称为POP（point-of-presence）节点，CDN POP点主要作为直接向用户提供服务的节点。 从节点构成上来说（微观上），CDN骨干点、POP点都由缓存设备和本地负载均衡设备构成，它与全局负载均衡设备及区域负载均衡设备属于不同视角下的东西，可以分开实现，也可以抽象出接口，对上层透明。 缓存设备和本地负载均衡设备的连接方式有两种：一种是旁路方式，一种是穿越方式。我们只说穿越方式。 在穿越方式下，SLB（Server Load Balancer）本地负载均衡一般由L4-7交换机实现，SLB向外提供可访问的公网IP地址，我们可以将其称之为VIP。每台缓存服务器仅分配私网IP地址，该台SLB下的所有缓存服务器构成一个服务组。所有用户请求和媒体流都经过该SLB设备，再由SLB设备进行向上向下转发。SLB实际上承担了NAT（Network Address Translation，网络地址转换）功能，向用户屏蔽了单台缓存服务器设备的IP地址。 也就是说，我们所访问的全局负载均衡设备，区域负载均衡设备，CDN边缘节点服务器的IP地址，实际上都是VIP。这种方式是CDN系统中应用较多的方式，优点是具有较高的安全性和可靠性。 CDN的功能架构CDN基于这样的原理： 挑选最优设备为用户提供服务； 如果某个内容被很多用户所需要，它就被缓存到距离用户最近的节点中。 CDN公司在整个互联网上部署数以百计的CDN服务器（Cache），这些服务器通常在运营商的IDC(互联网数据中心Internet Data Center）中，尽量靠近接入网络和用户。当内容的提供者更新内容时，CDN向缓存服务器重新分发这些被刷新的内容。CDN提供一种机制，当用户请求内容时，该内容能够由以最快速度交付的缓存服务器来向用户提供，这个挑选”最优”的过程就叫做负载均衡。被选中的最优缓存服务器可能最靠近用户，或者有一条与用户之间条件最好的路径。 关于国内有名的CDN公司，我们熟知的包括阿里云、腾讯云、百度云等等，他们都对外提供CDN服务。 至于IDC，请戳：云、CDN、IDC 三个概念的区别是什么？有什么相互包含和影响 - DADAman的回答 - 知乎 从功能上划分，典型的CDN系统架构由分发服务系统、负载均衡系统和运营管理系统三大部分组成。 分发服务系统该系统的主要作用是实现将内容从内容源中心向边缘的推送和存储，承担实际的内容数据流的全网分发工作和面向最终用户的数据请求服务。分发服务系统最基本的工作单元就是许许多多的缓存服务器，缓存服务器负责直接响应最终用户的访问请求，把缓存在本地的内容快速地提供给用户。同时缓存服务器还负责与源站点进行内容同步，把更新的内容以及本地没有的内容从源站点获取并保存在本地。 一般来说，根据承载内容类型和服务种类的不同，分发服务系统会分为多个子服务系统，如网页加速子系统、流媒体加速子系统、应用加速子系统等。每个子服务系统都是一个分布式服务集群，由一群功能近似的、在地理位置上分布部署的缓存服务器或缓存服务器集群组成，彼此间相互独立。每个子服务系统设备集群的数量根据业务发展和市场需要的不同，少则几十台，多则可达上万台，对外形成一个整体，共同承担分发服务工作。缓存服务器设备的数量、规模、总服务能力是衡量一个CDN系统服务能力的最基本的指标。 分发服务系统在承担内容的更新、同步和响应用户需求的同时，还需要向上层的调度控制系统提供每个缓存服务器的健康状况信息、响应情况，有时还需要提供内容分布信息，以便调度控制系统根据设定的策略决定由哪个缓存服务器（组）来响应用户的请求最优。 负载均衡系统负载均衡系统是一个CDN系统的神经中枢，主要功能是负责对所有发起服务请求的用户进行访问调度，确定提供给用户的最终实际访问地址。大多数CDN系统的负载均衡系统是分级实现的，这里以最基本的两级调度体系进行简要说明。一般而言，两级调度体系分为全局负载均衡（GSLB）和本地负载均衡（SLB）。 我们刚说的全局负载均衡设备及区域负载均衡设备都属于全局负载均衡，本地负载均衡设备则属于本地负载均衡。 其中，全局负载均衡（GSLB）主要根据用户就近性原则，通过对每个服务节点进行”最优”判断，确定向用户提供服务的缓存服务器集群的物理位置。最通用的GSLB实现方法是基于DNS解析的方式实现，也有一些系统采用了应用层重定向等方式来解决。本地负载均衡（SLB）主要负责节点内部的设备负载均衡，当用户请求从 GSLB调度到SLB时，SLB会根据节点内各缓存服务器设备的实际能力或内容分布等因素对用户进行重定向，常用的本地负载均衡方法有基于4层调度、基于7层调度、链路负载调度等。 运营管理系统CDN的运营管理系统与一般的电信运营管理系统类似，分为运营管理和网络管理两个子系统。 运营管理子系统是CDN系统的业务管理功能实体，负责处理业务层面的与外界系统交互所必需的一些收集、整理、交付工作，包含客户管理、产品管理、计费管理、统计分析等功能。 网络管理子系统实现对CDN系统的网络设备管理、拓扑管理、链路监控和故障管理，为管理员提供对全网资源进行集中化管理操作的界面，通常是基于Web方式实现的。 在CDN系统中，不仅分发服务系统和调度控制系统是分布式部署的，运营管理系统也是分布式部署的，每个节点都是运营管理数据的生成点和采集点，通过日志和网管代理等方式上报数据。可以说，CDN本身就是一个大型的具有中央控制能力的分布式服务系统。 试着使用一下腾讯云提供的CDN服务吧，或许会对你理解运营管理系统有帮助。 为什么需要CDN？当下的互联网应用都包含大量的静态内容，但静态内容以及一些准动态内容又是最耗费带宽的，特别是针对全国甚至全世界的大型网站，如果这些请求都指向主站的服务器的话，不仅是主站服务器受不了，单端口500M左右的带宽也扛不住，所以大多数网站都需要CDN服务。 根本上的原因是，访问速度对互联网应用的用户体验、口碑、甚至说直接的营收都有巨大的影响，任何的企业都渴望自己站点有更快的访问速度。而HTTP传输时延对web的访问速度的影响很大，在绝大多数情况下是起决定性作用的，这是由TCP/IP协议的一些特点决定的。物理层上的原因是光速有限、信道有限，协议上的原因有丢包、慢启动、拥塞控制等。 这就是你使用CDN的第一个也是最重要的原因：为了加速网站的访问。 除了加速网站的访问之外，CDN还有一些作用： 1. 实现跨运营商、跨地域的全网覆盖 互联不互通、区域ISP地域局限、出口带宽受限制等种种因素都造成了网站的区域性无法访问。CDN加速可以覆盖全球的线路，通过和运营商合作，部署IDC资源，在全国骨干节点合理部署CDN边缘分发存储节点，充分利用带宽资源，平衡源站流量。阿里云在国内有500+节点，海外300+节点，覆盖主流国家和地区不是问题，可以确保CDN服务的稳定和快速。 2. 保障你的网站安全 CDN的负载均衡和分布式存储技术，可以加强网站的可靠性，相当无无形中给你的网站添加了一把保护伞，应对绝大部分的互联网攻击事件。防攻击系统也能避免网站遭到恶意攻击。 3. 异地备援 当某个服务器发生意外故障时，系统将会调用其他临近的健康服务器节点进行服务，进而提供接近100%的可靠性，这就让你的网站可以做到永不宕机。 4. 节约成本 投入使用CDN加速可以实现网站的全国铺设，你根据不用考虑购买服务器与后续的托管运维，服务器之间镜像同步，也不用为了管理维护技术人员而烦恼，节省了人力、精力和财力。 5. 让你更专注业务本身 CDN加速厂商一般都会提供一站式服务，业务不仅限于CDN，还有配套的云存储、大数据服务、视频云服务等，而且一般会提供7x24运维监控支持，保证网络随时畅通，你可以放心使用。并且将更多的精力投入到发展自身的核心业务之上。 其它流量劫持其实，CDN本身就是一种DNS劫持，只不过是良性的。不同于黑客强制DNS把域名解析到自己的钓鱼IP上，CDN则是让DNS主动配合，把域名解析到临近的服务器上。 劫持通常分为两类： 域名劫持，又称DNS劫持，通常是指域名指向到非正常IP（恶意IP），该恶意IP通过反向代理的方式，在能返回网页正常内容的情况，可能插入恶意代码、监听网民访问、劫持敏感信息等操作。通常验证一个域名是否被劫持的方法是PING一个域名，如果发现PING出来的IP不是您的服务器真实IP，则可以确定被劫持了。 数据劫持，通常由电信运营商中某些员工等勾结犯罪分子，在公网中进行数据支持，插入，此类情况极隐蔽，不会改变用户域名解析IP，而是直接数据流经运营商宽带时在网页中挺入内容，此类情况，建议网页启用HTTPS加密，可以解决这一问题（通信是加密的，运营商无法插入恶意内容）。 如果使用CDN服务，当源站向CDN返回被劫持的内容时，此时CDN将获取不到正确的网页内容（而是经运营商篡改强制植入广告的页面），此时可能导致该内容在CDN中长时间缓存，发现这种问题，可以清理CDN缓存，一般即可恢复正常。 CDN缓存CDN边缘节点缓存策略因服务商不同而不同，但一般都会遵循HTTP标准协议，通过HTTP缓存机制来设置CDN边缘节点数据缓存时间。 当客户端向CDN节点请求数据时，CDN节点会判断缓存数据是否过期，若缓存数据并没有过期，则直接将缓存数据返回给客户端；否则，CDN节点就会向源站发出回源请求（back to the source request），从源站拉取最新数据，更新本地缓存，并将最新数据返回给客户端。 CDN服务商一般会提供基于文件后缀、目录多个维度来指定CDN缓存时间，为用户提供更精细化的缓存管理。 CDN缓存时间会对回源率产生直接的影响。若CDN缓存时间较短，CDN边缘节点上的数据会经常失效，导致频繁回源，增加了源站的负载，同时也增大了访问延时；若CDN缓存时间太长，会带来数据更新慢的问题。开发者需要增对特定的业务，来做特定的数据缓存时间管理。 CDN边缘节点对开发者是透明的，相比于浏览器Ctrl+F5的强制刷新来使浏览器本地缓存失效，开发者可以通过CDN服务商提供的“刷新缓存”接口来达到清理CDN边缘节点缓存的目的。这样开发者在更新数据后，可以使用刷新缓存功能来强制CDN节点上的缓存过期，保证客户端在访问时，拉取到最新的数据。 总结 熟悉CDN系统的请求与响应流程； 熟悉CDN系统的部署架构及功能架构； 掌握CDN系统中全局负载均衡与本地负载均衡的相关概念； 掌握CDN系统中全局负载均衡设备、区域负载均衡设备、本地负载均衡设备间各个的功能及相关概念； 了解使用CDN的好处； 了解流量劫持与CDN缓存。 参考阅读如何自己架设部署CDN — 视界云]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>CDN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机组成原理--64位CPU装载32位操作系统，它的寻址能力还是4GB吗？]]></title>
    <url>%2F2018%2F05%2F25%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BB%84%E6%88%90%E5%8E%9F%E7%90%86-64%E4%BD%8DCPU%E8%A3%85%E8%BD%BD32%E4%BD%8D%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%EF%BC%8C%E5%AE%83%E7%9A%84%E5%AF%BB%E5%9D%80%E8%83%BD%E5%8A%9B%E8%BF%98%E6%98%AF4GB%E5%90%97%EF%BC%9F%2F</url>
    <content type="text"><![CDATA[借由这个问题，今天我们就把 32 位 CPU、64 位 CPU、32 位操作系统、64 位操作系统之间的区别与联系彻底搞清楚。对于这个问题，博主也是一知半解了好长时间啊~ 基本概念32位的CPU与64位CPU以下内容摘自维基百科： 64 位 CPU 是指 CPU 内部的通用寄存器的宽度为 64 比特，支持整数的 64 比特宽度的算术与逻辑运算。那么 32 位 CPU 同理。 一个 CPU，联系外部的数据总线与地址总线，可能有不同的宽度；术语“64位”也常用于描述这些总线的大小。不过这一术语也可能指电脑指令集的指令长度，或其它的数据项。去掉进一步的条件，“64位”电脑架构一般具有 64 位宽的整数型寄存器，它可支持 64 位“区块”的整数型数据。 64 位架构无疑可应用在需要处理大量数据的应用程序，如数字视频、科学运算、和早期的大型数据库。 那么 32 位 CPU 与 64 位 CPU 到底有什么区别？ 数据处理能力增强：64 位 CPU 通用寄存器的位宽增加一倍，这也就意味着 64 位 CPU 可以一次性处理 64bit 的整形数据； 内存寻址能力增强：如果是 32 位 CPU 的话，它的地址总线最多不会超过 32，那么它所能达到的寻址范围也就不会超过 2 的 32 次方字节（存储单元以字节为单位），也就是 4GB，而如果是 64 位处理器的话，它所能达到的寻址范围理论上就会是 2 的 64 次方字节（上亿 GB）。 补充：一般处理器多少位是指通用寄存器的长度，当然数据线需要与之相同；地址线则不需要与之相等，好比 intel 64 位处理器则是 40 位地址总线，最大支持 1TB 的内存寻址。 32位操作系统与64位操作系统平时我们所说的 32 位操作系统也被称为 X86 系统，x64 代表 64 位操作系统，关于它的简略解释：为什么32位的计算机系统不叫x32而叫x86呢？ 关于 32 位操作系统与 64 位操作系统的区别如下： 32 位操作系统既可以运行在 32 位的 CPU 上，也可以运行在 64 位的 CPU 上，只不过，运行在 64 位 CPU 上的话，就有点“大马拉小车”的感觉了——无法发挥出 CPU 的全部能力。 64 位操作系统只能运行在 64 位的 CPU 上，因此如果一个操作系统是 64 位，那么它的 CPU 架构也必定是 64 位。 64 位操作系统相比于 32 位操作系统的优势正如上述 64 位 CPU 与 32 位 CPU 的优势一样。 总线结构与主板构成注：以下两部分内容摘抄自：32位系统最大只能支持4GB内存之由来~ 在解决本文标题所述的问题之前，我们再来了解一些关于总线结构与主板构成的相关知识。 说起总线大家肯定不陌生，而且大家平时肯定跟它打过交道，我们在用U盘拷贝数据的时候先要把U盘通过USB接口与电脑相连才能拷贝。USB接口实际上就是一种总线，一般称这种总线为USB总线（也叫做通用串行总线）。在很久之前是没有USB总线的，那个时候每个外设各自采用自己的接口标准，举个最简单的例子：鼠标生产厂商采用鼠标特有的接口，键盘生产厂商用键盘特有的接口，这样一来的话，PC机上就必须提供很多接口，这样一来增加了硬件设计难度和成本，直到后来USB接口的出现，它统一了很多外设接口的标准，不仅使得用户可以很方便地连接一些外设，更增强了PC的可扩展性。所以现在大家看到的鼠标、键盘、U盘、打印机等等这些外设都可以直接通过USB接口直接插到电脑上的。 在计算机系统中总线是非常重要的一个概念，正是因为有了总线，所有的组成部件才能一起正常协同分工合作。在很久以前的PC机中，采用的是三总线结构，即：数据总线、地址总线、控制总线。它们分别用来传输不同类型的数据，数据总线用来传输数据，地址总线用来传输地址，控制总线用来传输一些控制信号。 随着时代的发展，这种简单的总线结构逐渐被淘汰。下面这幅图是现代计算机采用的结构： 事实上这也是现代主板所采用的结构，当然可能部分地方有略微不同（大体结构是差不多的），仔细观察过主板构成的朋友可能对上面一幅图很熟悉。在主板上主要有两大主要部分：北桥（North Bridge也称Host Bridge）和南桥（South Bridge）。北桥主要负责CPU和内存、显卡这些部件的数据传送，而南桥主要负责I/O设备、外部存储设备以及BIOS之间的通信。现在有些主板已经没有北桥了，因为芯片厂商已经把北桥所负责的功能直接集成到CPU中了（不过暂且我们以上副图的模型来讨论）。 在上副图中，我没有画出数据总线和地址总线等，因为在某些总线标准中它们被集成到一起了，比如在PCI总线中，地址总线和数据总线总是分时复用的（也就是说假如PCI总线有32位数据总线，这32位总线在某个时刻可以充当数据总线的作用，在下一时刻可以充当地址总线的作用）。有的总线同时提供了数据总线和地址总线。 下面来说一下几个主要总线和南北桥的作用： FSB总线：即前端总线（Front Side Bus），CPU和北桥之间的桥梁，CPU和北桥传递的所有数据必须经过FSB总线，可以这么说，FSB总线的频率直接影响到CPU访问内存的速度。 北桥：北桥是CPU和内存、显卡等部件进行数据交换的唯一桥梁，也就是说CPU想和其他任何部分通信必须经过北桥。北桥芯片中通常集成的还有内存控制器等，用来控制与内存的通信。现在的主板上已经看不到北桥了，它的功能已经被集成到CPU当中了。 PCI总线：PCI总线是一种高性能局部总线，其不受CPU限制，构成了CPU和外设之间的高速通道。比如现在的显卡一般都是用的PCI插槽，PCI总线传输速度快，能够很好地让显卡和CPU进行数据交换。 南桥：主要负责I/O设备之间的通信，CPU要想访问外设必须经过南桥芯片。 在了解了这些基础东西之后，下面来讲解一下为何 32 位系统最大只支持 4GB 内存。（是的，就算是 64 位的 CPU，装载 32 位的操作系统，它的寻址能力还是 4GB。） 对于标题的解释在使用计算机时，其最大支持的内存是由操作系统和硬件两方面决定的。 先说一下硬件方面的因素，在上面已经提到了地址总线，在计算机中 CPU的地址总线数目 决定了CPU 的 寻址 范围，这种由地址总线对应的地址称作为物理地址。假如CPU有32根地址总线（一般情况下32位的CPU的地址总线是32位，也有部分32位的CPU地址总线是36位的，比如用做服务器的CPU），那么提供的可寻址物理地址范围 为 232=4GB（在这里要注意一点，我们平常所说的32位CPU和64位CPU指的是CPU一次能够处理的数据宽度，即位宽，不是地址总线的数目）。自从64位CPU出现之后，一次便能够处理64位的数据了，其地址总线一般采用的是36位或者40位（即CPU能够寻址的物理地址空间为64GB或者1T）。在CPU访问其它任何部件的时候，都需要一个地址，就像一个快递员送快递，没有地址他是不知道往哪里送达的，举个例子，CPU想从显存单元读取数据，必须知道要读取的显存单元的实际物理地址才能实现读取操作，同样地，从内存条上的内存单元读取数据也需要知道内存单元的物理地址。换句话说，CPU访问任何存储单元必须知道其物理地址。 用户在使用计算机时能够访问的最大内存不单是由CPU地址总线的位数决定的，还需要考虑操作系统的实现。实际上用户在使用计算机时，进程所访问到的地址是逻辑地址，并不是真实的物理地址，这个逻辑地址是操作系统提供的，CPU在执行指令时需要先将指令的逻辑地址变换为物理地址才能对相应的存储单元进行数据的读取或者写入（注意逻辑地址和物理地址是一一对应的）。 对于32位的windows操作系统，其逻辑地址编码采用的地址位数是32位的，那么操作系统所提供的逻辑地址寻址范围是4GB，而在intel x86架构下，采用的是内存映射技术(Memory-Mapped I/O, MMIO)，也就说将4GB逻辑地址中一部分要划分出来与BIOS ROM、CPU寄存器、I/O设备这些部件的物理地址进行映射，那么逻辑地址中能够与内存条的物理地址进行映射的空间肯定没有4GB了，看下面这幅图就明白了： 所以当我们装了32位的windows操作系统，即使我们买了4GB的内存条，实际上能被操作系统访问到的肯定小于4GB，一般情况是3.2GB左右。假如说地址总线位数没有32位，比如说是20位，那么CPU能够寻址到1MB的物理地址空间，此时操作系统即使能支持4GB的逻辑地址空间并且假设内存条是4GB的，能够被用户访问到的空间不会大于1MB（当然此处不考虑虚拟内存技术），所以用户能够访问到的最大内存空间是由硬件和操作系统两者共同决定的，两者都有制约关系。 于64位的操作系统，其逻辑地址编码采用的地址位数是40位，能够最大支持1T的逻辑地址空间。考虑一种情况，假如CPU是64位的，地址总线位数是40位，操作系统也是64位的，逻辑地址编码采用的地址位数也是40位，内存条大小是64GB，那么是不是内存条的64GB全部都能被利用了呢？答案是不一定，因为这里面还要考虑一个因素就是内存控制器，内存控制器位于北桥之内（现在基本都是放在CPU里面了），内存控制器的实际连接内存的地址线决定了可以支持的内存容量，也就是说内存控制器与内存槽实际连接的地址线如果没有40位的话，是无法完全利用64GB的内存条的存储空间的。当然对于内存控制器这个问题几乎可以不用考虑，因为现在大多数的内存控制器至少都采用的是40位地址总线。 总结对于以上所述，我进行一下简单的总结： 一个计算机，它的内存访问能力是由硬件和软件共同决定的。硬件层面就指 CPU 的寻址能力，也就是地址总线的个数。软件层面，指的就是操作系统。实际上我们（进程）在进行内存访问的时候，访问的都是逻辑地址，而逻辑地址是由操作系统提供的。对于 32 位的操作系统，其逻辑地址编码采用的地址位数是 32 位，那么操作系统所提供的逻辑地址寻址范围就是 4GB。从这个方面来说，纵使你的 CPU 实际寻址能力为 2 的 64 次方，由于操作系统只提供 4GB 的逻辑地址，那 CPU 透过操作系统所能访问到的内存大小也就只有4GB了。 对与上述总结，我还有一个简单的补充： 在和我的大神小伙伴讨论的时候，他对于“为什么 64 位 CPU 装载 32 位操作系统，它的寻址能力还是 4GB”这个问题的解答，只说了一句话：“32 位操作系统没有对应 64 位的寻址指令”。在细细品味之后，觉得颇有道理，我对于这句话的浅显理解是：32 位操作系统没有对应 64 位的寻址指令，所以它不能提供 4GB 以上的逻辑地址，所以 64 位 CPU 透过 32 位操作系统，它的寻址能力依旧是 4GB。 参考阅读维基百科：64位 32位系统最大只能支持4GB内存之由来]]></content>
      <categories>
        <category>计算机组成原理</category>
      </categories>
      <tags>
        <tag>体系结构</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发--synchronized实现原理及锁优化]]></title>
    <url>%2F2018%2F04%2F27%2FJava%E5%B9%B6%E5%8F%91-synchronized%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%E5%8F%8A%E9%94%81%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[注：本文中的部分内容摘抄自他人博客，如有侵权，请联系我，侵删~ 本篇博客主要讲述 synchronized 关键字的实现原理以及 JDK 1.6 后对 synchronized 的种种优化。synchronized 的使用不再赘述。 博主目前依旧存在的疑惑请在阅读完此篇博客之后，帮助博主回答这三个问题： 多线程争夺 Monitor 的具体过程是怎样的？是根据 ObjectMonitor 中的 _count 值判断当前 Monitor 是否被锁定吗？ JVM 如果检测到在单线程环境下执行同步代码（StringBuffer），是会进行锁消除呢，还是会使用偏向锁？ 对于偏向锁的撤销过程及膨胀过程，博主只是在一些博客的基础上给出了自己的理解！不权威，建议阅读源码，博主对这部分知识的讲解持怀疑态度，如果在阅读的过程中发现博主对偏向锁的撤销与膨胀理解有误，请指出，感激不尽~（网上基本上没有从源码角度分析的，对于偏向锁撤销与升级的详细过程也是众说纷纭） 引言我们先来看一份代码： 1234567891011public class SynchronizedTest &#123; public synchronized void test1() &#123; &#125; public void test2() &#123; synchronized (this) &#123; &#125; &#125;&#125; 对其进行 javap 反编译分析： 12345678910111213141516171819202122232425262728293031323334javap -c SynchronizedTest.classCompiled from "SynchronizedTest.java"public class org.xiyoulinux.SynchronizedTest &#123; public org.xiyoulinux.SynchronizedTest(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object."&lt;init&gt;":()V 4: return public synchronized void test1(); Code: 0: return public void test2(); Code: 0: aload_0 1: dup 2: astore_1 3: monitorenter 4: aload_1 5: monitorexit 6: goto 14 9: astore_2 10: aload_1 11: monitorexit 12: aload_2 13: athrow 14: return Exception table: from to target type 4 6 9 any 9 12 9 any&#125; 对比 javap 的输出结果，我们做一个简单的总结： 同步方法：synchronized 方法会被翻译成普通的方法调用。在 JVM 字节码层面并没有任何特别的指令来实现被 synchronized 修饰的方法。在 Class 文件的方法表中将该方法的 access_flags 字段中的 synchronized 标志位置 1，表示该方法是同步方法并使用调用该方法的对象（对象锁）或该方法所属的 Class（类锁） 做为锁对象。 同步块：monitorenter 指令插入到同步代码块的开始位置，monitorexit 指令插入到同步代码块的结束位置，JVM 需要保证每一个 monitorenter 都有一个 monitorexit 与之相对应。任何对象都有一个 monitor 与之相关联，当且一个 monitor 被持有之后，他将处于锁定状态。线程执行到 monitorenter 指令时，将会尝试获取对象所对应的 monitor 所有权，即尝试获取对象的锁。（关于上述字节码中一个 monitorenter 指令为什么对应两个 monitorexit 指令我们稍后进行说明） synchronized底层语义原理Java对象头要深入理解 synchronized 的实现原理，先来了解一下 Java 对象头。 对象在堆中由三部分组成： 对象头 实例变量 填充数据 实例变量：存放类的属性数据信息，包括父类的属性信息，如果是数组的实例部分还包括数组的长度，这部分内存按4字节对齐。 填充数据：由于虚拟机要求对象起始地址必须是 8 字节的整数倍。填充数据不是必须存在的，仅仅是为了字节对齐。 对象头：HotSpot 虚拟机的对象头主要包括两部分数据：Mark Word（标记字段）、Class Point（类型指针）。其中 Class Point 是对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例，Mark Word 用于存储对象自身的运行时数据，它是实现轻量级锁和偏向锁的关键。它还用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程 ID、偏向时间戳等等。 Java 对象头一般占有两个字宽（在 32 位虚拟机中，1 个字宽等于 4 字节，也就是 32bit），但是如果对象是数组类型，则需要三个字宽，因为 JVM 虚拟机可以通过 Java 对象的元数据信息确定 Java 对象的大小，但是无法从数组的元数据来确认数组的大小，所以用一块来记录数组长度。 对象头的存储结构如下： 长度 内容 说明 32/64 bit Mark Word 存储对象的 hashCode 或锁信息等。 32/64 bit Class Metadata Address 存储到对象类型数据的指针 32/64 bit Array length 数组的长度（如果当前对象是数组） 32 位 JVM 的 Mark Word 的默认存储结构如下： 锁状态 25bit 4bit 1bit是否是偏向锁 2bit 锁标志位 无锁状态 对象HashCode 对象分代年龄 0 01 由于对象头的信息是与对象自身定义的数据没有关系的额外存储成本，因此考虑到 JVM 的空间效率，Mark Word 被设计成为一个非固定的数据结构，以便存储更多有效的数据，它会根据对象本身的状态复用自己的存储空间，如 32 位 JVM 下，除了上述列出的 Mark Word 默认存储结构外，还有如下可能变化的结构： Monitor（管程） 什么是 Monitor（管程）？ 我们可以把它理解为一个同步工具，也可以描述为一种同步机制，它通常被描述为一个对象。所有的 Java 对象都是天生的 Monitor，在 Java 的设计中 ，每一个 Java 对象都带了一把看不见的锁，它叫做内置锁或者 Monitor 锁。 观察 Mark Word 存储结构的那张图（上图）： 这里我们主要分析一下重量级锁也就是通常说 synchronized 的对象锁，锁标识位为 10，其中指针指向的是 monitor 对象（也称为管程或监视器锁）的起始地址。每个对象都存在着一个 monitor 与之关联，对象与其 monitor 之间的关系存在多种实现方式，如 monitor 可以与对象一起创建销毁或当线程试图获取对象锁时自动生成，但当一个 monitor 被某个线程持有后，它便处于锁定状态。在 Java 虚拟机(HotSpot)中，monitor 是由 ObjectMonitor 实现的，其主要数据结构如下：（位于 HotSpot 虚拟机源码 ObjectMonitor.cpp 文件，C++实现） 123456789101112131415161718ObjectMonitor() &#123; _header = NULL; _count = 0; // 记录个数 _waiters = 0, _recursions = 0; _object = NULL; _owner = NULL; _WaitSet = NULL; // 处于 wait 状态的线程，会被加入到 _WaitSet _WaitSetLock = 0; _Responsible = NULL; _succ = NULL; _cxq = NULL; FreeNext = NULL; _EntryList = NULL; // 处于等待锁 block 状态的线程，会被加入到该列表 _SpinFreq = 0; _SpinClock = 0; OwnerIsThread = 0;&#125; ObjectMonitor 中有两个队列，_WaitSet 和 _EntryList，用来保存 ObjectWaiter 对象列表( 每个等待锁的线程都会被封装成 ObjectWaiter 对象)，_owner 指向持有 ObjectMonitor 对象的线程，当多个线程同时访问一段同步代码时，首先会进入 _EntryList 集合，当线程获取到对象的 monitor 后会把 monitor 中的 _owner 变量设置为当前线程，同时 monitor 中的计数器 _count 加 1。若线程调用 wait() 方法，将释放当前持有的 monitor，_owner 变量恢复为 null，_count 自减 1，同时该线程进入 _WaitSet 集合中等待被唤醒。若当前线程执行完毕也将释放 monitor（锁）并复位变量的值，以便其它线程进入获取 monitor(锁)。 由此看来，monitor 对象存在于每个 Java 对象的对象头中(存储的是指针)，synchronized 便是通过这种方式获取锁的，也是为什么 Java 中任意对象可以作为锁的原因，同时也是 notify/notifyAll/wait 等方法存在于顶级对象 Object 中的原因（锁可以是任意对象，所以可以被任意对象调用的方法是定义在 object 类中）。 synchronized方法底层原理我们在引言部分对 synchronized 方法已经做了一个简单的总结，现在对它进行一点补充： 在 Java 早期版本中，synchronized 属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层操作系统的 Mutex Lock 来实现的，而操作系统实现线程之间的切换时需要从用户态转换到核心态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，这也是为什么早期的 synchronized 效率低的原因。庆幸的是在 Java 6 之后 Java 官方从 JVM 层面对 synchronized 进行了较大优化，所以现在的 synchronized 锁效率也优化得很不错了。Java 6 之后，为了减少获得锁和释放锁所带来的性能消耗，引入了轻量级锁和偏向锁，关于锁优化的内容，我们稍后再谈。 synchronized代码块底层原理在引言部分，我们对 synchronized 代码块也做了一个简单的总结。同样，对其做一点补充： 当执行 monitorenter 指令时，当前线程将试图获取对象锁所对应的 monitor 的持有权，当对象锁的 monitor 的进入计数器为 0，那线程可以成功取得 monitor，并将计数器值设置为 1，取锁成功。如果当前线程已经拥有对象锁的 monitor 的持有权，那它可以重入这个 monitor，重入时计数器的值会加 1。倘若其他线程已经拥有对象锁的 monitor 的所有权，那当前线程将被阻塞，直到正在执行的线程执行完毕，即 monitorexit 指令被执行，执行线程将释放 monitor 并设置计数器值为 0，其他线程将有机会持有 monitor。值得注意的是编译器将会确保无论方法通过何种方式完成，方法中调用过的每条 monitorenter 指令都有执行其对应 monitorexit 指令，无论这个方法是正常结束还是异常结束。为了保证在方法异常完成时 monitorenter 和 monitorexit 指令依然可以正确配对执行，编译器会自动产生一个异常处理器，这个异常处理器可处理所有的异常，它的目的就是用来执行 monitorexit 指令。从字节码中也可以看出多了一个 monitorexit 指令。 锁优化自旋锁与自适应自旋如前面所述，synchronized 在 JDK 1.6 之前之所以被称为“重量级锁”，是因为对于互斥同步的性能来说，影响最大的就是阻塞的实现。挂起线程与恢复线程的操作都需要转入内核态中完成。从用户态转入内核态是比较耗费系统性能的。 研究表明，大多数情况下，线程持有锁的时间都不会太长，如果直接挂起操作系统层面的线程可能会得不偿失，毕竟操作系统实现线程之间的切换时需要从用户态转换到核心态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高。自旋锁会假设在不久将来，当前的线程可以获得锁，因此虚拟机会让当前想要获取锁的线程做几个空循环，使当前线程不放弃处理器的执行时间(这也是称为自旋的原因)，在经过若干次循环后，如果得到锁，就顺利进入临界区。 但是自旋不能代替阻塞，首先，自旋锁需要多处理器或一个处理器拥有多个核心的 CPU 环境，这样才能保证两个及以上的线程并行执行（一个是获取锁的执行线程，一个是进行自旋的线程）。除了对处理器数量的要求外，自旋虽然避免了线程切换的开销，但它是要占用处理器时间的，因此，如果锁被占用的时间比较短，自旋的效果就比较好，否则只是白白占用了 CPU 资源，带来性能上的浪费。 那么自旋就需要有一定的限度，如果自旋超过了一定的次数后，还没有成功获取锁，就只能进行挂起了，这个次数默认是 10。 在 JDK 1.4.2 中引入了自旋锁，在 JDK 1.6 中引入了自适应自旋锁。自适应意味自旋的时间不再固定： 如果同一个锁对象上，自旋等待刚刚成功获取锁，并且持有锁的线程正在运行，那么虚拟机就会认为此次自旋也很有可能成功，进而它将允许自旋等待持续相对更长的时间，比如 100 个循环。如果对于某个锁，自旋很少成功获取过，那么在以后获取这个锁时将可能自动省略掉自旋过程，以避免浪费处理器资源。有了自适应自旋，随着程序运行和性能监控信息的不断完善，虚拟机对程序锁的状况预测就会越来越精准，虚拟机也就会越来越“聪明”。 锁消除消除锁是虚拟机另外一种锁的优化，这种优化更彻底，Java 虚拟机在 JIT 编译时(关于 JIT 编译可以参考我的这篇博客：JVM–解析运行期优化与JIT编译器)，通过对运行上下文的扫描，去除不可能存在共享资源竞争的锁，通过这种方式消除没有必要的锁，可以节省毫无意义的请求锁时间。 锁消除的主要判定依据来源于逃逸分析技术的支持(关于逃逸分析技术可以参考周志明老师所出的《深入理解 Java 虚拟机》一书中第 11 章内容或自行百度)。 也许你会有疑惑，变量是否逃逸，程序员本身应该就可以判断，怎么会存在明知道不存在数据争用的情况下还使用同步？来看如下代码： 123public String concatString(String s1, String s2, String s3) &#123; return s1 + s2 + s3;&#125; 由于 String 是一个不可变类，因此对字符串的连接操作总是通过新生成的 String 对象来进行的，在 JDK 1.5 之前，javac 编译器会对 String 连接进行自动优化，将连接转换为 StringBuffer 对象的连续 append 操作，在 JDK 1.5 之后，会转化为 StringBuilder 对象的连续 append 操作。也就是说，上述代码经过 javac 优化之后，有可能变为下面这样： 12345678public String concatString(String s1, String s2, String s3) &#123; StringBuffer sb = new StringBuffer(); sb.append(s1); sb.append(s2); sb.append(s3); return sb.toString();&#125; StringBuffer 是一个线程安全的类，在它的 append 方法中有一个同步块，锁对象就是 sb，但是虚拟机观察变量 sb，发现它是一个局部变量，本身线程安全，并不需要额外的同步机制。因此，这里虽然有锁，但可以被安全的清除，在 JIT 编译之后，这段代码就会忽略掉所有的同步而直接执行。这就是锁消除。 锁粗化原则上，我们在使用同步块的时候，总是建议将同步块的作用范围限制的尽量小—使需要同步的操作数量尽可能变小，在存在锁竞争的情况下，等待锁的线程可以尽快的拿到锁。 大部分情况下，上述原则都正确，但是存在特殊情况，如果一系列操作下来，都对同一个对象反复加锁与解锁，甚至加锁与解锁操作出现在循环体中，那即使没有线程竞争，频繁的进行互斥同步操作也会导致不必要的性能损耗。 如上述代码中的 append 方法。如果虚拟机探测到了这样的操作，就会把加锁的同步范围扩展（粗化）到整个操作序列的外部。以上述代码为例，就是扩展到第一个 append 操作之前直至最后一个 append 操作之后，这样只需要加锁一次。 偏向锁偏向锁会偏向第一个获取它的线程，如果在接下来的执行过程中，该锁没有被其他线程获取，则持有偏向锁的线程将永远不需要进行同步。 HotSpot 的作者经过以往的研究发现大多数情况下锁不仅不存在多线程竞争，而且总是由同一线程多次获得（比如在单线程中使用 StringBuffer 类），为了让线程获得锁的代价更低而引入了偏向锁。当锁对象第一次被线程获取的时候，虚拟机把对象头中的标志位设为“01”，即偏向模式。同时使用 CAS 操作把获取这个锁的线程 ID 记录在对象的 Mark Word 中，如果 CAS 操作成功，持有偏向锁的线程以后每次进入这个锁相关的同步块时，虚拟机都可以不用进行任何同步操作。 当有另一个线程去尝试获取这个锁时，偏向模式就宣告结束。 如上图，当线程 2 争夺锁对象时，偏向模式宣告结束。由线程 2 通知线程 1 进行偏向锁的撤销，此时线程 1 在全局安全点（没有字节码执行的地方）处进行暂停，进行解锁操作。 偏向锁只能被第一个获取它的线程进行 CAS 操作，一旦出现线程竞争锁对象，其它线程无论何时进行 CAS 操作都会失败。 在解锁成功之后，JVM 将判断当前线程的状态，如果还没有执行完同步代码块，则直接将偏向锁膨胀为轻量级锁，然后继续执行同步代码块，否则将偏向锁先撤销为无锁状态，当下一次执行同步代码块的时候再由 JVM 将其膨胀为轻量级锁。 使用偏向锁的优点在于在没有多线程竞争的情况下，只需进行一次 CAS 操作，就可执行同步代码块，但是我们也必须保证撤销偏向锁所耗费的性能资源要低于省去加锁取锁所节省下来的性能资源。 轻量级锁偏向锁一旦受到多线程竞争，就会膨胀为轻量级锁。 偏向锁在执行同步块的时候不用做任何同步操作，而轻量级锁是在多线程交替执行同步代码块，不产生线程阻塞的情况下使用 CAS 操作去消除同步使用的互斥量。 轻量级锁加锁：线程在执行同步块之前，如果同步对象没有被锁定，JVM 会先在当前线程的栈桢中创建用于存储锁记录（Lock Record）的空间，并将对象头中的 Mark Word 复制到锁记录中，官方称为 Displaced Mark Word。然后线程尝试使用 CAS 将对象头中的 Mark Word 替换为指向锁记录的指针。如果成功，当前线程获得锁，如果失败，表示其他线程竞争锁，当前线程便尝试使用自旋来获取锁，如果自旋还是无法获取到锁，轻量级锁便会膨胀为重量级锁。 轻量级锁解锁：轻量级解锁时，会使用 CAS 操作来将 Displaced Mark Word 替换回到对象头，如果成功，则表示没有竞争发生。如果失败，表示当前锁存在竞争，锁就会膨胀成重量级锁。下图是两个线程同时争夺锁，导致锁膨胀的流程图： 如上图，当线程 1 还在使用轻量级锁执行同步代码块的时候，线程 2 尝试争夺轻量级锁，就会失败，失败之后线程 2 并不会直接将轻量级锁膨胀为重量级锁，而是先进行自旋等待，如果成功获取到锁，则不进行锁的膨胀。在线程 2 成功将锁升级之后，线程 2 进行阻塞。线程 1 执行完同步代码块之后尝试 CAS 解锁，解锁失败，发现有线程对锁进行过竞争，则释放锁并唤醒等待线程。 补充锁的升级 锁主要存在四种状态，依次是：无锁状态、偏向锁状态、轻量级锁状态、重量级锁状态，他们会随着竞争的激烈而逐渐升级。锁可以升级不可降级，这种策略是为了提高获得锁和释放锁的效率。 各个状态锁的优缺点对比 锁 优点 缺点 适用场景 偏向锁 加锁和解锁不需要额外的消耗，和执行非同步方法比仅存在纳秒级的差距。 如果线程间存在锁竞争，会带来额外的锁撤销的消耗。 适用于只有一个线程访问同步块场景。 轻量级锁 竞争的线程不会阻塞，提高了程序的响应速度。 始终得不到锁的线程使用自旋会消耗CPU。 追求响应时间。同步块执行速度非常快。 重量级锁 线程竞争不使用自旋，不会消耗CPU。 线程阻塞，响应时间缓慢。 同步块执行速度较慢。 总结 synchronized 的底层实现主要依靠 Monitor（管程）； 从管程我们需要延伸至 Java 对象头这一部分； 了解过 Java 对象头之后，我们可以对 Monitor 的底层实现（ObjectMonitor）再进行简单的了解； 熟悉多线程争夺 Monitor 的过程； 最后分类讨论同步方法与同步块； 熟悉锁粗化、锁消除、自旋与自适应自旋等相关概念； 熟悉偏向锁、轻量级锁、重量级锁的相关概念； 熟悉偏向锁、轻量级锁解锁的过程； 熟悉偏向锁、轻量级锁、重量级锁膨胀的过程。 参考阅读《深入理解Java虚拟机》–周志明 深入理解Java并发之synchronized实现原理 聊聊并发（二）Java SE 1.6中的Synchronized 死磕 Java 并发 - 深入分析 synchronized 的实现原理]]></content>
      <categories>
        <category>Java并发</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>synchronized</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java源码--JDK 1.8 HashMap重点源码部分剖析]]></title>
    <url>%2F2018%2F04%2F20%2FJava%E6%BA%90%E7%A0%81-JDK-1-8-HashMap%E9%87%8D%E7%82%B9%E6%BA%90%E7%A0%81%E9%83%A8%E5%88%86%E5%89%96%E6%9E%90%2F</url>
    <content type="text"><![CDATA[注：感谢 美团点评技术团队 的分享~~，博客部分内容摘抄自其中。侵删！ 今天我们来探究一下 HashMap 的内部实现机制。 明确 JDK 1.8 中的 HashMap 使用数组 + 链表 + 红黑树的结构进行实现。 HashMap 的底层思想主要是哈希表，我们来看看 Java 的设计者们是怎么使用数组 + 链表 + 红黑树设计出 HashMap 的。 HashMap的基本属性既然是用哈希表进行实现，那么基本的数据结构就是数组了，HashMap 部分源码如下： 1234567public class HashMap&lt;K,V&gt; extends AbstractMap&lt;K,V&gt; implements Map&lt;K,V&gt;, Cloneable, Serializable &#123; transient Node&lt;K,V&gt;[] table; // HashMap 底层数据结构（Node 数组） transient int size; // HashMap 中实际存在的键值对数量 transient int modCount; // 记录 HashMap 内部结构发生变化的次数，用于快速失败机制 int threshold; // 所能容纳的 key-value 对极限（我将之称为“负载”） final float loadFactor; // 负载因子：默认 0.75&#125; 除了 table 数组之外，我将源码中的常用字段也贴了出来。对于上面的代码，我们需要注意以下几点： 不了解 AbstractMap&lt;K,V&gt; 抽象类、Map&lt;K,V&gt;, Cloneable, Serializable 接口的请自行百度 transient 关键字：阻止本字段进行序列化（具体使用请自行百度） threshold = length（哈希表长度） * loadFactor modCount 记录的是 HashMap 内部结构发生变化的次数，内部结构发生变化指的是结构发生变化，例如 put 新键值对，但是某个 key 对应的 value 值被覆盖不属于结构变化。 有了对 table 数组的认识，那么我们用一张图来描述一下 HashMap 中的哈希表结构（来自 “美团点评技术团队” 侵删）： 了解了 HashMap 中的成员变量，再来看一下 HashMap 中定义的常量： 1234567891011121314151617181920// 默认的初始容量，必须是2的幂。 static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; //最大容量（必须是 2 的幂且小于 2 的 30 次方，传入容量过大将被这个值替换） static final int MAXIMUM_CAPACITY = 1 &lt;&lt; 30; // 装载因子static final float DEFAULT_LOAD_FACTOR = 0.75f; // JDK1.8特有 // 当 hash 值相同的记录超过 TREEIFY_THRESHOLD，会动态的使用一个专门的红黑树实现来代替链表结构，使得查找时间复杂度从 O(n) 变为 O(logn) static final int TREEIFY_THRESHOLD = 8; // JDK1.8特有 // 也是阈值，同上一个相反，当桶(bucket)上的链表数小于 UNTREEIFY_THRESHOLD 时红黑树转链表 static final int UNTREEIFY_THRESHOLD = 6; // JDK1.8特有 // 树的最小的容量，至少是 4 x TREEIFY_THRESHOLD = 32 然后为了避免(resizing 和 treeification thresholds) 设置成64 static final int MIN_TREEIFY_CAPACITY = 64; HashMap中的Node元素现在，我们关心的是 table 数组中 Node 元素的实现，源码如下： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051// 静态内部类、操纵了 Map 接口中的 Entry&lt;K,V&gt; 接口static class Node&lt;K,V&gt; implements Map.Entry&lt;K,V&gt; &#123; // key 所产生的 hash 值 （不变） final int hash; // key （不变） final K key; // value V value; // 指向下一个 Node 节点、（链地址法解决冲突） Node&lt;K,V&gt; next; // 构造函数 Node(int hash, K key, V value, Node&lt;K,V&gt; next) &#123; this.hash = hash; this.key = key; this.value = value; this.next = next; &#125; // 这些方法都不可被重写 public final K getKey() &#123; return key; &#125; public final V getValue() &#123; return value; &#125; public final String toString() &#123; return key + "=" + value; &#125; // 计算 key 所产生的 hash 码 public final int hashCode() &#123; return Objects.hashCode(key) ^ Objects.hashCode(value); &#125; // 设置新值，返回旧值 public final V setValue(V newValue) &#123; V oldValue = value; value = newValue; return oldValue; &#125; // 比较两个对象是否相等 public final boolean equals(Object o) &#123; if (o == this) return true; // 是否都操作 Map.Entry 接口 if (o instanceof Map.Entry) &#123; // 属于同一个类之后再对对象的属性进行比较 Map.Entry&lt;?,?&gt; e = (Map.Entry&lt;?,?&gt;)o; if (Objects.equals(key, e.getKey()) &amp;&amp; Objects.equals(value, e.getValue())) return true; &#125; return false; &#125;&#125; 在这里我们需要注意： Node 的实现是一个静态内部类，有关内部类与静态内部类的理解，请查看我的知乎回答：为什么Java内部类要设计成静态和非静态两种？ hash 值与 key 的不变性：即使在 HashMap 中对 key 及 hash 做了final 关键字的约束，但是我们还是需要注意，最好使用不变对象作为 key。 首先我们来了解一下 final 关键字在基本类型与引用类型的使用上有什么不同？ 当 final 修饰基本变量类型时，不能对基本类型变量重新赋值，因此基本类型变量不能被改变。 当 final 修饰引用类型变量时，final 只保证这个引用类型变量所引用的地址不会改变，即一直引用同一个对象，但是这个对象(对象的非 final 成员变量的值可以改变)完全可以发生改变。 再来讨论，我们在使用 HashMap 时，为什么最好选用不可变对象作为 key。 来看一下选用可变对象作为 HashMap 的 key 有可能会造成什么影响？ 123456789101112131415161718import java.util.HashMap;import java.util.Map; public class MutableDemo1 &#123; public static void main(String[] args) &#123; Map&lt;MutableKey, String&gt; map = new HashMap&lt;&gt;(); MutableKey key = new MutableKey(10, 20); map.put(key, "Robin"); System.out.println(map.get(key)); key.setI(30); System.out.println(map.get(key)); &#125;&#125; 输出： 12Robinnull 为什么最好不要使用可变对象作为 HashMap 的 key，结论： 如果 key 对象是可变的，那么 key 的哈希值就可能改变。在 HashMap 中可变对象作为 key 会造成数据丢失。 怎么解决？ 在 HashMap 中，尽量使用 String、Integer 等不可变类型用作 key。 重写自定义类的 hashcode 方法，保证在成员变量改变的同时该对象的哈希值不变即可。（具体实现参见：HashMap 的 key 可以是可变的对象吗？） HashMap中的put方法Hash值的计算我们对 HashMap 的基本组成结构已经有了完整的认识，接下来我们分析 HashMap 中最常用的方法之一：put()。 直接上源码： 123public V put(K key, V value) &#123; return putVal(hash(key), key, value, false, true);&#125; 在分析 putVal 的源码之前，我们先来看看 hash(key)： 1234static final int hash(Object key) &#123; int h; return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);&#125; key 的 hash 值就是这样得到的，key.hashCode()是一个本地方法，具体实现在源码中并没有给出，但这并不是重点，我们需要注意的是在计算出 hash 值后，它又与本身的高 16 位进行了异或。（hash 值本身是 32 位） 为什么这样做？这样做的好处是什么呢？ 主要是从速度、功效、质量来考虑的，这么做可以在数组 table 的 length 比较小的时候，也能保证考虑到高低 Bit 都参与到 Hash 的计算中，同时不会有太大的开销。在混合了原始 hashCode 值的高位和低位后，加大了低位的随机性，而且混合后的低位掺杂了高位的部分特征，这样高位的信息也被变相保留下来，这就使得 hash 方法返回的值，具有更高的随机性，减少了冲突。 下面举例说明，n 为 table 的长度（假设为 16）。 put方法的解析在分析 put 方法的源码之前，我们先来看一张有关 put 方法执行过程的图解，来自 美团点评技术团队，侵删~ 根据图片我们再对 put 方法的执行流程做一个总结，方便等下阅读源码： 判断键值对数组 table 是否为空或为 null，否则执行 resize() 进行扩容； 根据键值 key 计算 hash 值得到插入的数组索引 i，如果 table[i] == null，直接新建节点添加，转向 6，如果 table[i] 不为空，转向 3； 判断 table[i] 的首个元素是否和 key 一样，如果相同直接覆盖 value，否则转向 4，这里的相同指的是 hashCode 以及 equals； 判断 table[i] 是否为 treeNode，即 table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向 5； 遍历 table[i]，判断链表长度是否大于 8，大于 8 的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现 key 已经存在直接覆盖 value 即可； 插入成功后，判断实际存在的键值对数量 size 是否超过了负载 threshold，如果超过，进行扩容。 putVal 方法源码： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859final V putVal(int hash, K key, V value, boolean onlyIfAbsent, boolean evict) &#123; Node&lt;K,V&gt;[] tab; Node&lt;K,V&gt; p; int n, i; // 哈希表为null || 哈希表的长度为 0（resize 也是一个经典的方法） if ((tab = table) == null || (n = tab.length) == 0) n = (tab = resize()).length; // (n - 1) &amp; hash 计算出 key 在哈希表中应该存储的位置（除留余数法，使用 &amp; 运算 比 % 运算更快） if ((p = tab[i = (n - 1) &amp; hash]) == null) tab[i] = newNode(hash, key, value, null); else &#123; Node&lt;K,V&gt; e; K k; // 插入的 key 在 HashMap 中已经存在（之后进行 value 的直接覆盖） if (p.hash == hash &amp;&amp; ((k = p.key) == key || (key != null &amp;&amp; key.equals(k)))) e = p; // 产生冲突，当前节点为红黑树节点 else if (p instanceof TreeNode) e = ((TreeNode&lt;K,V&gt;)p).putTreeVal(this, tab, hash, key, value); // 普通节点，使用链地址法进行处理 else &#123; // 遍历链表（插入新节点之后，判断链表长度） for (int binCount = 0; ; ++binCount) &#123; if ((e = p.next) == null) &#123; p.next = newNode(hash, key, value, null); // 当处理冲突的链节点数大于等于 8 的时候，转换红黑树 if (binCount &gt;= TREEIFY_THRESHOLD - 1) treeifyBin(tab, hash); break; &#125; // 插入的 key 在 HashMap 中已经存在 if (e.hash == hash &amp;&amp; ((k = e.key) == key || (key != null &amp;&amp; key.equals(k)))) break; p = e; &#125; &#125; // key 已经存在，直接覆盖旧值 if (e != null) &#123; V oldValue = e.value; if (!onlyIfAbsent || oldValue == null) e.value = value; afterNodeAccess(e); return oldValue; &#125; &#125; ++modCount; // 记录 HashMap 内部结构发生变化的次数，用于快速失败机制 if (++size &gt; threshold) resize(); // 扩容 afterNodeInsertion(evict); // 作用不明确 return null;&#125; put 方法分析到这里基本上就结束了，但是我们同样有两个值得思考的问题： 哈希表索引定位：(n - 1) &amp; hash； 扩容机制：resize()。 关于红黑树与快速失败机制，不在这篇博客中进行讲述。 索引定位你不觉得以(n - 1) &amp; hash这种方式定位元素在哈希表中的位置很有趣吗？ 本质上，它还是“除留余数法”，只不过由于位运算的缘故，会比取模运算要高效许多。 但是使用这种方法有一个前提，就是哈希表 table 的长度 n 必须满足 2 幂次方，因为 n-1 对应的二进制就是前面全是 0，后面全是 1，相与后，只留下 hash 的后几位，正好在长度为 n 的数组下标范围内。 举个例子，假设 hash 值为 3，数组长度 n 为 16，那么我们使用取模运算得到：3 % 16 = 3，使用 &amp; 运算：0011 &amp; (16 - 1) 即 0011 &amp; 1111 = 0011 得到的还是 3。 而在 HashMap 中，哈希表 table 的默认初始值也为 16（源码如下）： 1static final int DEFAULT_INITIAL_CAPACITY = 1 &lt;&lt; 4; 扩容机制我们不谈红黑树，但必须探究包含在 put 方法中的 resize（扩容）机制。了解过 resize 方法之后，你会感叹其设计之巧妙！ 首先，对扩容机制做一个简单的介绍： 扩容(resize)就是重新计算容量，向 HashMap 对象里不停的添加元素，而 HashMap 对象内部的数组无法装载更多的元素时，对象就需要扩大数组的长度，以便能装入更多的元素。如果 `HashMap 的实际大小 &gt; 负载，则 HashMap 中的 table 的容量扩充为当前的一倍。容量翻倍后，重新计算每个 Node 的 index，将有限的元素映射到更大的数组中，减少 hash 冲突的概率。 我将扩容机制分为了两部分：1. 创建新的 table 数组；2. 对元素进行 rehash。 创建新的 table 数组，过程还是比较简单的： （1）原 table 数组的大小已经最大，无法扩容，则修改 threshold 的大小为 Integer.MAX_VALUE。产生的效果就是随你碰撞，不再扩容；（2）原 table 数组正常扩容，更新 newCap（新数组的大小） 与 newThr（新数组的负载）；（3）原 table 数组为 null || length 为 0，则扩容使用默认值；（4）原 table 数组的大小在扩容后超出范围，将 threshold 的大小更改为 Integer.MAX_VALUE。 我们先截取第一部分（创建新数组）的源码进行研究： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546final Node&lt;K,V&gt;[] resize() &#123; Node&lt;K,V&gt;[] oldTab = table; int oldCap = (oldTab == null) ? 0 : oldTab.length; int oldThr = threshold; int newCap, newThr = 0; if (oldCap &gt; 0) &#123; // 超过最大值就不再扩充，随你去碰撞（将 threshold 设置为 Integer.MAX_VALUE，则不会产生扩容） if (oldCap &gt;= MAXIMUM_CAPACITY) &#123; threshold = Integer.MAX_VALUE; return oldTab; &#125; // 扩容成功，更新 newCap 与 newThr 的大小（2 倍扩展） else if ((newCap = oldCap &lt;&lt; 1) &lt; MAXIMUM_CAPACITY &amp;&amp; oldCap &gt;= DEFAULT_INITIAL_CAPACITY) newThr = oldThr &lt;&lt; 1; &#125; // ！！！对应的哪种情况？ else if (oldThr &gt; 0) newCap = oldThr; // oldCap == 0 || oldTab == null else &#123; newCap = DEFAULT_INITIAL_CAPACITY; newThr = (int)(DEFAULT_LOAD_FACTOR * DEFAULT_INITIAL_CAPACITY); &#125; // 扩容失败（扩容后 newCap &gt;= MAXIMUM_CAPACITY） if (newThr == 0) &#123; float ft = (float)newCap * loadFactor; newThr = (newCap &lt; MAXIMUM_CAPACITY &amp;&amp; ft &lt; (float)MAXIMUM_CAPACITY ? (int)ft : Integer.MAX_VALUE); &#125; // 更新负载的值 threshold = newThr; @SuppressWarnings(&#123;"rawtypes","unchecked"&#125;) Node&lt;K,V&gt;[] newTab = (Node&lt;K,V&gt;[])new Node[newCap]; table = newTab; // rehash 的过程 ... ... return newTab;&#125; JDK 1.7中的rehash直接阅读 JDK 1.8 中的 rehash 过程让人有点头大，为了便于理解，我们先来看看 JDK 1.7 中的 rehash ，总体来说，两个版本差别不大： 1234567891011121314151617void transfer(Entry[] newTable) &#123; Entry[] src = table; // src 引用了旧的 Entry 数组 int newCapacity = newTable.length; for (int j = 0; j &lt; src.length; j++) &#123; // 遍历旧的 Entry 数组 Entry&lt;K,V&gt; e = src[j]; // 取得旧 Entry 数组的每个元素 if (e != null) &#123; src[j] = null; // 释放旧 Entry 数组的对象引用（for 循环后，旧的 Entry 数组不再引用任何对象） do &#123; Entry&lt;K,V&gt; next = e.next; int i = indexFor(e.hash, newCapacity); // 重新计算每个元素在数组中的位置 e.next = newTable[i]; // 头插法 newTable[i] = e; // 将元素放在数组上 e = next; // 访问下一个 Entry 链上的元素 &#125; while (e != null); &#125; &#125;&#125; 为了方便大家的理解，下面举个例子说明下扩容过程： 注：JDK 1.7 中的 put 方法使用的是头插法进行新节点的插入，在 JDK 1.8 中，则使用的是尾插法（见上述源码）。对 JDK 1.7 put 方法感兴趣的同学可自行查阅有关资料。 假设我们的 hash 算法就是简单的用 key mod 一下表的大小。其中的哈希桶数组 table 的 size = 2，key = 3、7、5，put 顺序依次为 5、7、3（JDK 1.7 头插法）。在 mod 2 以后都冲突在 table[1] 这里了。这里假设负载因子 loadFactor=1，即当键值对的实际大小 size 大于 table 的负载（threshold）时进行扩容。接下来的步骤是哈希桶数组 resize 成 4，然后所有的 Node 重新 rehash 的过程。 JDK 1.8中的rehashJDK 1.8 中的 rehash 过程与 JDK 1.7 大同小异，相比 JDK 1.7， 它主要对重新定位元素在哈希表中的位置做了优化： 经过观测可以发现，我们使用的是2次幂的扩展(指长度扩为原来2倍)，所以，元素的位置要么是在原位置，要么是在原位置再移动2次幂的位置。看下图可以明白这句话的意思，n 为 table 的长度，图（a）表示扩容前的 key1 和 key2 两种 key 确定索引位置的示例，图（b）表示扩容后 key1 和 key2 两种 key 确定索引位置的示例，其中hash1是key1对应的哈希与高位运算结果。 table 在扩容之后，因为 n 变为 2 倍，那么 n-1 的 mask 范围在高位多 1bit(红色)，因此新的 index 就会发生这样的变化： 因此，我们在扩充 HashMap 的时候，不需要像 JDK1.7 的实现那样重新计算 hash，只需要看看原来的 hash 值新增的那个 bit 是 1 还是 0 就好了，是 0 的话索引没变，是 1 的话索引变成“原索引 + oldCap”。 了解了 JDK 1.8 相比 JDK 1.7 所做的优化之后，我们再看一下 JDK 1.8 中的 rehash 过程： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061final Node&lt;K,V&gt;[] resize() &#123; ... ... // rehash 的过程 if (oldTab != null) &#123; for (int j = 0; j &lt; oldCap; ++j) &#123; Node&lt;K,V&gt; e; if ((e = oldTab[j]) != null) &#123; // 释放旧 Node 数组的对象引用（for循环后，旧的 Node 数组不再引用任何对象） oldTab[j] = null; // oldTab[j] 只有一个元素，直接进行 rehash if (e.next == null) newTab[e.hash &amp; (newCap - 1)] = e; else if (e instanceof TreeNode) ((TreeNode&lt;K,V&gt;)e).split(this, newTab, j, oldCap); else &#123; // 原索引（头指针与尾指针） Node&lt;K,V&gt; loHead = null, loTail = null; // 原索引 + oldCap（头指针与尾指针） Node&lt;K,V&gt; hiHead = null, hiTail = null; Node&lt;K,V&gt; next; // 对元素进行 rehash 的过程 do &#123; next = e.next; // 原索引（尾插法） if ((e.hash &amp; oldCap) == 0) &#123; if (loTail == null) loHead = e; else loTail.next = e; loTail = e; &#125; // 原索引 + oldCap（尾插法） else &#123; if (hiTail == null) hiHead = e; else hiTail.next = e; hiTail = e; &#125; &#125; while ((e = next) != null); // 将建立的链表放到新 table 数组合适的位置上 if (loTail != null) &#123; loTail.next = null; newTab[j] = loHead; &#125; if (hiTail != null) &#123; hiTail.next = null; newTab[j + oldCap] = hiHead; &#125; &#125; &#125; &#125; &#125; return newTab;&#125; HashMap的线程安全性在多线程使用场景中，应该尽量避免使用线程不安全的 HashMap，而使用线程安全的 ConcurrentHashMap。那么为什么说 HashMap 是线程不安全的，下面举例子说明在并发的多线程使用场景中使用 HashMap 可能造成死循环。代码例子如下(便于理解，仍然使用JDK1.7的环境)： 12345678910111213141516171819public class HashMapInfiniteLoop &#123; private static HashMap&lt;Integer,String&gt; map = new HashMap&lt;Integer,String&gt;(2，0.75f); public static void main(String[] args) &#123; map.put(5， "C"); new Thread("Thread1") &#123; public void run() &#123; map.put(7, "B"); System.out.println(map); &#125;; &#125;.start(); new Thread("Thread2") &#123; public void run() &#123; map.put(3, "A); System.out.println(map); &#125;; &#125;.start(); &#125; &#125; 其中，map 初始化为一个长度为 2 的数组，loadFactor = 0.75，threshold = 2 * 0.75 = 1，也就是说当 put 第二个 key 的时候，map 就需要进行 resize。 通过设置断点让线程1和线程2同时 debug 到 transfer 方法的首行。注意此时两个线程已经成功添加数据。放开 thread1 的断点至 transfer 方法的Entry next = e.next 这一行；然后放开线程2的的断点，让线程2进行 resize。结果如下图。 newTable 是局部变量，所以两个线程都有自己扩容后开辟的新的 table 数组。（对应图中橙色与紫色方块） 注意，由于 Thread1 执行到了Entry next = e.next这一行，因此 e 指向了 key(3)，而 next 指向了 key(7)，其在线程二 rehash 后，指向了线程二重组后的链表（rehash 之后，会将 newtable 赋值给 HashMap 的成员变量 table）。 接着下一部分： 线程一被调度回来执行，先是执行 newTalbe[i] = e（对应图中 thread1 的索引 3 处指向了 thread2 中 索引 3 处的 key = 3 的节点（thread2 中的 table 此时已经是成员变量了，因此共享））， 然后是 e = next，导致了 e 指向了 key(7)，而下一次循环的 next = e.next 导致了 next 指向了 key(3)。 当 next 指向 key(3) 的时候，e 为 key(7)，又经过一次循环后，结果如下图： 虚线也表示有引用指向 key(7)，只不过是想将 thread1 所拥有的 table 与 成员变量 table 区分开。 此时再更新 e 与 next 的值，e 为 key(3)，next 为 null，因此下一次循环就是最后一次循环。经过下一次循环之后，由于 e.next = newTable[i] 导致 key(3).next 指向了 key(7)，而此时的 key(7).next 已经指向了 key(3)，环形链表就此形成。结果如下图： 于是，当我们用线程一调用 map.get(11) 时，悲剧就出现了——无限循环。 博主将这块内容看了好几遍，确实不好理解，如果大家对这部分内容还有任何疑惑的话，欢迎在评论区进行提问~~ 总结 明白静态内部类 Node 的相关实现，清楚 HashMap 的底层实现是有关 Node 的 table 数组（哈希表）。 注意使用 HashMap 时最好使用不变的对象作为 key。 注意 HashMap 计算 key 的 hash 值时，使用了低位与高位异或的方式，返回最终的 hashcode。 了解 HashMap 中的定位方式：(n - 1) &amp; hash。 在 HashMap 中使用链地址法解决冲突，并且当链表的节点个数大于 8 的时候，会转换为红黑树。（JDK 1.8 新特性） JDK 1.8 中使用尾插法进行 put 与 resize，JDK 1.7 中使用头插法进行 put 与 resize。 JDK 1.8 中的 rehash 过程不用重新计算元素的哈希值，因为元素的位置只有两种情况：原位置 与 原位置 + 原本哈希表的长度。 清楚多线程环境下使用 HashMap 可能会造成的一种错误—形成环形链表。 在 Java 8系列之重新认识HashMap 这篇文章中，美团点评技术团队还对 JDK 1.8 与 JDK 1.7 做了性能上的比较，有兴趣的同学可以自行查阅！ 参考阅读Java 8系列之重新认识HashMap—美团点评技术团队 Java HashMap工作原理及实现(二)]]></content>
      <categories>
        <category>Java源码</category>
      </categories>
      <tags>
        <tag>源码剖析</tag>
        <tag>HashMap</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java Web--XiyouLinux Group图书借阅平台的实现]]></title>
    <url>%2F2018%2F03%2F13%2FJava-Web-XiyouLinux-Group%E5%9B%BE%E4%B9%A6%E5%80%9F%E9%98%85%E5%B9%B3%E5%8F%B0%E7%9A%84%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[源码地址：XiyouLinux Group 图书借阅平台 项目地址中包含了一份README，因此对于项目的介绍省去部分内容。这篇博客，主要讲述项目中各个模块的实现细节。 项目概述及成果首先将本项目使用到技术罗列出来： 使用Spring + Spring MVC进行后台开发 使用Bootstrap和jQuery框架进行前端开发 使用自定义注解与自定义的JdbcRowMapper简化JdbcTemplate对数据库的操作 使用腾讯云的对象存储服务进行图书照片的远程存储 使用MD5加密算法对用户密码在后台进行加密存储 使用过滤器进行一个会话中的身份校验 手动从Spring容器中获取bean 数据库设计中的诸多细节… … 由于前端开发是由团队中的其他人在负责，在加上博主对前端这块并不了解，因此本篇博客并不讨论有关第二点技术实现上的细节。 本项目如README中所述，在后期还有许多需要进行优化的地方。如果你对本项目感兴趣，不妨在GitHub中将其Star，以获得对本项目的持续关注～ 至于项目成果大家可以阅读README，我在其中有贴上程序运行后的部分截图。或直接在本地搭建环境，运行此项目。过程中如有任何疑问，你也可以联系我： 1spider_hgyi@outlook.com 关于项目的整体架构我也不再描述，README中对其进行了补充。 项目背景这个项目的产生是有需求背景的。我们旨在为XiyouLinux Group开发一个管理图书借阅与归还的平台，从而能对小组中存在的大量书籍进行有效的管理。 我们的“老一届boss”刚开始给我们提出了第一版的需求，在此需求上，我们最初使用Servlet + JSP的方式进行后台开发。当然第一版由于太low我们对其进行了阉割。在我们学习了Spring与Spring MVC之后，就开始打算对其进行version 2.0的开发，并找来了一个专门学习前端的小可爱，才有了当前的图书借阅平台。 此图书借阅平台实现的功能模块请大家移步至README进行查看。 接下来，我就按照每个模块的顺序，给大家讲一下本项目中用到的重点技术及其实现细节。 实现细节注：博主只会挑几个重点模块去进行讲述，因此有些模块将不会涉及到。 模块一：登录模块登录模块分为三个部分，登录前主页面、登录后主页面以及登录框。 在这里我给大家截一张图看一下登录前后主页面的功能差距： 登录前： 登录后： 我对登录后的页面只截取了和登录前有不同功能的区域。效果展示完毕，那么接下来就谈一谈这个模块中使用到的技术及其实现细节（只需考虑登录后页面实现的功能即可）。 分页功能的实现作为一个展示信息的Web页面，怎么可能没有分页功能呢，只不过是由于上图中的测试数据太少，没有给大家展现出来罢了。我们使用的是传统分页功能，而传统分页中又分为“真分页”与“假分页”： 真分页：每次从数据库中只返回当前页的数据，然后将数据交由视图进行渲染 假分页：从数据库中拿取所有需要或将要展示的数据，将数据交由视图，由视图实现数据的分页功能（JS实现或JSTL实现） 我们也很容易判断出哪种情况下何种方法最优： 如果数据量较小，使用假分页的效果会更优；如果数据量庞大，使用真分页的效果更优。 本项目使用的是“真分页”，我们接下来看一下实现思路与实现代码： 实现思路： 首先我们需要一个存储页面信息的Java Bean，也就是传统的Java对象 使用GET方法进行页面跳转请求，也就是说，我们可以从URL中得到当前页面是第几页 在后台中进行逻辑构造，将Java Bean中的实例字段进行部分（完全）填充 使用Java Bean所提供的页面信息，构造相应的SQL语句，拿到当前页数据 使用TreeMap对数据进行时间维度上的排序，最终返回给视图进行渲染 实现代码： 存储页面信息的Java Bean： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061/** * Created by dela on 12/27/17. */public class PagePO &#123; private int everyPage; // 每页显示记录数 private int totalCount; // 总记录数 private int totalPage; // 总页数 private int currentPage; // 当前页 private int beginIndex; // 查询起始点 private boolean hasPrePage; // 是否有上一页 private boolean hasNexPage; // 是否有下一页 public PagePO() &#123; &#125; public PagePO(int currentPage) &#123; this.currentPage = currentPage; this.everyPage = 5; this.beginIndex = (currentPage - 1) * everyPage; &#125; public PagePO(int currentPage, int everyPage) &#123; this.currentPage = currentPage; this.everyPage = everyPage; this.beginIndex = (currentPage - 1) * everyPage; &#125; ... ... public int getEveryPage() &#123; return everyPage; &#125; public void setEveryPage(int everyPage) &#123; this.everyPage = everyPage; &#125; ... ... public boolean isHasPrePage() &#123; return hasPrePage; &#125; public void setHasPrePage(boolean hasPrePage) &#123; this.hasPrePage = hasPrePage; &#125; ... ... @Override public String toString() &#123; return "PagePO&#123;" + "everyPage=" + everyPage + ", totalCount=" + totalCount + ", totalPage=" + totalPage + ", currentPage=" + currentPage + ", beginIndex=" + beginIndex + ", hasPrePage=" + hasPrePage + ", hasNexPage=" + hasNexPage + '&#125;'; &#125;&#125; 从URL中得到当前页面是第几页，进行逻辑处理，填充上面Java Bean中的部分实例字段： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263/** * Created by dela on 1/21/18. * * @Description: 登录后主页面对应的控制器 */@Controller@RequestMapping("/auth")public class MainController &#123; ... ... @RequestMapping(value = &#123;"", "/", "/page/&#123;currentPagePre&#125;"&#125;, method = RequestMethod.GET) public String getMainPage(Model model, @PathVariable(value = "currentPagePre", required = false) String currentPagePre, @RequestParam(value = "tag", required = false) String labelIdPre) &#123; ... ... // 得到当前页面的页码 int currentPage = 1; if (currentPagePre != null) &#123; currentPage = Integer.parseInt(currentPagePre); &#125; PagePO pagePO = new PagePO(currentPage); ... ... // 得到当前分类下的数据总数（默认无分类） if (labelId == -1) &#123; bookCount = bookInfoService.getBookCount(); &#125; else &#123; bookCount = bookInfoService.getBookCountByLabelId(labelId); &#125; pagePO.setTotalCount(bookCount); pagePO.setTotalPage((bookCount % 5 == 0) ? bookCount / 5 : bookCount / 5 + 1); // 根据页面信息构造SQL语句，拿取当前页的数据 ... ... // 对获取到的信息进行排序（按时间维度） ... ... ... ... /** * 分页在后台中的逻辑处理主要是以下部分： */ // 在这里添加分页的逻辑是因为JSP页面中EL表达式对算数运算的支持不太良好 model.addAttribute("ELPageValue", (currentPage - 1) / 5 * 5); // 当总页数大于5时，需要如下属性 if (pagePO.getTotalPage() &gt;= 6) &#123; model.addAttribute("isOneOfNextFivePage", (pagePO.getTotalPage() - 1) / 5 * 5 + 1); model.addAttribute("reachNextFivePage", (currentPage + 4) / 5 * 5 + 1); &#125; // 当前页面大于等于6页的时候, 需要显示"[...]"按钮--返回到前一个5页 if (currentPage &gt;= 6) &#123; model.addAttribute("returnPreFivePage", (currentPage - 1) / 5 * 5 - 4); &#125; ... ... &#125;&#125; 根据页面信息构造SQL语句，拿取当前页的数据： 123456789101112131415/** * Created by dela on 11/23/17. */@Repositorypublic class BookInfoServiceImpl implements BookInfoService &#123; private JdbcOperations jdbcOperations; private final static String GET_ONE_PAGE_BOOKINFO = "SELECT * FROM book_info WHERE amount &gt; 0 ORDER BY pk_id DESC LIMIT ?, ?"; @Override public List&lt;BookInfoPO&gt; getBookByPage(PagePO page) &#123; return jdbcOperations.query(GET_ONE_PAGE_BOOKINFO, JdbcRowMapper.newInstance(BookInfoPO.class), page.getBeginIndex(), page.getEveryPage()); &#125;&#125; 对获取到的信息进行排序：（按时间维度） 1234567891011public class BookUserMapUtil &#123; public static Map&lt;BookInfoPO, String&gt; getBookInfo(List&lt;BookInfoPO&gt; bookInfoPOS, UserService userService) &#123; ... ... // TreeMap可对数据进行排序，当然BookInfoPO要实现Comparable接口，并重写compareTo方法 Map&lt;BookInfoPO, String&gt; bookMap = new TreeMap&lt;BookInfoPO, String&gt;(); ... ... return bookMap; &#125;&#125; JSP页面中对应的分页实现（JSTL与EL）： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283&lt;!--分页的实现--&gt;&lt;div id="index_pingination"&gt; &lt;ul class="pagination"&gt; &lt;!--当当前页面不是第一页的时候, 要显示 "首页"和 "&lt;&lt;"按钮--&gt; &lt;c:if test="$&#123;pageInfo.currentPage != 1 &amp;&amp; pageInfo.totalPage != 0&#125;"&gt; &lt;c:if test="$&#123;labelId == -1&#125;"&gt; &lt;li&gt; &lt;a href="$&#123;pageContext.request.contextPath&#125;/page/1"&gt;首页&lt;/a&gt;&lt;/li&gt; &lt;li&gt; &lt;a href="$&#123;pageContext.request.contextPath&#125;/page/$&#123;pageInfo.currentPage-1&#125;"&gt;&amp;laquo;&lt;/a&gt;&lt;/li&gt; &lt;/c:if&gt; &lt;c:if test="$&#123;labelId != -1&#125;"&gt; &lt;li&gt; &lt;a href="$&#123;pageContext.request.contextPath&#125;/page/1?tag=$&#123;labelId&#125;"&gt;首页&lt;/a&gt;&lt;/li&gt; &lt;li&gt; &lt;a href="$&#123;pageContext.request.contextPath&#125;/page/$&#123;pageInfo.currentPage-1&#125;?tag=$&#123;labelId&#125;"&gt;&amp;laquo;&lt;/a&gt;&lt;/li&gt; &lt;/c:if&gt; &lt;/c:if&gt; &lt;!--当当前页面大于等于6页的时候, 要显示 "[...]"按钮--返回到前一个5页--&gt; &lt;c:if test="$&#123;pageInfo.currentPage &gt;= 6&#125;"&gt; &lt;c:if test="$&#123;labelId == -1&#125;"&gt; &lt;li&gt; &lt;a href="$&#123;pageContext.request.contextPath&#125;/page/$&#123;returnPreFivePage&#125;"&gt;[...]&lt;/a&gt;&lt;/li&gt; &lt;/c:if&gt; &lt;c:if test="$&#123;labelId != -1&#125;"&gt; &lt;li&gt; &lt;a href="$&#123;pageContext.request.contextPath&#125;/page/$&#123;returnPreFivePage&#125;?tag=$&#123;labelId&#125;"&gt;[...]&lt;/a&gt;&lt;/li&gt; &lt;/c:if&gt; &lt;/c:if&gt; &lt;!--显示当前页面所有应显示的页码--&gt; &lt;c:forEach varStatus="i" begin="$&#123;ELPageValue+1&#125;" end="$&#123;ELPageValue+5&#125;" step="$&#123;1&#125;"&gt; &lt;c:if test="$&#123;i.current &lt;= pageInfo.totalPage&#125;"&gt; &lt;!--当前页的超链接处理为不可点击--&gt; &lt;c:if test="$&#123;i.current == pageInfo.currentPage&#125;"&gt; &lt;li class="pa_in"&gt; &lt;a disabled="true"&gt;$&#123;pageInfo.currentPage&#125;&lt;/a&gt;&lt;/li&gt; &lt;/c:if&gt; &lt;c:if test="$&#123;i.current != pageInfo.currentPage&#125;"&gt; &lt;c:if test="$&#123;labelId == -1&#125;"&gt; &lt;li&gt; &lt;a href="$&#123;pageContext.request.contextPath&#125;/page/$&#123;i.current&#125;"&gt;$&#123;i.current&#125;&lt;/a&gt;&lt;/li&gt; &lt;/c:if&gt; &lt;c:if test="$&#123;labelId != -1&#125;"&gt; &lt;li&gt; &lt;a href="$&#123;pageContext.request.contextPath&#125;/page/$&#123;i.current&#125;?tag=$&#123;labelId&#125;"&gt;$&#123;i.current&#125;&lt;/a&gt;&lt;/li&gt; &lt;/c:if&gt; &lt;/c:if&gt; &lt;/c:if&gt; &lt;/c:forEach&gt; &lt;!--如果不是最后一个五页中的页码, 要在后面显示[...]按钮--跳到下一个5页--&gt; &lt;c:if test="$&#123;pageInfo.currentPage &lt; isOneOfNextFivePage &amp;&amp; pageInfo.totalPage &gt;= 6&#125;"&gt; &lt;c:if test="$&#123;labelId == -1&#125;"&gt; &lt;li&gt; &lt;a href="$&#123;pageContext.request.contextPath&#125;/page/$&#123;reachNextFivePage&#125;"&gt;[...]&lt;/a&gt;&lt;/li&gt; &lt;/c:if&gt; &lt;c:if test="$&#123;labelId != -1&#125;"&gt; &lt;li&gt; &lt;a href="$&#123;pageContext.request.contextPath&#125;/page/$&#123;reachNextFivePage&#125;?tag=$&#123;labelId&#125;"&gt;[...]&lt;/a&gt;&lt;/li&gt; &lt;/c:if&gt; &lt;/c:if&gt; &lt;!--如果不是尾页, 要显示 "&gt;&gt;"和 "尾页"按钮--&gt; &lt;c:if test="$&#123;pageInfo.currentPage != pageInfo.totalPage &amp;&amp; pageInfo.totalPage != 1 &amp;&amp; pageInfo.totalPage != 0&#125;"&gt; &lt;c:if test="$&#123;labelId == -1&#125;"&gt; &lt;li&gt; &lt;a href="$&#123;pageContext.request.contextPath&#125;/page/$&#123;pageInfo.currentPage+1&#125;"&gt;&amp;raquo;&lt;/a&gt;&lt;/li&gt; &lt;li&gt; &lt;a href="$&#123;pageContext.request.contextPath&#125;/page/$&#123;pageInfo.totalPage&#125;"&gt;尾页&lt;/a&gt;&lt;/li&gt; &lt;/c:if&gt; &lt;c:if test="$&#123;labelId != -1&#125;"&gt; &lt;li&gt; &lt;a href="$&#123;pageContext.request.contextPath&#125;/page/$&#123;pageInfo.currentPage+1&#125;?tag=$&#123;labelId&#125;"&gt;&amp;raquo;&lt;/a&gt;&lt;/li&gt; &lt;li&gt; &lt;a href="$&#123;pageContext.request.contextPath&#125;/page/$&#123;pageInfo.totalPage&#125;?tag=$&#123;labelId&#125;"&gt;尾页&lt;/a&gt;&lt;/li&gt; &lt;/c:if&gt; &lt;/c:if&gt; &lt;/ul&gt;&lt;/div&gt; 登录校验之过滤器实现既然系统具有登录功能，那么我们就需要注意一些事情： 怎么防止未登录的用户访问登录后的页面 用户的cookie失效之后，我们需要引导用户进行重新登录 为了解决这两个问题，就需要引入过滤器。关于过滤器的功能与在Serlvet中的使用请移步至这一篇博客：Servlet–Servlet进阶API、过滤器、监听器 我现在要说的是过滤器在Spring框架中的使用，先看实现代码，并不难理解： 12345678910111213141516171819202122232425262728293031323334353637383940/** * Created by dela on 1/18/18. * * @Description: 对于想要在Spring中使用过滤器, 就要继承OncePerRequestFilter * OncePerRequestFilter, 顾名思义, 就是每个请求只通过一次这个过滤器 */public class LoginFilter extends OncePerRequestFilter &#123; protected void doFilterInternal(HttpServletRequest httpServletRequest, HttpServletResponse httpServletResponse, FilterChain filterChain) throws ServletException, IOException &#123; final String INDEX_PAGE = &quot;/&quot;; // 未登录的URL Object sessionId = null; HttpSession session = httpServletRequest.getSession(false); // 未登录和Cookie失效的处理机制 if (session != null) &#123; sessionId = session.getAttribute(&quot;uid&quot;); if (sessionId != null) &#123; filterChain.doFilter(httpServletRequest, httpServletResponse); &#125; &#125; // 如果当前操作的用户没有登录令牌, 那就弹出弹框提示重新登录, 并跳转到未登录页面 if (session == null || sessionId == null) &#123; // 设置response的字符集, 防止乱码 httpServletResponse.setCharacterEncoding(&quot;GBK&quot;); PrintWriter out = httpServletResponse.getWriter(); String builder = &quot;&lt;script language=\&quot;javascript\&quot;&gt;&quot; + &quot;alert(\&quot;网页过期，请重新登录！\&quot;);&quot; + &quot;top.location=&apos;&quot; + INDEX_PAGE + &quot;&apos;;&quot; + &quot;&lt;/script&gt;&quot;; out.print(builder); &#125; &#125;&#125; 有两个问题需要解决～ 1.什么叫做每个请求只通过一次这个过滤器。Filter不都是仅仅经过一次的吗？ 不是的！不然就不会有这个类了。 此方式是为了兼容不同的Web容器，特意而为之，也就是说并不是所有的Web容器都像我们期望的只过滤一次，Servlet版本不同，表现也不同。 如，Servlet2.3与Servlet2.4也有一定差异 ： 在Servlet-2.3中，Filter会过滤一切请求，包括服务器内部使用forward转发请求和&lt;%@ include file=“/index.jsp”%&gt;的情况。 到了Servlet-2.4中Filter默认下只拦截外部提交的请求，forward和include这些内部转发都不会被过滤，但是有时候我们需要forward的时候也要用到Filter。 因此，为了兼容各种不同的运行环境和版本，默认Filter继承OncePerRequestFilter是一个比较稳妥的选择。 2.有关HttpSession session = httpServletRequest.getSession(false)的一点小知识。 现实中我们经常会遇到以下3中用法： 123HttpSession session = request.getSession();HttpSession session = request.getSession(true);HttpSession session = request.getSession(false); 他们之间的区别是什么？ getSession(boolean create)意思是返回当前reqeust中的HttpSession，如果当前request中的HttpSession为null且create为true，就创建一个新的HttpSession，否则就直接返回null。 简而言之： request.getSession(true)等同于如果当前没有HttpSession还要新创建一个HttpSession request.getSession(false)则等同于如果当前没有HttpSession就直接返回null 那么我们在使用的时候： 当向HttpSession中存储登录信息时，一般建议：HttpSession session = request.getSession(true) 当从HttpSession中获取登录信息时，一般建议：HttpSession session = request.getSession(false) 还有一种更简洁的方式： 如果你的项目中使用到了Spring，对Session的操作就方便多了。如果需要在Session中取值，可以用WebUtils工具的getSessionAttribute(HttpServletRequestrequest, String name)方法，看看源码： 123456public static Object getSessionAttribute(HttpServletRequest request, String name) &#123; Assert.notNull(request, "Request must not be null"); HttpSession session = request.getSession(false); return (session != null ? session.getAttribute(name) : null); &#125; 使用时： 12WebUtils.setSessionAttribute(request, “user”, User)；User user = (User) WebUtils.getSessionAttribute(request, “user”); 密码加密之MD5算法也许你没有听过MD5加密算法，但是有些人看到这个标题首先会产生一个疑问：对密码为什么还要加密？ 主要是从安全性的角度上考虑，我们知道如果不对密码进行加密，那么密码将会在后台以明文的形式存储到数据库中。如果你的数据库足够安全，保证不会被别人所侵略，这当然没有什么问题。但事实是，我们不得不小心SQL注入等一系列数据库安全性问题，这时候，在数据库中所存储的有关个人隐私的信息，就显得十分重要了。因此将密码在后台进行加密，对于真正的企业级开发来说，是一件不可或缺的事情。 解决掉这个疑惑之后，让我们一起来看看MD5加密算法的核心思想及代码实现。 好吧，博主看了一些关于MD5的核心思想，并没有看懂，先在这里给大家放一篇讲述MD5加密算法实现原理的博客链接：MD5算法原理 — 博客中有少量错误，大家理性阅读。 关于MD5在Java中的使用，则要简单许多： 通过MessageDigest.getInstance()确定加密算法，MessageDigest不止提供MD5 调用update(byte[] input)对指定的byte数组更新摘要 执行digest()方法进行哈希计算。在调用此方法之后，摘要被重置 对第三步返回的结果进行处理：128位级联值（16组有符号字节值）—&gt;将每组10进制数字转换为16进制，并生成相应字符串 123456789101112131415161718192021222324252627282930313233343536373839/** * @Author: spider_hgyi * @Date: Created in 上午11:53 18-3-11. * @Modified By: * @Description: MD5加密算法 */public class MD5 &#123; public static String codeByMD5(String inStr) &#123; MessageDigest md5; try &#123; // 得到MD5加密算法实例 md5 = MessageDigest.getInstance("MD5"); &#125; catch (Exception e) &#123; e.printStackTrace(); &#125; // 使用指定的byte数组更新摘要 assert md5 != null; md5.update(inStr.getBytes()); // 通过执行诸如填充之类的最终操作完成哈希计算。返回值是16个有符号字节数，共128位 byte[] md5Bytes = md5.digest(); // 用于存储最终得到的32位小写16进制字符串 StringBuilder hexValue = new StringBuilder(); // 将其中的字节转换为16进制字符 for (byte md5Byte : md5Bytes) &#123; // 将得到的有符号字节转换为无符号字节 int val = ((int) md5Byte) &amp; 0xff; if (val &lt; 16) &#123; hexValue.append("0"); &#125; hexValue.append(Integer.toHexString(val)); &#125; return hexValue.toString(); &#125;&#125; 以上代码生成小写16进制字符串，代码运行结果经过本人与在线MD5加密网站生成的结果进行了对比，测试无误，可放心使用。 模块二：标签页模块效果展示： 目录树结构的数据库设计在标签页这一模块中，我们主要对在MySQL数据库中如何存储一个树状结构而进行一个简单的介绍。 我在项目中设计的存储结构并不高效，是一种最简单且基本的实现。在网上有很多结构良好且性能高效的树形结构的数据库表设计，大家可以查阅一些相关资料。 对比上面的效果展示图，我的标签分类其实就是三层树深度： 根节点（唯一） 一级标签（大数据与云计算… …）（多节点） 二级标签（Hadoop、Spark等等）（多节点） 可以看到，虽然树的深度只有3，但其每个父节点都拥有多个子节点。 既然已经将标签信息组织成多路树结构，那么数据库结构设计如下： 1pk_id name parent_id pk_id用来标识此标签名的唯一索引，name就是标签名，parent_id则是此标签其父节点对应的pk_id。 我将一级标签的parent_id都设置为0，表明一级标签的父节点提供空数据，标签页只需要一级标签及二级标签的信息。 如此，我们便可查找任一一级标签信息及其所拥有的二级标签信息。 至于标签页面中的显示形式，我们在后台只要将每个一级标签作为Map数据结构中的键，当前一级标签所拥有的二级标签作为对应的值，然后将Map作为model返回给视图进行解析渲染即可。 代码实现如下： 12345678910111213141516171819202122232425262728293031323334353637/** * @Author: spider_hgyi * @Date: Created in 下午1:36 17-11-20. * @Modified By: * @Description: */@Controller@RequestMapping(&quot;/auth&quot;)public class TagsController &#123; ... ... @RequestMapping(value = &quot;/tags&quot;, method = RequestMethod.GET) public String showLabel(Model model) &#123; // 得到所有的一级标签（parent_id == 0） List&lt;BookLabelPO&gt; parentLabels = bookLabelRepository.getBookLabelByParentId(0); // 得到所有的二级标签（parent_id != 0） List&lt;BookLabelPO&gt; childrenLabels = bookLabelRepository.getChildrenLabelsByParentId(0); // 返回给视图的model Map&lt;String, Map&lt;Integer, String&gt;&gt; labelsName = new HashMap&lt;String, Map&lt;Integer, String&gt;&gt;(); // 找到每个一级标签所拥有的二级标签 for (BookLabelPO parentLabel : parentLabels) &#123; Map&lt;Integer, String&gt; childLabelsName = new HashMap&lt;Integer, String&gt;(); for (BookLabelPO childrenLabel : childrenLabels) &#123; if (parentLabel.getPkId() == childrenLabel.getParentId()) &#123; childLabelsName.put(childrenLabel.getPkId(), childrenLabel.getName()); &#125; &#125; labelsName.put(parentLabel.getName(), childLabelsName); &#125; // 将存储标签信息的Map对象添加进model对象 model.addAttribute(&quot;labelsName&quot;, labelsName); return &quot;alltags&quot;; &#125;&#125; 模块三：上传书籍模块腾讯云存储服务—图片存储由于有些书籍会上传封面照片，而腾讯云又提供了对象存储服务，因此我并没有选择将图片存储至本地或云服务器上，而是使用了腾讯云所提供的云对象存储。 使用云对象存储，腾讯所提供的开发者文档：对象存储 — SDK 文档 手动获取beanSpring MVC给我们提供了文件上传功能（两种使用形式）： 给控制器方法参数上添加@RequestPart注解，参数类型为字节数组 给控制器方法参数上添加@RequestPart注解，参数类型为Part 但是我在使用Spring MVC所提供的文件上传功能时，始终无法获取到对应的字节流对象。我查阅了大量的相关文档，并仔细的检查了所写的代码，最终也没有找到问题的根源。因此在项目中，对于书籍图片的处理，我使用了Servlet所提供的原生API：request.getPart()。 既然使用了Servlet所提供的原生API，因此图书上传模块所对应的控制器便继承于HttpServlet。在继承了HttpServlet之后，还是出现了很多问题—怎么使原生Servlet与Spring MVC的bean之间进行协作？ 在使用了HttpServlet之后，便无法给此Servlet添加@controller注解，也就无法使用依赖注入。大概的原因是Servlet由Web容器管理，而bean由Spring容器管理。在这种情况下，我对bean进行了手动获取。 手动获取bean的代码我写到了Servlet的init方法中，对于此方法我不在这里进行描述。 博主之所以将这一技术细节提取出来，也是想给那些遇到同样问题的朋友们提供一些思路。 代码实现如下： 12345678910111213141516171819202122232425262728293031/** * @Author: spider_hgyi * @Date: Created in 下午8:14 17-12-3. * @Modified By: * @Description: */@WebServlet(urlPatterns = "/auth/upload.do")@MultipartConfigpublic class NewBookController extends HttpServlet &#123; private static final Logger logger = LoggerFactory.getLogger(NewBookController.class); private BookInfoService bookInfoService; private BookLabelService bookLabelService; private BookRelationLabelService bookRelationLabelService; private COSStorage cosStorage; // 手动获取bean public void init() throws ServletException &#123; // 得到Servlet应用上下文 ServletContext servletContext = this.getServletContext(); // 得到Web应用上下文 WebApplicationContext ctx = WebApplicationContextUtils.getWebApplicationContext(servletContext); // 根据beanId获取相应bean bookInfoService = (BookInfoService) ctx.getBean("bookInfoServiceImpl"); bookLabelService = (BookLabelService) ctx.getBean("bookLabelServiceImpl"); bookRelationLabelService = (BookRelationLabelService) ctx.getBean("bookRelationLabelServiceImpl"); cosStorage = (COSStorage) ctx.getBean("cosStorage"); &#125; ... ...&#125; 模块四：对Jdbc RowMapper的简易封装本项目的架构采用Spring + Spring MVC + JdbcTemplate，其中Spring + Spring MVC对应ssm框架中的ss，我们并没有使用Mybatis框架。Spring提供了相应的JDBC框架—JdbcTemplate。 对于JdbcTemplate的使用如下（在使用之前需要进行相关的Spring配置）： 12345678910111213141516171819202122232425262728293031323334353637383940414243/** * Created by hg_yi on 17-11-7. */@Repositorypublic class JdbcSpitterRepository implements SpitterRepository &#123; JdbcOperations jdbcOperations; private final static String INSERT_SPITTER = "INSERT INTO spitter (username, password, " + "firstname, lastname) VALUES (?, ?, ?, ?)"; private final static String QUERY_SPITTER_BY_USERNAME = "SELECT * FROM spitter " + "WHERE username = ?"; @Inject public JdbcSpitterRepository(JdbcOperations jdbcOperations) &#123; this.jdbcOperations = jdbcOperations; &#125; // 数据库插入操作 public Spitter save(Spitter spitter) &#123; jdbcOperations.update(INSERT_SPITTER, spitter.getUsername(), spitter.getPassword(), spitter.getFirstName(), spitter.getLastName()); return spitter; &#125; // 数据库查询操作 public List&lt;Spitter&gt; findByUsername(String username) &#123; return jdbcOperations.query(QUERY_SPITTER_BY_USERNAME, new SpitterRowMapper(), username); &#125; private final static class SpitterRowMapper implements RowMapper&lt;Spitter&gt; &#123; public Spitter mapRow(ResultSet resultSet, int rowNum) throws SQLException &#123; return new Spitter( resultSet.getInt("id"), resultSet.getString("username"), resultSet.getString("password"), resultSet.getString("firstname"), resultSet.getString("lastname") ); &#125; &#125;&#125; 对上述代码有几点说明： JdbcOperations是一个接口，定义了JdbcTemplate所实现的操作。通过注入JdbcOperations从而使JdbcSpitterRepository与JdbcTemplate保持了松耦合 使用RowMapper对Spitter对象进行填充，最后得到从数据库中查询到的结果集合 使用JdbcTemplate极大的方便了对JDBC的操作，没有了创建JDBC连接和语句的代码，也没有了异常处理的代码，只剩下单纯的数据插入与查询代码 那么我们为何还要对RowMapper进行封装？ 由上面的代码可知，每当我们从相同（不同）的数据库表中得到不同的数据时，就有可能创建不同的RowMapper。那么问题就凸显出来了，我们的系统中必定有多张数据库表，也必定要从各个表中查询不同的数据，那么就会创建大量不同的RowMapper类，这些RowMapper散落于项目中的各个角落。这样的设计，显然很失败。 我们自己封装的JdbcRowMapper（与Spring所提供的RowMapper所区分）有什么功能呢？ 我们尝试对RowMapper进行封装，以提供这样的功能：对于不同的对象，RowMapper在从数据库中查询到相应的数据之后，都可对其相应的字段进行自动填充。 我们先来看一下它的使用效果： 1234jdbcOperations.query(GET_BOOK_BY_LABEL_AND_PAGE_TYPESCONTROLLER, JdbcRowMapper.newInstance(BookInfoPO.class), labelId,pagePO.getBeginIndex(), pagePO.getEveryPage()); jdbcOperations.query(QUERY_CHILDREN_LABELS_BY_PARENT_ID, JdbcRowMapper.newInstance(BookLabelPO.class), parentId); 可以看到，我们不必再为不同的PO对象编写不同的RowMapper。 现在开始分析它的具体实现： 根据上述代码，我们先来分析它的newInstance方法： 123public static &lt;T&gt; JdbcRowMapper&lt;T&gt; newInstance (Class&lt;T&gt; mappedClass) &#123; return new JdbcRowMapper&lt;T&gt;(mappedClass);&#125; 这是一个泛型方法，返回值是泛型类：JdbcRowMapper&lt;T&gt;，方法参数是泛型Class对象。这个方法调用了JdbcRowMapper如下的构造方法： 123public JdbcRowMapper(Class&lt;T&gt; mappedClass) &#123; initialize(mappedClass);&#125; 继续跟踪，initialize方法：（核心方法之一） initialize方法的作用： 在说initialize方法的作用之前，我们先要知道什么是PO。之前我所使用的Java Bean为什么都以PO为后缀？简单来说，这是Java Bean与持久化层之间的一层规约。这层规约可以简单的概述为：数据库中表字段的命名方式都以下划线分割单词，而Java Bean中则是以驼峰式命名，并且，每个PO对象基本对应一张数据库表。就拿BookInfoPO中的private int pkId属性来说，它对应的就是数据库表book_info中的pk_id字段。这里涉及到了数据库建表时的规范，我们之后再说。目前你就先这样记住。 有了这层规约，我们在封装RowMapper的时候，就可以通过一些逻辑代码，将Java Bean中的实例字段名转换为数据库表中相应的字段名，也就为我们的下一个方法：把从数据库表中读取到的数据填充到Java Bean中的相应字段做了铺垫。 initialize方法的实现思路： 通过BeanUtils.getPropertyDescriptor()得到当前JavaBean(mappedClass对应的PO)的PropertyDescriptor数组 对PropertyDescriptor数组进行遍历，拿到每一个实例变量的变量名 对变量名做相应转换，转为对应的数据库表字段名 将这些名字保存在合适的数据结构中，供接下来的mapRow方法使用（JdbcRowMapper中真正从数据库中读取所需数据的方法） 有了实现思路，那么接下来看代码实现： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253protected void initialize(Class&lt;T&gt; mappedClass) &#123; // 以下三个变量都是实例变量，在这里进行初始化 this.mappedClass = mappedClass; this.mappedFileds = new HashMap&lt;String, PropertyDescriptor&gt;(); this.mappedProperties = new HashSet&lt;String&gt;(); /** * 通过BeanUtils.getPropertyDescriptor()得到当前JavaBean(mappedClass对应的PO)的PropertyDescriptor数组，PropertyDescriptors类是Java内省类库的一个类。 * Java JDK中提供了一套API用来访问某个对象属性的getter/setter方法，这就是内省。 */ // 获取bean的所有属性（也就是实例变量）列表 PropertyDescriptor[] propertyDescriptors = BeanUtils.getPropertyDescriptors(mappedClass); // 遍历属性列表 for (PropertyDescriptor propertyDescriptor : propertyDescriptors) &#123; // propertyDescriptor.getWriteMethod()获得用于写入属性值的方法 if (propertyDescriptor.getWriteMethod() != null) &#123; // 得到此属性名（变量名） String name = propertyDescriptor.getName(); try &#123; // 通过反射取得Class里名为name的字段信息 Field field = mappedClass.getDeclaredField(name); if (field != null) &#123; // 得到该属性(field)上存在的注解值（下一个代码给出示例） Column column = field.getAnnotation(Column.class); // 如果取得的column值不为null, 那就给name赋值column.name if (column != null) &#123; name = column.name(); &#125; &#125; &#125; catch (NoSuchFieldException e) &#123; e.printStackTrace(); &#125; // 将&lt;属性名字, 属性&gt;加入mappedFileds中 this.mappedFileds.put(lowerCaseName(name), propertyDescriptor); // 不使用自定义注解，使用代码将所得name转换为对应数据库表字段 String underscoredName = underscoreName(name); // 如果两个不等，则将nderscoredName也添加进mappedFileds，相当于一种容错机制 if (!lowerCaseName(name).equals(underscoredName)) &#123; this.mappedFileds.put(underscoredName, propertyDescriptor); &#125; // 将属性名添加至mappedProperties this.mappedProperties.add(name); &#125; &#125;&#125; 这就是initialize方法。接下来看一下其中所用到的自定义注解，也就是对这一行代码的解释：Column column = field.getAnnotation(Column.class) 定义自定义注解：123456789101112131415161718/** * @Author: dela * @Date: * @Modified By: * @Description: @Retention是JDK的元注解, 当RetentionPolicy取值为RUNTIME的时候, * 意味着编译器将Annotation记录在class文件中, 当Java文件运行的时候, * JVM也可以获取Annotation的信息, 程序可以通过反射获取该Annotation的信息. */ @Retention(RetentionPolicy.RUNTIME)@Target(ElementType.TYPE)// @Target也是JDK的一个元注解, 当ElementType取不同值的时候, 意味着这个注解的作用域也不同,// 比如, 当ElementType取TYPE的时候, 说明这个注解用于类/接口/枚举定义public @interface Table &#123; // 数据库中表的名字 String name();&#125; 12345678910/** * Created by dela on 12/20/17. */@Retention(RetentionPolicy.RUNTIME)@Target(ElementType.FIELD)public @interface Column &#123; // 数据库中的表上的字段的名字 String name();&#125; 自定义注解在BookInfoPO中的应用： 12345678910111213141516171819202122232425262728/** * Created by dela on 11/22/17. */// 书籍信息表@Table(name = "book_info")public class BookInfoPO implements Comparable&lt;BookInfoPO&gt; &#123; @Column(name = "pk_id") private int pkId; // 无意义主键 @Column(name = "ugk_name") private String ugkName; // 书名(组合索引) @Column(name = "author") private String author; // 作者 @Column(name = "ugk_uid") private int ugkUid; // 所有者(即用户表里的id)(组合索引) @Column(name = "amount") private int amount; // 数量 @Column(name = "upload_date") private String uploadDate; // 上传时间 @Column(name = "book_picture") private String bookPicture; // 书籍照片 @Column(name = "describ") private String describ; // 书籍描述 public BookInfoPO() &#123; &#125; ... ...&#125; 在initialize方法中还有一个underscoreName()：（此方法就不打注解了） 1234567891011121314151617181920protected String underscoreName(String name) &#123; if (!StringUtils.hasLength(name)) &#123; return ""; &#125; StringBuilder result = new StringBuilder(); result.append(lowerCaseName(name.substring(0, 1))); for (int i = 1; i &lt; name.length(); i++) &#123; String s = name.substring(i, i + 1); String slc = lowerCaseName(s); if (!s.equals(slc)) &#123; result.append("_").append(slc); &#125; else &#123; result.append(s); &#125; &#125; return result.toString();&#125; Ok，接下来我们继续探究核心方法二：mapRow() 刚说过initialize方法是为了使mapRow方法可以把从数据库表中读取到的结果填充到Java Bean相应的字段上而做的一个铺垫。那么mapRow必定实现了如下功能： 从数据库表中读取结果集 将结果集中的元素填充到相应的Java Bean中（别忘了initialize方法已经帮我们将Java Bean中的实例变量名转换为了数据库表中相应的字段名） 明白了mapRow中实现的大致功能，那么我们直接来看源码： 123456789101112131415161718192021222324252627282930313233343536373839404142434445public T mapRow(ResultSet resultSet, int rowNumber) throws SQLException &#123; // Spring的断言表达式, 传入的Java Bean的Class对象不能为空 Assert.state(this.mappedClass != null, "Mapped class was not specified"); // 实例化一个Java Bean T mappedObject = BeanUtils.instantiate(this.mappedClass); // BeanWrapper可以设置及访问被包装对象的属性值 BeanWrapper beanWrapper = PropertyAccessorFactory.forBeanPropertyAccess(mappedObject); // 从resultSet中拿到有关此数据库表的元数据（字段名称、类型以及数目等） ResultSetMetaData resultSetMetaData = resultSet.getMetaData(); // 得到此数据库表的字段数目 int columnCount = resultSetMetaData.getColumnCount(); for (int index = 1; index &lt;= columnCount; index++) &#123; // 得到数据库表中当前字段名 String column = JdbcUtils.lookupColumnName(resultSetMetaData, index); String field = lowerCaseName(column.replaceAll(" ", "")); // 根据数据库表中的字段名拿到Java Bean中对应实例字段属性的描述 PropertyDescriptor propertyDescriptor = this.mappedFileds.get(field); if (propertyDescriptor != null) &#123; try &#123; // 得到该field所对应的数据库表中字段所对应的值（下一个代码给出示例） Object value = getColumnValue(resultSet, index, propertyDescriptor); ... ... try &#123; // 将得到值填充到Java Bean中相应的实例变量上 beanWrapper.setPropertyValue(propertyDescriptor.getName(), value); &#125; catch (TypeMismatchException ex) &#123; ... ... &#125; &#125; catch (NotWritablePropertyException ex) &#123; ... ... &#125; &#125; else &#123; // 没有发现相应实例字段的属性描述 ... ... &#125; &#125; return mappedObject; &#125; getColumnValue()的源码如下：1234// 得到数据库表中字段(column)对应的值(value)protected Object getColumnValue(ResultSet resultSet, int index, PropertyDescriptor propertyDescriptor) throws SQLException &#123; return JdbcUtils.getResultSetValue(resultSet, index, propertyDescriptor.getPropertyType());&#125; 设计数据库由于博主负责了本项目的数据库设计，因此在这里有一点心得想分享给大家。 首先是MySQL的建表规范（当然并不绝对）： 主键一律无意义，就算有意义，也必须是以后不会被更新，修改并且是自增的字段。命名规范一律是pk_id,数据类型为int unsigned,字段not null。 唯一索引命名一律以uk_为前缀，唯一索引并不以提高查询速率为主要目的，主要是进行唯一性约束。 唯一组合索引命名一律以ugk_为前缀，目的同上，注意最左前缀的问题。由于主键一律设置的是无意义的自增字段，所以对于有外键约束的字段，只设置了级联删除（只更新父表的主键会存在外键约束）。 日期字段的数据类型一律为datetime。 所有表的字段设置为not null，数字默认值为0,字符串默认值为’’，datetime没有设置默认值，因此在后台必须处理时间问题。 当初在设计本项目的数据库时分别使用了主键与外键约束、唯一索引与组合索引、级联更新与级联删除等技术。对于这些技术的讲解在博主所置顶的几篇博客中就可以看到，因此不再讲解。 至于数据库结构与数据的SQL文件，在本人GitHub的README中有提供，感兴趣的可以去下载，源码地址在本篇博客开始已经给出～ Ok，XiyouLinux Group图书借阅平台的实现分析至此结束！]]></content>
      <categories>
        <category>后端开发</category>
      </categories>
      <tags>
        <tag>项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络--详解DNS域名系统]]></title>
    <url>%2F2018%2F03%2F05%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-%E8%AF%A6%E8%A7%A3DNS%E5%9F%9F%E5%90%8D%E7%B3%BB%E7%BB%9F%2F</url>
    <content type="text"><![CDATA[注：本篇博客大部分内容截选自阮一峰老师的DNS 原理入门一文。其中少部分内容是博主自己的理解。 什么是DNS我们知道，网络本身只能理解数字形式的地址，也就是IP地址。但是直观的IP地址毫无规律，很难让人记住，并且如果使用IP地址浏览一个公司的主页，意味着这家公司一旦将主页移动到了另一台机器上，且该机器具有不同的IP地址，那么必须将该机器的IP地址通知给每一个人。因此人们引入了类似于www.baidu.com这样的域名。而要将域名转换为对应的IP地址，就需要DNS服务器（Domain Name System）。 在早期的ARPANET时代，只有一个简单的hosts.txt文件，它列出了所有的计算机名字和其对应的IP地址。每天晚上，所有的主机都从一个维护此文件的站点将该文件取回，然后在本地进行更新。对于一个拥有几百台大型分时机器的网络而言，这种方法工作的相当好。 然而当几百万台PC连接到互联网以后，问题就出现了。首先这个文件会变的非常大，并且主机名冲突的现象将会频繁发生。为了解决这些问题，DNS服务器应运而生。 注：DNS服务器和域名服务器同义。 查询过程虽然只需要返回一个IP地址，但是DNS的查询过程非常复杂，分成多个步骤。 工具软件dig可以显示整个查询过程。 1dig math.stackexchange.com 上面的命令会输出六段信息： ;;开头的表示这一行是注释。 1. 第一段是查询参数和统计。可以看到dig命令的一些基本信息，如版本和参数说明。还有一些对查询结果的简单统计： 2. 第二段是查询内容： 上面结果表示，查询域名math.stackexchange.com的A记录，A是address的缩写，也就是查询域名的IP地址。 3. 第三段是DNS服务器的答复： 我们将上述图片中的每一行记录称为域名资源记录，DNS数据库就是由这些记录所构成。最常见的资源记录就是它的IP地址，但除此之外还有许多其他种类的资源记录。当解析器把一个域名传给DNS时，它能获得的DNS返回结果就是与该域名相关联的资源记录。 因此，DNS的基本功能是将域名映射至资源记录。 我们来看一下资源记录的格式：五元组。 1Domain_name Time_to_live Class Type Value Domain_name（域名）：这条资源记录属于哪一个域 Time_to_live（生存期）：TTL值，表示缓存时间，在上图中就是600秒之内不用重新查询 Class（类别）：对于Internet信息，它总是IN。对于非Internet信息，则可以使用其他的代码，但实际很少见 Type（类型）：指出了本条资源记录是什么样的类型。DNS有许多类型，我们在DNS的记录类型进行详细讨论 Value：可以是数字、域名、ASCII字符串，其取决于资源记录的类型 那么上面结果就显示，math.stackexchange.com有四个A记录，即四个IP地址。600是TTL值，表示缓存时间，即600秒之内不用重新查询。 4. 第四段显示stackexchange.com的NS记录（Name Server的缩写），即哪些服务器负责管理stackexchange.com的DNS记录： 上面结果显示stackexchange.com共有四条NS记录，即四个域名服务器，向其中任一台查询就能知道math.stackexchange.com的IP地址是什么。 这里有一些分级查询的内容，我们稍后在讲。我们此时应该注意为什么stackexchange.com有四台域名服务器？ 在理论上，一台域名服务器就足以。但实际上，这台服务器有可能会因负载过重而变得毫无用处。而且，一旦它停机，则域名必然会解析失败。这就是单个信息源所带来的问题。 5. 第五段是上面四个域名服务器的IP地址，这是随着前一段一起返回的： 6. 第六段是DNS服务器的一些传输信息： 上面结果显示，本机的DNS服务器是192.168.1.253，查询端口是53（DNS服务器的默认端口），以及回应长度是305字节。 如果不想看到这么多内容，可以使用+short参数： 123456dig +short math.stackexchange.com151.101.129.69151.101.65.69151.101.193.69151.101.1.69 上面命令只返回math.stackexchange.com对应的4个IP地址（即A记录）。 DNS服务器下面我们根据前面这个例子，一步步还原，本机到底怎么得到域名math.stackexchange.com的IP地址。 首先，本机一定要知道DNS服务器的IP地址，否则上不了网。通过DNS服务器，才能知道某个域名的IP地址到底是什么。 除了本地DNS服务器。有一些公网的DNS服务器，也可以使用，其中最有名的就是Google的8.8.8.8和Level 3的4.2.2.2。 本机只向自己的DNS服务器查询，dig命令有一个@参数，显示向其他DNS服务器查询的结果。 1dig @4.2.2.2 math.stackexchange.com 上面命令指定向DNS服务器4.2.2.2查询。 域名层级DNS服务器怎么会知道每个域名的IP地址呢？答案是分级查询。 请仔细看前面的例子，每个域名的尾部都多了一个点。 比如，域名math.stackexchange.com显示为math.stackexchange.com.。这不是疏忽，而是所有域名的尾部，实际上都有一个根域名。 举例来说，www.example.com真正的域名是www.example.com.root，简写为www.example.com.。因为，根域名.root对于所有域名都是一样的，所以平时是省略的。 根域名的下一级，叫做”顶级域名”（top-level domain，缩写为TLD），比如.com、.net；再下一级叫做”次级域名”（second-level domain，缩写为SLD），比如www.example.com里面的.example，这一级域名是用户可以注册的（可以了解一下域名抢注问题）；再下一级是主机名（host），比如www.example.com里面的www，又称为&quot;三级域名&quot;，这是用户在自己的域里面为服务器分配的名称，是用户可以任意分配的。 总结一下，域名的层级结构如下： 123主机名.次级域名.顶级域名.根域名host.sld.tld.root 根域名服务器DNS服务器根据域名的层级，进行分级查询。 需要明确的是，每一级域名都有自己的NS记录，NS记录指向该级域名的域名服务器。这些服务器知道下一级域名的各种记录。 所谓“分级查询”，就是从根域名开始，依次查询每一级域名的NS记录，直到查到最终的IP地址，过程大致如下： 从根域名服务器查到顶级域名服务器的NS记录和A记录（IP地址） 从顶级域名服务器查到次级域名服务器的NS记录和A记录（IP地址） 从次级域名服务器查出主机名的IP地址 仔细看上面的过程，你可能发现了，没有提到DNS服务器怎么知道根域名服务器的IP地址。回答是根域名服务器的NS记录和IP地址一般是不会变化的，所以内置在本地DNS服务器里面。 下面是内置的根域名服务器IP地址的一个例子： 上面列表中，列出了根域名（.root）的三条NS记录A.ROOT-SERVERS.NET、B.ROOT-SERVERS.NET和C.ROOT-SERVERS.NET，以及它们的IP地址（即A记录）198.41.0.4、192.228.79.201、192.33.4.12。 另外，可以看到所有记录的TTL值是3600000秒，相当于1000小时。也就是说，每1000小时才查询一次根域名服务器的列表。 目前，世界上一共有十三组根域名服务器，从A.ROOT-SERVERS.NET一直到M.ROOT-SERVERS.NET。 分级查询实例dig命令的+trace参数可以显示DNS的整个分级查询过程。 1dig +trace math.stackexchange.com 上面命令的第一段列出根域名.的所有NS记录，即所有根域名服务器： 根据内置的根域名服务器IP地址，DNS服务器向所有这些IP地址发出查询请求，询问math.stackexchange.com的顶级域名服务器com.的NS记录。最先回复的根域名服务器将被缓存（缓存到本地DNS服务器上），以后只向这台服务器发请求。 接着是第二段： 上面结果显示.com域名的13条NS记录，同时返回的还有每一条记录对应的IP地址。 然后，DNS服务器向这些顶级域名服务器发出查询请求，询问math.stackexchange.com的次级域名stackexchange.com的NS记录： 上面结果显示stackexchange.com有四条NS记录，同时返回的还有每一条NS记录对应的IP地址。 然后，DNS服务器向上面这四台NS服务器查询math.stackexchange.com的主机名： 上面结果显示，math.stackexchange.com有4条A记录，即这四个IP地址都可以访问到网站。并且还显示，最先返回结果的NS服务器是ns-463.awsdns-57.com，IP地址为205.251.193.207。 在分级查询的过程中，还有2个技术要点值得讨论。 递归查询与迭代查询我们再来补充一些概念： 域名解析：查询一个域名和找出其对应地址的过程 本地DNS服务器：参考知乎—本地DNS服务器的作用 先抛开分级查询不说，DNS的使用方法大致如下：为了将一个域名映射成IP地址，应用程序调用一个名为解析器的库程序，并将域名作为参数传递给此程序。然后解析器向本地DNS服务器发送一个包含该名字的请求报文；本地DNS服务器查询该名字，并且返回一个包含该名字对应IP地址的响应报文给解析器，然后解析器再将IP地址返回给调用方。 但是如果我们要查询的域名在远端，即本地DNS服务器没有相关域名的缓存信息，那么域名服务器就会进行一次远程查询，而远程查询的过程，则对应我们上面所说的分级查询。图表形式如下： 递归查询 主机向本地域名服务器的查询一般都是采用递归查询。 所谓递归查询就是：如果主机所询问的本地域名服务器不知道被查询的域名的IP地址，那么本地域名服务器就以DNS客户的身份，向其它域名服务器继续发出查询请求报文(即替主机继续查询)，而不是让主机自己进行下一步查询。因此，递归查询返回的查询结果或者是所要查询的IP地址，或者是报错，表示无法查询到所需的IP地址。 迭代查询 当根域名服务器收到本地域名服务器发出的迭代查询请求报文时，要么给出所要查询的IP地址，要么告诉本地服务器：“你下一步应当向哪一个域名服务器进行查询”。然后让本地服务器进行后续的查询。根域名服务器通常是把自己知道的顶级域名服务器的IP地址告诉本地域名服务器，让本地域名服务器再向顶级域名服务器查询。顶级域名服务器在收到本地域名服务器的查询请求后，要么给出所要查询的IP地址，要么告诉本地服务器下一步应当向哪一个次级域名服务器进行查询… …最后，知道了所要解析的IP地址或报错，然后把这个结果返回给发起查询的主机。 由上图可知，DNS域名系统同时涉及了两种机制。如果采用单一的迭代查询方式，则查询过程如下： 可以看到，如果使用单一的迭代查询，DNS客户端将变的异常繁忙，CPU资源被抢占，用户体验将会下降。 通过结合使用递归查询与迭代查询，将DNS查询的重担交给本地DNS服务器，客户端就可以在DNS查询的过程中干自己想干的事情。 总结： 递归：客户端只发一次请求，要求对方给出最终结果。 迭代：客户端发出一次请求，对方如果没有授权回答，它就会返回一个能解答这个查询的其它名称服务器列表，客户端会再向返回的列表中发出请求，直到找到最终负责所查域名的名称服务器，从它得到最终结果。 从递归和迭代查询可以看出： 客户端—本地DNS服务端：这部分属于递归查询。 本地dns服务端—外网：这部分属于迭代查询。 DNS的记录类型域名与IP之间的对应关系，称为”记录”（record）。根据使用场景，“记录”可以分成不同的类型（type），前面已经看到了有A记录和NS记录。 常见的DNS记录类型如下： A：地址记录（Address），返回域名指向的IPv4地址。 AAAA：地址记录（Address），返回域名指向的IPv6地址。 NS：域名服务器记录（Name Server），返回保存下一级域名信息的服务器地址。该记录只能设置为域名，不能设置为IP地址。 MX：邮件记录（Mail eXchange），返回接收电子邮件的服务器地址。 CNAME：规范名称记录（Canonical Name），返回另一个域名，即当前查询的域名是另一个域名的跳转，详见下文。 PTR：逆向查询记录（Pointer Record），只用于从IP地址查询域名，详见下文。 一般来说，为了服务的安全可靠，至少应该有两条NS记录，而A记录和MX记录也可以有多条，这样就提供了服务的冗余性，防止出现单点失败。 CNAME记录主要用于域名的内部跳转，为服务器配置提供灵活性，用户感知不到。举例来说，facebook.github.io这个域名就是一个CNAME记录： 1234567dig facebook.github.io...;; ANSWER SECTION:facebook.github.io. 3370 IN CNAME github.map.fastly.net.github.map.fastly.net. 600 IN A 103.245.222.133 上面结果显示，facebook.github.io的CNAME记录指向github.map.fastly.net。也就是说，用户查询facebook.github.io的时候，实际上返回的是github.map.fastly.net的IP地址。这样的好处是，变更服务器IP地址的时候，只要修改github.map.fastly.net这个域名就可以了，用户的facebook.github.io域名不用修改。 由于CNAME记录就是一个替换，所以域名一旦设置CNAME记录以后，就不能再设置其他记录了（比如A记录和MX记录），这是为了防止产生冲突。举例来说，foo.com指向bar.com，而两个域名各有自己的MX记录，如果两者不一致，就会产生问题。由于顶级域名通常要设置MX记录，所以一般不允许用户对顶级域名设置CNAME记录。 PTR记录用于从IP地址反查域名。dig命令的-x参数用于查询PTR记录： 123456dig -x 192.30.252.153...;; ANSWER SECTION:153.252.30.192.in-addr.arpa. 3600 IN PTR pages.github.com. 逆向查询的一个应用，是可以防止垃圾邮件，即验证发送邮件的IP地址，是否真的有它所声称的域名。 dig命令可以查看指定的记录类型： 123dig a github.comdig ns github.comdig mx github.com 总结 熟悉DNS的查询过程：分级查询； 熟悉分级查询中，递归查询与迭代查询的概念及使用场景； 熟悉本地DNS服务器的作用，了解DNS缓存机制； 掌握资源记录的格式：五元组； 掌握域名层级的概念：根域名、顶级域名、次级域名、主机名； 了解DNS的记录类型。 参考阅读计算机网络（第五版）— Andrew S.Tanenbaum、David J.Wetherall DNS原理入门 — 阮一峰 DNS递归查询与迭代查询 — 皈依之路 本地DNS服务器的作用 — ALEXIRC DNS缓存服务器配置详解 — long9617 例解DNS递归/迭代名称解析原理 — 茶乡浪子]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>DNS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java网络爬虫--多线程爬虫（抓取淘宝商品详情页URL）]]></title>
    <url>%2F2018%2F03%2F02%2FJava%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB-%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%88%AC%E8%99%AB%EF%BC%88%E6%8A%93%E5%8F%96%E6%B7%98%E5%AE%9D%E5%95%86%E5%93%81%E8%AF%A6%E6%83%85%E9%A1%B5URL%EF%BC%89%2F</url>
    <content type="text"><![CDATA[源码地址：多线程爬虫–抓取淘宝商品详情页URL 项目地址中包含了一份README，因此对于项目的介绍省去部分内容。这篇博客，主要讲述项目的构建思路以及实现细节。 项目概述及成果首先将本项目使用到技术罗列出来： MySQL数据库进行数据持久化及对宕机情况的发生做简单的处理 Redis数据库做IP代理池及部分已抓取任务的缓存 自制IP代理池 使用多线程执行任务（同步块，读写锁，等待与通知机制，线程优先级） HttpClient与Jsoup的使用 序列化与反序列化 布隆过滤器 之后会对其中使用到的技术进行详细的解释。 本项目如README中所述，还有许多不完善的地方，但IP代理池与任务抓取线程之间的调度与协作基本已无问题。也就是说，在此项目的框架上，如果你想修改其中代码用作其他抓取任务，也是完全可以的。我抓取到的数据所保存的源文件也放在GitHub的README上供大家免费浏览与下载（近90000的商品ID）。 整体思路 首先你需要一个IP代理池 使用本机IP将淘宝中基本的商品分类抓取下来 页面源链接：https://www.taobao.com/tbhome/page/market-list 从页面源链接中解析到的URL形如下：https://s.taobao.com/search?q=羽绒服&amp;style=grid 将诸如此类的URLhttps://s.taobao.com/search?q=羽绒服&amp;style=grid作为任务队列，使用多线程对其进行抓取与解析（使用代理IP）,解析的内容为第4点 我们需要分析每一种类的商品在淘宝中大概具有多少数量，为此我解析出带有页面参数的URL，在第3点中URL的基础上：https://s.taobao.com/search?q=羽绒服&amp;style=grid&amp;s=44，在浏览器中打开URL可发现此页面为此种类衣服的第二页 我们得到了每一种商品带有页面参数的URL，意味着我们可以得到此类商品中全部或部分的商品ID，有了商品ID，我们就可以进入商品详情页抓取我们想要的数据了 为了实现第5点，我们先将第4点中抓取到的URL全部存储进MySQL中 从MySQL中将待抓取URL全部取出，存储到一个队列中，使用多线程对此共享队列进行操作，使用代理IP从待解析URL中解析出本页面中包含的商品ID，并构建商品详情页URL 在第7点中解析商品ID的时候，同时使用布隆过滤器，对重复ID进行过滤，并将已经抓取过的URL任务放入Redis缓存中，等达到合适的阈值时，将存储在MySQL中对应的URL行记录中的flag置为true，表示此URL已经被抓取过，等到下一次重启系统，可以不用对此URL进行抓取 实现细节（省略大量实现代码，如有需要请阅读源码）IP代理池我们先从IP代理池说起，在这个项目中所运用到的IP代理池与我在Java网络爬虫（十一）–重构定时爬取以及IP代理池（多线程+Redis+代码优化）这一篇博客中所讲述的IP代理池的实现思想有一些细小的差别。 差别1：不再使用定时更新IP代理池的方法 由于是将IP代理池真正的运用到一个工程中，因此定时更新IP代理池的方法已经不可取。我们的IP代理池作为一个生产者，众多线程都要使用其中的代理IP，我们就可以认为这些线程都为消费者，根据多线程中经典的生产者与消费者模型，在没有足够的产品供消费者使用的时候，生产者就应该开始进行生产。也就是说，IP代理池的更新变为，当池中已经没有足够的代理IP供众多线程使用的时候，IP代理池就应该开始进行更新。而在IP代理池进行更新的时候，众多线程作为消费者，也只能等待。 具体的代码实现如下： 12// 创建生产者（ip-proxy-pool）与消费者（thread-tagBasicPageURL-i）等待/通知机制所需的对象锁Object lock = new Object(); 生产者—IP代理池：1234567891011121314151617181920212223242526272829/** * Created by hg_yi on 17-8-11. * * @Description: IP代理池的整体构建逻辑 */public class MyTimeJob extends TimerTask &#123; // IP代理池线程是生产者，此锁用来实现等待/通知机制，实现生产者与消费者模型 private final Object lock; MyTimeJob(Object lock) &#123; this.lock = lock; &#125; @Override public void run() &#123; ... ... // 如果IP代理池中没有IP信息，则IP代理池进行工作 while (true) &#123; while (myRedis.isEmpty()) &#123; synchronized (lock) &#123; ... ... lock.notifyAll(); &#125; &#125; &#125; &#125;&#125; 消费者—thread-tagBasicPageURL-i： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546/** * @Author: spider_hgyi * @Date: Created in 下午1:01 18-2-1. * @Modified By: * @Description: 得到带有分页参数的主分类搜索页面的URL */public class TagBasicPageCrawlerThread implements Runnable &#123; private final Object lock; // 有关生产者、消费者的锁 ... ... public TagBasicPageCrawlerThread(Queue&lt;String&gt; tagBasicUrls, Object lock, Queue&lt;String&gt; tagBasicPageUrls, Object taskLock) &#123; this.tagBasicUrls = tagBasicUrls; this.lock = lock; this.tagBasicPageUrls = tagBasicPageUrls; this.taskLock = taskLock; &#125; @Override public void run() &#123; ... ... // 此flag用于---&gt;如果IP可以进行抓取，则一直使用此IP，不在IP代理池中重新拿取新IP的逻辑判断 boolean flag = true; // 每个URL用单独的代理IP进行分析 while (true) &#123; if (flag) &#123; synchronized (lock) &#123; while (myRedis.isEmpty()) &#123; try &#123; System.out.println("当前线程：" + Thread.currentThread().getName() + ", " + "发现ip-proxy-pool已空, 开始进行等待... ..."); lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; ipMessage = myRedis.getIPByList(); &#125; &#125; ... ... &#125; &#125;&#125; 从上面的代码中，我们可以清楚的看到等待/通知机制的经典范式： 等待方（伪代码）： 123456synchronized(对象) &#123; while(条件不满足) &#123; 对象.wait(); &#125; 对应的逻辑处理&#125; 通知方（伪代码）： 1234synchronized(对象) &#123; 改变条件 对象.notifyAll();&#125; 关于等待/通知机制更详细的使用，参考这篇博客：Java线程之间的通信(等待/通知机制) 差别2：不再给每个线程分配固定数目的任务。将任务放在共享队列中，供线程使用 在重构IP代理池的那一版本中，我将待抓取任务平分给了多个线程，每个线程将自己拿到的那些任务执行完毕即可。在将IP代理池运用到工程中的时候，我并没有那样做，而是维护了一个任务队列，每个线程都可以在这个任务队列中取任务，直到队列为空为止。这就改善了在多个线程平分任务的这种情况下，由于一个线程需要完成多个任务，而这多个任务间不是并发执行的缺点。 具体的代码实现如下（我们只需要注意其中的saveIP方法，方法参数urls就是共享任务队列）： 1234567891011121314151617181920212223242526272829303132/** * Created by hg_yi on 17-8-11. * * @Description: 抓取xici代理网的分配线程 * 抓取不同页面的xici代理网的html源码，就使用不同的代理IP，在对IP进行过滤之后进行合并 */public class CreateIPProxyPool &#123; ... ... public void saveIP(Queue&lt;String&gt; urls, Object taskLock) &#123; ... ... while (true) &#123; /** * 随机挑选代理IP(本步骤由于其他线程有可能在位置确定之后对ipMessages数量进行 * 增加，虽说不会改变已经选择的ip代理的位置，但合情合理还是在对共享变量进行读写的时候要保证 * 其原子性，否则极易发生脏读) */ ... ... // 任务队列是共享变量，对其的读写必须进行正确的同步 synchronized (taskLock) &#123; if (urls.isEmpty()) &#123; System.out.println("当前线程：" + Thread.currentThread().getName() + ", 发现任务队列已空"); break; &#125; url = urls.poll(); &#125; ... ... &#125; &#125;&#125; IP代理池在项目中是如何对抗反爬虫的我在使用IP代理池对抗反爬虫的时候，对IP代理池还做了些许改变：修改了IPMessage类结构。看过我关于IP代理池项目博客的同学应该清楚IPMessage这个类是做什么的，就是用来存储有关代理IP信息的。类结构如下： 12345678910111213141516171819202122232425262728293031323334353637/** * Created by hg_yi on 17-8-11. * * @Description: IPMessage JavaBean */public class IPMessage implements Serializable &#123; private static final long serialVersionUID = 1L; private String IPAddress; private String IPPort; private String IPType; private String IPSpeed; private int useCount; // 使用计数器，连续三十次这个IP不能使用，就将其从IP代理池中进行清除 public IPMessage() &#123; this.useCount = 0; &#125; public IPMessage(String IPAddress, String IPPort, String IPType, String IPSpeed) &#123; this.IPAddress = IPAddress; this.IPPort = IPPort; this.IPType = IPType; this.IPSpeed = IPSpeed; this.useCount = 0; &#125; public int getUseCount() &#123; return useCount; &#125; public void setUseCount() &#123; this.useCount++; &#125; public void initCount() &#123; this.useCount = 0; &#125; ... ...&#125; 可以看到，我给其中添加了useCount这一成员变量。我在使用xici代理网上的IP时发现，大部分的代理IP一次不能使用并不代表每次都不可使用，因此我在用代理IP进行网页抓取时的策略作出了如下的改变： 当前代理IP如果解析当前任务失败，则将此代理IP中的useCount变量进行加1，并将此代理IP进行序列化之后，重新丢进IP代理池，切换至其他代理IP 如果当前代理IP解析当前任务成功，则将此代理IP中的useCount变量置0，并且继续使用此代理对其它任务进行抓取，直到任务解析失败，然后重复第1点 如果发现从IP代理池中取出的代理IP的useCount变量数值已为30，则对此代理IP进行舍弃，并切换至其他代理IP 具体的代码实现如下： 舍弃代理IP，flag用于判断是否需要从IP代理池中拿取新的IP： 1234567891011121314151617181920212223242526272829303132333435363738394041/** * @Author: spider_hgyi * @Date: Created in 下午4:25 18-2-6. * @Modified By: * @Description: 负责解析带有页面参数的商品搜索页url，得到本页面中的商品id */public class GoodsDetailsUrlThread implements Runnable &#123; private final Object lock; // 用于与 ip-proxy-pool 进行协作的锁 ... ... @Override public void run() &#123; ... ... boolean flag = true; while (true) &#123; if (flag) &#123; synchronized (lock) &#123; while (myRedis.isEmpty()) &#123; try &#123; System.out.println("当前线程：" + Thread.currentThread().getName() + ", " + "发现ip-proxy-pool已空, 开始进行等待... ..."); lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; ipMessage = myRedis.getIPByList(); &#125; &#125; if (ipMessage.getUseCount() &gt;= 30) &#123; System.out.println("当前线程：" + Thread.currentThread().getName() + ", 发现此ip：" + ipMessage.getIPAddress() + ":" + ipMessage.getIPPort() + ", 已经连续30次不能使用, 进行舍弃"); continue; &#125; ... ... &#125; &#125;&#125; 当前代理IP解析任务成功（失败），useCount置0（++），并持续使用此代理IP抓取新任务（将代理IP丢进IP代理池并拿取新IP）： 123456789101112131415161718192021222324252627282930313233343536373839404142434445/** * @Author: spider_hgyi * @Date: Created in 下午4:25 18-2-6. * @Modified By: * @Description: 负责解析带有页面参数的商品搜索页url，得到本页面中的商品id */public class GoodsDetailsUrlThread implements Runnable &#123; ... ... @Override public void run() &#123; ... ... while (true) &#123; if (flag) &#123; synchronized (lock) &#123; while (myRedis.isEmpty()) &#123; try &#123; System.out.println("当前线程：" + Thread.currentThread().getName() + ", " + "发现ip-proxy-pool已空, 开始进行等待... ..."); lock.wait(); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125; ipMessage = myRedis.getIPByList(); &#125; &#125; ... ... if (html != null) &#123; ... ... flag = false; &#125; else &#123; // 当前任务解析失败，将当前任务重新放入任务队列中，并将flag置为true synchronized (tagBasicPageUrls) &#123; tagBasicPageUrls.offer(tagBasicPageUrl); &#125; flag = true; &#125; &#125; &#125;&#125; 12345678910111213141516171819202122232425262728293031323334353637383940/** * Created by hg_yi on 17-5-23. * * @Description: 对淘宝页面的请求，得到页面的源码 * setConnectTimeout：设置连接超时时间，单位毫秒. * setSocketTimeout：请求获取数据的超时时间，单位毫秒.如果访问一个接口， * 多少时间内无法返回数据，就直接放弃此次调用。 */public class HttpRequest &#123; // 成功抓取淘宝页面计数器 public static int pageCount = 0; // 使用代理IP进行网页的获取 public static String getHtmlByProxy(String requestUrl, IPMessage ipMessage, Object lock) &#123; ... ... try &#123; ... ... // 得到服务响应状态码 if (statusCode == 200) &#123; ... ... &#125; else &#123; ... ... &#125; // 只要能返回状态码，没有出现异常，则此代理IP就可使用 ipMessage.initCount(); &#125; catch (IOException e) &#123; ... ... ipMessage.setUseCount(); synchronized (lock) &#123; myRedis.setIPToList(ipMessage); &#125; &#125; finally &#123; ... ... &#125; return html; &#125;&#125; 布隆过滤器在这篇博客中，详细的介绍了布隆过滤器的实现原理：海量URL去重之布隆过滤器，我在将布隆过滤器应用到项目中的时候，有些方法发生了改变。 之所以将布隆过滤器在这里单独提出来，是因为想给大家提供自己之前写的有关布隆过滤器的实现原理。搞清楚原理之后，大家再看项目中布隆过滤器的相关实现，也就会轻松许多。 监控线程—tagBasicPageURLs-cache这个线程的主要作用是将Redis数据库中缓存的，已经成功解析过的任务，将其对应MySQL中所在的行记录中的flag位设置为true。在前面也说了，我将任务队列保存在了MySQL数据库中，其中对应的每一条记录，都有一个额外的标志位，flag。设置这一标志位的主要目的是，对爬虫系统做了一个简单的宕机恢复。我们应当对已经抓取过的任务做一定的标记手段，以防止在系统突然死机或其他突发状况下，需要重启项目的情况。这个时候，我们当然不可能对所有的任务重新进行抓取。 对于这个问题的处理，我在项目中的实现思路如下： 在任务抓取线程：thread-GoodsDetailsUrl-i，主要用来解析商品ID的线程中，如果抓取完一个任务，就将这个任务先缓存到Redis数据库中，毕竟如果直接将这个任务在MySQL中所在的行记录中的flag置为true的话，效率就有点低下了 设置监控线程：tagBasicPageURLs-cache，监控缓存在Redis数据库中已抓取过任务的数量，我设置的阈值是大于等于100，当然这个数字不绝对，因为线程调度是不可控的。但为了接近我所设置的这个阈值，我将此线程的优先级设置为最高 监控线程开始工作，期间使用同步块保证任务抓取线程不得给Redis数据库中添加新的已经抓取成功的任务，以达到监控线程与任务抓取线程对Redis数据库操作之间的互斥性 具体的代码实现如下： 监控线程—tagBasicPageURLs-cache： 1234567891011121314151617181920212223242526272829303132333435363738394041/** * @Author: spider_hgyi * @Date: Created in 上午11:51 18-2-6. * @Modified By: * @Description: 处理缓存的线程，将 tag-basic-page-urls 中存在的url标记进MySQL数据库中 */public class TagBasicPageURLsCacheThread implements Runnable &#123; private final Object tagBasicPageURLsCacheLock; public TagBasicPageURLsCacheThread(Object tagBasicPageURLsCacheLock) &#123; this.tagBasicPageURLsCacheLock = tagBasicPageURLsCacheLock; &#125; public static void start(Object tagBasicPageURLsCacheLock) &#123; Thread thread = new Thread(new TagBasicPageURLsCacheThread(tagBasicPageURLsCacheLock)); thread.setName(&quot;tagBasicPageURLs-cache&quot;); thread.setPriority(MAX_PRIORITY); // 将这个线程的优先级设置最大，允许出现误差 thread.start(); &#125; @Override public void run() &#123; MyRedis myRedis = new MyRedis(); MySQL mySQL = new MySQL(); while (true) &#123; synchronized (tagBasicPageURLsCacheLock) &#123; while (myRedis.tagBasicPageURLsCacheIsOk()) &#123; System.out.println(&quot;当前线程：&quot; + Thread.currentThread().getName() + &quot;, &quot; + &quot;准备开始将 tag-basic-page-urls-cache 中的url在MySQL中进行标记&quot;); List&lt;String&gt; tagBasicPageURLs = myRedis.getTagBasicPageURLsFromCache(); System.out.println(&quot;tagBasicPageURLs-size: &quot; + tagBasicPageURLs.size()); // 将MySQL数据库中对应的url标志位置为true mySQL.setFlagFromTagsSearchUrl(tagBasicPageURLs); &#125; &#125; &#125; &#125;&#125; 任务抓取线程—thread-GoodsDetailsUrl-i：（截取了部分代码） 1234567... ...// 将tagBasicPageUrl写进Redis数据库synchronized (tagBasicPageURLsCacheLock) &#123; System.out.println("当前线程：" + Thread.currentThread().getName() + "，准备将tagBasicPageUrl写进Redis数据库，tagBasicPageUrl：" + tagBasicPageUrl); myRedis.setTagBasicPageURLToCache(tagBasicPageUrl);&#125;... ... MyRedis中的tagBasicPageURLsCacheIsOk()方法： 12345678// 判断 tagBasicPageURLs-cache 中的url数量是否达到100条public boolean tagBasicPageURLsCacheIsOk() &#123; tagBasicPageURLsCacheReadWriteLock.readLock().lock(); Long flag = jedis.llen("tag-basic-page-urls-cache"); tagBasicPageURLsCacheReadWriteLock.readLock().unlock(); return flag &gt;= 100;&#125; 其实，我为什么会称自己对宕机情况的发生做了简单的处理：这个解决方案并不完美，可以说存在很大的瑕疵。 我在将已经缓存至Redis数据库中，并解析完成的任务URL通过监控线程—tagBasicPageURLs-cache进行MySQL中相关标志位置true的时候，设置的是当Redis数据库中缓存的任务数量达到100及以上的时候，这个监控线程才会启动。 那么就会出现一种情况：Redis数据库中的URL数量没有达到100及以上，这个时候系统发生宕机，那么这些已经抓取过的URL在MySQL中所对应的flag标志位就不会被置为true。也就是说，在我们下次重新启动该系统的时候，这些已经抓取过的URL还会被重新抓取，并且每次存在的误差并无法严格判定，有可能没有误差，有可能误差达到了百条左右。 针对这个bug，目前博主还没有想到比较好的解决办法，相信日后会攻破它。]]></content>
      <categories>
        <category>Java网络爬虫</category>
      </categories>
      <tags>
        <tag>爬虫</tag>
        <tag>项目</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM--深入剖析继承与多态实现原理（合集篇）]]></title>
    <url>%2F2018%2F01%2F12%2FJVM-%E6%B7%B1%E5%85%A5%E5%89%96%E6%9E%90%E7%BB%A7%E6%89%BF%E4%B8%8E%E5%A4%9A%E6%80%81%E5%AE%9E%E7%8E%B0%E5%8E%9F%E7%90%86%EF%BC%88%E5%90%88%E9%9B%86%E7%AF%87%EF%BC%89%2F</url>
    <content type="text"><![CDATA[由于我将继承与多态的讲解分成了三篇博客，所以在这篇博客给出三篇博客总的链接，阅读顺序由第一篇至第三篇就可以。 第一篇：主要讲解分派：JVM–详解虚拟机字节码执行引擎之静态链接、动态链接与分派 第二篇：主要讲解invokevirtual指令：JVM–从JVM层面深入解析对象实例化、多态性实现机制 第三篇：主要讲解继承与方法表：JVM–再谈继承与多态 相信这三篇博客会给你带来惊喜~~~]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>多态性</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM--再谈继承与多态]]></title>
    <url>%2F2018%2F01%2F10%2FJVM-%E5%86%8D%E8%B0%88%E7%BB%A7%E6%89%BF%E4%B8%8E%E5%A4%9A%E6%80%81%2F</url>
    <content type="text"><![CDATA[此文试图从JVM层面深刻剖析Java中的继承与多态，知识面覆盖class字节码文件，对象的内存布局，JVM的内存区域、分派，方法表等相关知识，内容整合于大量博客，知乎，书籍，并加上博主自己的理解，相信看完会对你大有裨益！ 即使博主在JVM专栏已经有两篇博客对多态的实现机制进行了分析，但是今天在分析了一波继承的原理之后，发觉之前对于多态的讲述还不完整，在查阅的相关资料之后，决定在这一篇博客真正的将继承与多态讲透彻！ 注：本篇博客有部分内容摘抄自：从JVM角度看Java多态。表示感谢~ 先来看一份代码： 12345678910111213141516171819202122232425262728293031323334353637class Parent &#123; protected int age; public Parent() &#123; age = 40; &#125; void eat() &#123; System.out.println("父亲在吃饭"); &#125;&#125;class Child extends Parent &#123; protected int age; public Child() &#123; age = 18; &#125; void eat() &#123; System.out.println("孩子在吃饭"); &#125; void play() &#123; System.out.println("孩子在打CS"); &#125;&#125;public class TestPolymorphic &#123; public static void main(String[] args) &#123; Parent c = new Child(); c.eat();// c.play(); System.out.println("年龄：" + c.age); &#125;&#125; 运行结果： 12孩子在吃饭年龄：40 并且如果我在代码中没有将c.play()进行注释的话，将会编译错误。 对于这些结果，我会在随后给大家进行说明。我将以问答的形式来组成这篇博客的架构。随着问题的深入这些疑惑都会被解决。 类之间的继承，都继承了哪些东西？既然要谈多态，就不能绕开继承。那就从继承开始讲起。很经典也很值得思考的问题，子类从父类上都继承了哪些东西？在类的字节码文件中是怎么体现的呢？实例化后在内存中又是怎么体现的呢？ 从语言层面上分析我们先来说清楚子类到底都继承了父类的哪些东西，当然这都是语言层面上的继承，不涉及它的具体实现： 所有的东西，所有的父类的成员，包括变量（静态变量）、常量和方法（存储在方法表中），都成为了子类的成员，除了构造方法。构造方法是父类所独有的，因为它的名字就是类的名字，所以父类的构造方法在子类中不存在。除此之外，子类继承得到了父类所有的成员。 网上有些博客给出，子类没有继承父类的private成员，这种说法是错误的。我们只能说子类不能覆盖且访问父类的private变量，所以当我们试图在子类中覆盖或访问父类的private变量的时候，编译器会给我们报错，但这并不意味着子类并没有继承父类的private变量与常量（隐藏了而已）。 从字节码文件上分析也许你会凭借上面所述的子类会继承父类的一切东西（除了构造器）而感觉在子类的字节码文件中也会包含父类的所有属性和方法。很遗憾，这种想法并不正确。先不说上面的例子，我们知道在Java中所有的类都默认继承自Object，你可以尝试使用javap命令编译一个普通类的class文件，看看其产生的字节码文件中是否含有Object中默认定义的方法信息，好比toString，equals方法等，如果你并没有重写这些方法的话。 那么在字节码文件中是如何表示两个类之间的继承关系呢？如果你对class文件熟悉的话，应该知道字节码中含有字段表集合，方法表集合与父类索引和接口索引集合。 字段表集合用于描述接口或类中声明的变量。方法表用于描述接口或类中所定义的方法。而父类索引与接口索引（implement也是一种继承）则是用来确定这个类的继承关系。父类索引用两个u2类型（表示两个字节）的索引值表示，它指向一个类型为CONSTANT_Class_info的类描述符常量，这个类型常量存储于字节码的常量池中，通过CONSTANT_Class_info类型常量中的索引值可以找到定义在CONSTANT_Utf8_info类型常量中的全限定名字符串。CONSTANT_Utf8_info在常量池中表示的就是UTF-8编码的字符串，也就是父类的名称。而接口索引的索引表之前还有一个接口计数器，也是u2类型的，之所以有计数器，我们也知道，在Java中，类都是单根继承，但是可以同时操作多个接口。索引表的内容则和父类索引相似，就不再赘述了。 因此在子类的字节码文件中，它的字段表集合中不会列出从基类或父接口中继承而来的字段。与字段表相对应，如果父类方法在子类中没有被重写，方法表集合中也不会出现来自父类的方法信息。我们在语言层面上所使用的继承，对应到字节码文件中，只不过是子类的字节码文件中含有父类的索引罢了，父类中的属性，方法都是通过这个索引找到指定的父类从而解析出来的。至于怎么找，怎么解析，则是类加载器与类加载机制部分的知识了，我在JVM专栏的相关博客中也有说明。 实例化后从内存上分析首先问大家一个问题：创建子类对象的时候，会一同创建父类的对象吗？ 我没有查阅过官方文档，但是我在网络上搜索了大量的相关资料，并且与学长也进行了讨论，我目前偏向于，我觉得的确也是这样设计的：创建子类对象的时候不会一同创建父类的对象。 在知乎上，对这个问题进行了激烈的探讨：java中，创建子类对象时，父类对象会也被一起创建么？ 首先我先说支撑自己观点的原因： 引用一下知乎网友的回答： new指令开辟空间，用于存放对象的各个属性，方法等，反编译字节码你会发现只有一个new指令，所以开辟的是一块空间，一块空间就放一个对象。然后，子类调用父类的属性，方法啥的，那并不是一个实例化的对象。并且如果一个子类继承自抽象类或着接口，那么实例化这个子类的时候还能实例化抽象类与接口不成？ 而像一些博客与书籍所说的“子类对象的一部分空间存放的是父类对象”，我觉得这涉及到对象的内存布局，等下在说这个问题。 现在解答一下上面代码中的部分运行结果吧：c.eat()。我之前已经写了两篇关于多态的文章，具体的链接我不再贴出，直接在我的JVM专栏中寻找就可以。看过我前两篇博客的读者对这个代码的运行结果应该不会有太大的疑惑，也就是我们前面讲述的那些动态分派与invokevirtual指令，但是在这篇博客中，对于多态的实现性机制，我还要再阐述一个关于虚方法表的概念。 在《深入理解Java虚拟机》这本书中，关于多态的实现机制也是讲述了这三方面的内容，我之所以将三个东西分开讲，是觉得没有前面两篇博客的沉淀，这三个东西还真的是不好串起来。当初博主看这部分内容的时候是一种似懂非懂的状态，完全对这个三个东西没有明确的认识，我昨天对这三个东西做了如下总结，觉得大概可以将多态的实现机制概括清楚： 动态分派能够让我们从语言层面正确辨析重写（多态），我觉得它是Java语义上多态的实现； invokevirtual指令则是对动态分派这个概念在JVM层面上功能的具体实现，即在JVM中是用怎样一种逻辑实现了动态分派。明白了这个指令，感觉也就体现了多态实现代码中的实现逻辑； 虚方法表则是支撑着invokevirtual指令的实现，我们知道invokevirtual指令代表了递归查找当前栈帧操作数栈顶上引用所代表的实际类型的过程，而虚方法表的实现就是让invokevirtual指令有地方可查。 而且《深入理解Java虚拟机》一书中，也称虚方法表是“虚拟机动态分派”的实现。由此可见虚方法表对于多态的重要意义。 说了这么多，到底什么是虚方法表呢？ 虚方法表一般在类加载的连接阶段进行初始化，准备了类的变量初始值之后，虚拟机会把该类的虚方法表也初始化完毕。虚方法表存储在方法区，虚方法表中存放的都是类中的实例方法，不会存储静态方法，因为静态方法属于非虚方法，会在类加载的解析阶段就将符号引用解析为直接引用，因此不需要虚方法表。关于非虚方法的描述请参考这篇博客：JVM–详解虚拟机字节码执行引擎之静态链接、动态链接与分派。 虚方法表中的这些直接引用会指向JVM中相关类Class对象相应的方法信息上，当然这只是本类的方法，表中还有父类的方法，相应地指向父类类型Class对象的具体位置。 如果与上述代码对应的话，应该是这样： 如上图所示，Parent，Child都没有重写来自Object的方法，所以它们的方法表中所有从Object继承来的方法都指向了Object的数据类型。然后再各自指向本类中方法所存在的数据类型。但是这里有两点需要注意： 如果子类重写了父类的方法，如上面中的eat方法，则子类方法表中的地址将会替换为指向子类实现版本的入口地址，对应至上图就是父类中有属于自己的eat方法入口地址，子类也有属于自己的eat方法入口地址。因此invokevirtual指令才能正确的找到重写方法后的地址入口。 我们从上图中可以看出，相同的方法，在子类和父类的虚方法表中都具有一样的索引序号，这主要是为了程序实现上的方便，因为当实际类型发生变化时，仅需要变更查找的方法表，就可以从不同的虚方法表中按索引转换出所需的入口地址。 好了，如果将此篇博客中的虚方法表和前两篇博客中的动态分派与invokevirtual指令的查找过程完全弄明白的话，我觉的在理论方面你的多态已经算是完全没有问题了，如果你还想更加深入，我觉得无非就是看JVM中多态的实现源码了。 谈到这，我觉得c.eat()方法的运行结果不用我说你们也完全明白了吧。 那么接着上面所遗留的一个问题，对象的内存布局，解决掉这个东西，c.play()为什么会编译错误以及System.out.println(&quot;年龄：&quot; + c.age)等于40的真相也将慢慢浮上水面。 以下内容引入自知乎用户：祖春雷 Java对象的内存布局是由对象所属的类确定。也可以这么说，当一个类被加载到虚拟机中时，由这个类创建的对象的布局就已经确定下来了。 Hotspot中Java对象的内存布局： 每个Java对象在内存中都由对象头和对象体组成。 对象头是存放对象的元信息，包括该对象所属类对象Class的引用以及hashcode和monitor的一些信息。关于对象头的介绍，这篇博客有些许说明JVM–详解创建对象与类加载的区别与联系。 对象体主要存放的是Java对象自身的实例域以及从父类继承过来的实例域，并且内部布局满足以下规则（从我所标出的重点来看，创建子对象的时候，确实不是真正意义上的同时创建一个基类对象）： 规则1：任何对象都是8个字节为粒度进行对齐的。规则2：实例域按照如下优先级进行排列：长整型和双精度类型；整型和浮点型；字符和短整型；字节类型和布尔类型，最后是引用类型。这些实例域都按照各自的单位对齐。规则3：不同类继承关系中的实例域不能混合排列。首先按照规则2处理父类中的实例域，接着才是子类的实例域。规则4：当父类中最后一个成员和子类第一个成员的间隔如果不够4个字节的话，就必须扩展到4个字节的基本单位。规则5：如果子类第一个实例域是一个双精度或者长整型，并且父类并没有用完8个字节，JVM会破坏规则2，按照整形（int），短整型（short），字节型（byte），引用类型（reference）的顺序，向未填满的空间填充。 还是以一个例子说明一下： 12345678910class Parent &#123; private short six; private int age;&#125;class Sub extend Parent &#123; private String name; private int age; private float price;&#125; 当前Sub对象的内存布局由下： 但是这些东西还不足以解释为什么上述代码中c.play()会报错以及为什么System.out.println(&quot;年龄：&quot; + c.age)的答案是40。继续往下看。 我们需要注意这一句代码：Parent c = new Child()，可以发现，c的实际类型虽然是Child，但它的静态类型却是Parent，问题就出在了静态类型上！ 学了这么长时间的Java，博主一直没有搞懂静态类型存在的真实意义，在网上查到的都是以面向对象的思想给你解释为什么Java中存在实际类型的同时还要存在静态类型，而没有从根本上说明静态类型到底会对变量产生什么样的影响。 博主目前查阅到的设计静态类型的真正作用有如下两点（也许还有更多）： Java的类型检查机制是静态类型检查 规定了引用能够访问内存空间的大小 对于第一点，不是本文的重点，直接给大家贴一篇相关博客深入分析Java的静态类型检查。 我们直接来讨论第二点。 我们都知道在C中有void类型的指针，而给指针前面限定一个类型就限制了指针访问内存的方式，比如char *p就表示p只能一个字节一个字节地访问内存，但是int *p中p就必须四个字节四个字节地访问内存。但是我们都知道指针是不安全的，其中一个不安全因素就是指针可能访问到没有分配的内存空间，也就是说char *p虽然限制了p指针访问内存的方式，但是没有限制能访问内存的大小，这一点要完全靠程序员自己掌握。 但是在Java中的静态类型不但指定了以何种方式访问内存，也规定了能够访问内存空间的大小。 对应于刚开始贴出得代码： 我们看Parent实例对象的大小是占两行，但Child实例对象占三行（这里就是简单量化一下）。 如下图： 所以虽然引用c指向的是Child实例对象，但是前面有Parent修饰它，它也只能访问两行的数据，也就是说c根本访问不到Child类中的age！！！只能访问到Parent类的age，所以输出40。你也可以对照着我上面贴出的“Sub对象的内存布局”那张图来对刚开始贴出的代码进行分析。 而且我们注意两个类的方法表： 我们看到Parent的方法表占三行，Child的方法表占4行，c虽然指向了Child类的实例对象，而对象中也有指针指向Child类的方法表，但是由于c受到了Parent的修饰，通过c也只能访问到Child方法表中前3行的内容！！！！因此c.play()编译会出错。就是这个原因，它在方法表中根本找不到play方法。 前面说过，在方法表的形成过程中，子类重写的方法会覆盖掉表中原来的数据，也就是Child类的方法表的第三行是指向Child.eat的引用，而不是指向Parent.eat（因为方法表产生了覆盖），所以c访问到的是Child.eat。也就是子类的方法（这也是作为多态的一种解释，比invokevirtual指令更加深入）！！！这种情况下，c是没有办法直接访问到父类的eat方法的。 好了，本篇博客的内容已结束，对开头的代码也做出了完整的解释。但是我们还是有一些地方没有涵盖，比如super关键字。对于super关键字的使用，我觉得如果你已经将我写的三篇有关于多态的博客吸收与消化，那么，对于super关键字的使用与基本理解，应该是没有问题的，至于对它的深入研究，我们以后再说~~~ 参考阅读《深入理解Java虚拟机》—周志明 JAVA基础探究：子类与父类 从JVM角度看Java多态 java中，创建子类对象时，父类对象会也被一起创建么？]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>多态性</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM--从JVM层面深入解析对象实例化、多态性实现机制]]></title>
    <url>%2F2018%2F01%2F08%2FJVM-%E4%BB%8EJVM%E5%B1%82%E9%9D%A2%E6%B7%B1%E5%85%A5%E8%A7%A3%E6%9E%90%E5%AF%B9%E8%B1%A1%E5%AE%9E%E4%BE%8B%E5%8C%96%E3%80%81%E5%A4%9A%E6%80%81%E6%80%A7%E5%AE%9E%E7%8E%B0%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[之前一直觉得对于字节码的执行过程，对象的实例化过程，多态的实现机制没有进行深刻的探讨，只是进行了简单的总结，一直也苦于没有找到恰当的例子，所幸今天看到一前辈的博客，对其进行钻研之后，终于解决了这个历史遗留问题。 首先贴出前辈的原文链接，并且这篇博客会引用其中的一些内容：Java重写方法与初始化的隐患 问题的还原先来看一份代码： 123456789101112public class SuperClass &#123; private int mSuperX; public SuperClass() &#123; setX(99); &#125; public void setX(int x) &#123; mSuperX = x; &#125;&#125; 1234567891011121314151617public class SubClass extends SuperClass &#123; private int mSubX = 1; public SubClass() &#123;&#125; @Override public void setX(int x) &#123; super.setX(x); mSubX = x; System.out.println("mSubX is assigned " + x); &#125; public void printX() &#123; System.out.println("mSubX = " + mSubX); &#125;&#125; 最后在main里调用： 123456public class Main &#123; public static void main(String[] args) &#123; SubClass sc = new SubClass(); sc.printX(); &#125;&#125; 如果你认为答案是这样的： 12mSubX is assigned 99mSubX = 99 那么请继续往下看，因为真实的答案是这样的： 12SubX is assigned 99SubX = 1 实际分析方法重写所产生的影响及其JVM层面的原因我觉得首先要给大家说一件非常重要的事情： SuperClass构造器中的这个方法调用，事实会调用重写后的方法，也就是SubClass中的setX方法。 1234567891011 public SuperClass() &#123; setX(99); &#125;// 事实调用的是 SubClass 中的setX方法@Override public void setX(int x) &#123; super.setX(x); mSubX = x; System.out.println("SubX is assigned " + x); &#125; 要想知道发生了什么，最简单的方法就是看看到底程序到底是怎么执行的，比如单步调试，或者直接一点，看看Java字节码。 下面是Main的字节码： 12345678910Compiled from "Main.java"public class bugme.Main &#123; ...... public static void main(java.lang.String[]); Code: 0: new #2 // class bugme/SubClass 3: dup 4: invokespecial #3 // Method bugme/SubClass."&lt;init&gt;":()V ...... &#125; 这段代码首先new一个SubClass实例, 把引用入栈, dup是把栈顶复制一份再入栈, invokespecial # 3将栈顶元素出栈并调用它的某个方法, 这个方法具体是什么要看常量池里第3个条目是什么, 但是javap生成的字节码直接给我们写在旁边了, 即SubClass.&lt;init&gt;。 接下来看SubClass.&lt;init&gt;： 123456public class bugme.SubClass extends bugme.SuperClass &#123; public bugme.SubClass(); Code: 0: aload_0 1: invokespecial #1 // Method bugme/SuperClass."&lt;init&gt;":()V ...... 好了，先看到这，我们来解决几个问题： new指令之后为什么需要dup指令（操作数栈中为什么会有两个指向SubClass的引用） &lt;init&gt;方法是什么 首先来解决第一个问题： 分析一下mian方法的执行顺序吧： 1) 其中new指令在java堆上为SubClass对象分配内存空间，并将指向其地址的引用压入操作数栈顶；2) 然后dup指令为复制操作数栈顶值，并将其压入栈顶，也就是说此时操作数栈上有连续相同的两个引用；3) invokespecial指令调用实例初始化方法&lt;init&gt;:()V，所以需要从操作数栈顶弹出一个this引用，也就是说这一步会弹出一个之前入栈的引用；4) sc.printX()也需要从操作数栈顶取出一个引用类型的值，进行使用；5) 最后由return指令结束方法。 main方法后面的字节码没有贴出，大家可以使用javap命令进行查看。 从上面的五个步骤中可以看出，需要从栈顶弹出两个实例对象的引用，这就是为什么会在new指令下面有一个dup指令，其实对于每一个new指令来说一般编译器都会在其下面生成一个dup指令，这是因为实例的初始化方法肯定需要用到一次，然后第二个留给程序员使用，例如给变量赋值，调用方法，抛出异常等，如果我们不用，那编译器也会生成dup指令，在初始化方法调用完成后再从栈顶pop出来。 再来解决第二个问题： 我曾经在JVM的其他篇章讲述过&lt;clinit&gt;，如果你对类构造器还不是很清楚，可以翻翻我以前的JVM相关博客或Baidu一下相关资料。 如果你清楚&lt;clinit&gt;，那么&lt;init&gt;与其是相类似的，其名为实例构造器，其实对于实例构造器，我们在之前也做过相关的介绍，但我还是要再次总结。 首先要清楚，我们平常所说的对象的构造方法实际上只是&lt;init&gt;的一个真子集。这是Java帮我们合成的一个方法, 里面的指令会帮我们按顺序进行普通成员变量初始化, 也包括初始化块里的代码, 注意是按顺序执行, 这些都执行完了之后才轮到构造方法里代码生成的指令执行。 但是一般来说，我们都是将成员变量的初始化放在构造方法中，所以&lt;init&gt;事实上就是将实例代码块中的代码放在对超类构造方法的调用语句之后（super方法），对象自身的构造方法之前合并所产生的一块代码。 对&lt;init&gt;方法的介绍在这篇博客中也有：JVM–详解创建对象与类加载的区别与联系 因此我们平常所记忆的关于对象实例化的顺序是这样：父类&lt;clinit&gt; —&gt; 子类&lt;clinit&gt; —&gt; 父类实例块代码 —&gt; 父类构造方法 —&gt; 子类实例块代码 —&gt; 子类构造方法 现在我们可以对其实例化的顺序进行简化：父类&lt;clinit&gt; —&gt; 子类&lt;clinit&gt; —&gt; 父类&lt;init&gt; —&gt; 子类&lt;init&gt; 刚才说到JVM在处理了new指令、dup指令之后首先调用了SubClass.&lt;init&gt;，我们也解释了&lt;init&gt;构造器。从前面说的我们知道了在&lt;init&gt;构造器中的一个指令就是对父类&lt;init&gt;构造器的调用，结合上面所贴的SubClass.&lt;init&gt;字节码，aload_0就将局部变量表中下标为0的元素入栈, 其实就是Java中的this, 结合invokespecial #1, 是在调用父类的&lt;init&gt;构造器。（注意这里调用父类构造器的this代表是SubClass） 解释了所有的问题之后，让我们再继续看SuperClass.&lt;init&gt;： 123456789101112public class bugme.SuperClass &#123; public bugme.SuperClass(); Code: 0: aload_0 1: invokespecial #1 // Method java/lang/Object."&lt;init&gt;":()V 4: aload_0 5: bipush 99 7: invokevirtual #2 // Method setX:(I)V 10: return ...... &#125; 同样是先调了父类Object的&lt;init&gt;构造器, 然后再将this, 99入栈, invokevirtual #2旁边注释了是调用setX, 参数分别是this和99也就是this.setX(99)，最后是return指令，方法结束。 博主当初看到这里的时候，又产生了一个疑惑，为什么JVM会调用重写后的方法，在父类中使用的是this.setX(99)进行调用，JVM是怎么找到重写后方法的入口的？ 事实上博主之前认为this代表的是当前对象，方法在哪个对象中，this就代表哪个对象。 如果你和博主有一样的疑惑，那么你也应该好好了解一下this这个关键字了。问题就出在对this的理解上。博主目前并没有找到官方的说法，但是经过代码验证之后，SuperClass中的this表示的还是SubClass。并且在JavaScript中对于this调用是这样描述的：一个方法由哪个对象调用，这个方法所属的对象就是this。 这个方法被谁调用，这个this就是谁。可以好好体会这句话。 博主一直以为，this仅代表着当前对象。但是事实看来好像并不如此。由于是在子类构造器中调用的父类构造器，因此父类中的this代表的也是SubClass。甚至，我现在基本可以肯定，在SuperClass中对Object类的调用，也是SubClass。 那么事情已经变得简单了。既然已经确定了this，那么运用我们之前所说的动态分派知识，也可以明白为什么调用父类构造器中的setX方法会对应至子类的setX方法。 但是内容还不止于此… … 在和学长讨论之后，并且重新翻阅了分派那一节的内容之后，我觉得多态从本质上来说是根据当前栈帧上操作数栈顶引用所代表的实际类型来进行方法的查找，而不能简单的理解为根据方法接受者的实际类型来进行判断（那只是从我们程序员的角度来说）。正如我们上面分析的那样。 怎么理解“当前栈帧上操作数栈顶引用所代表的实际类型”呢？等下再说明这个问题。 昨天跟学长的讨论中觉得对多态的浅显认识可以这样理解： 当初始化子类的时候，所有子类继承的父类，父类的父类的方法都被子类所拥有，而因为子类可以重写父类的方法，所以被重写的方法就不会有体现。 我对其进行了一点补充：“相当于JVM把父类方法隐藏了，只有通过super.xxx()显式调用才能调用父类方法”。 如果你不想刨根问底，对于多态这样理解的话，我觉得也无可厚非。但是我们需要从JVM层面来考虑一下JVM到底是怎么找到重写后方法的地址入口而将父类方法的地址入口给隐藏了。 在之前我讲多态性实现机制的时候，我遗漏了一个非常重要的东西invokevirtual指令，因为当初没有学习JVM指令集，所以直接将这一部分知识略过了，这也导致了我当初对于多态的实现机制一知半解，就直接带大家上车了。 现在我来详细说一下invokevirtual指令的多态查找过程： 找到操作数栈顶的第一个元素所指向的对象的实际类型，记做C； 如果在C中找到与常量中的描述符和简单名称都相符的方法，则进行访问权限校验，如果通过则返回这个方法的直接引用；不通过则抛出IllegalAccessError异常； 否则，按照继承关系从下往上依次对C的各个父类进行第二步的搜索和验证过程； 如果始终没有找到合适的方法，就抛出AbstractMethodError异常。 而上述步骤就是Java语言中方法重写的本质，而这种在运行期根据实际类型（对应步骤一）确定方法执行版本的分派过程就是动态分派！！！ 那么我们回到刚才所讨论的代码上，要找到当前栈帧上操作数栈顶引用所代表的实际类型，看一下上面贴出的SuperClass.&lt;init&gt;的字节码。我们发现在调用setX方法之前，对操作数栈压入了this，又弹出this调用了Object的&lt;init&gt;构造器，之后又压入了99和this，此时操作数栈顶引用this所代表的实际类型就是subClass（上面已经进行了验证）。根据动态分派的原理，最后会调用SubClass中的setX方法，也就是重写后的方法。 对象实例化的顺序对运行结果所产生的影响上面所述将这篇博客的主要内容已经阐述清楚，但是还有一个问题，我们明白了在子类重写父类方法之后JVM为什么会调用重写后的方法，但是还没有说明程序运行结果的原因。 让我们继续来看Java字节码，调用重写setX方法中的字节码： 123456789public class bugme.SubClass extends bugme.SuperClass &#123; ...... public void setX(int); Code: 0: aload_0 1: iload_1 2: invokespecial #3 // Method bugme/SuperClass.setX:(I)V ......&#125; 这里将局部变量表前两个元素都入栈, 第一个是this, 第二个是括号里的参数, 也就是99, invokespecial #3调用的是父类的setX, 也就是我们代码中写的super.setX(int)。 SuperClass.setX就很简单了: 123456789public class bugme.SuperClass &#123; ...... public void setX(int); Code: 0: aload_0 1: iload_1 2: putfield #3 // Field mSuperX:I 5: return &#125; 这里先把this入栈, 再把参数入栈, putfield #3使得前两个入栈的元素全部出栈, 而成员mSuperX被赋值, 这四条指令只对应代码里的一句this.mSuperX = x。 接下来控制流回到子类的setX： 12345678910111213141516171819202122public class bugme.SubClass extends bugme.SuperClass &#123; ...... public void setX(int); Code: 0: aload_0 1: iload_1 2: invokespecial #3 // Method bugme/SuperClass.setX:(I)V --&gt;5: aload_0 // 即将执行这句 6: iload_1 7: putfield #2 // Field mSubX:I 10: getstatic #4 // Field java/lang/System.out:Ljava/io/PrintStream; 13: new #5 // class java/lang/StringBuilder 16: dup 17: invokespecial #6 // Method java/lang/StringBuilder."&lt;init&gt;":()V 20: ldc #7 // String SubX is assigned 22: invokevirtual #8 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 25: iload_1 26: invokevirtual #9 // Method java/lang/StringBuilder.append:(I)Ljava/lang/StringBuilder; 29: invokevirtual #10 // Method java/lang/StringBuilder.toString:()Ljava/lang/String; 32: invokevirtual #11 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 35: return&#125; 现在应该从上面所指向的5处开始执行了，5,6,7将参数的值赋给mSubX, 此时mSubX是99了, 下面那一堆则是在执行System.out.println(“mSubX is assigned “ + x);并返回, 还可以看到Java自动帮我们使用StringBuilder优化字符串拼接, 就不分析了。 都分析到这里了，你也许都会说，子类中的mSubX就是99啊，没毛病。为什么最后答案是1呢？ 你也许忘了，好好想一想刚才程序所走的流程—是不是才将父类中&lt;init&gt;构造器流程走完啊。子类初始化，调用父类的&lt;init&gt;，父类的&lt;init&gt;中调用了子类的setX方法，此时mSubX等于99，剩下的子类&lt;init&gt;还没有执行呢！而我们刚才也说了，&lt;init&gt;中包括了实例变量的初始化，因此在执行子类的&lt;init&gt;过程中把1赋给mSubX, 99被1覆盖了。这就是产生最后运行结果的真相！ 我们还可以再对照SubClass的字节码进行查看，刚才并没有将SubClass的字节码分析完毕： 123456789101112public class bugme.SubClass extends bugme.SuperClass &#123; public bugme.SubClass(); Code: 0: aload_0 --&gt;1: invokespecial #1 // Method bugme/SuperClass."&lt;init&gt;":()V 4: aload_0 5: iconst_1 6: putfield #2 // Field mSubX:I 9: return ...... &#125; 我们刚才分析到1处就去分析SuperClass中的&lt;init&gt;构造器了，此时mSubX已经是99了, 再执行下面的4,5,6, 将this入栈，将变量1入栈，将1赋值给this.mSubX，这一部分才是SubClass的初始化, 代码将1赋给mSubX, 99被1覆盖了。 最后return指令将方法返回，才相当于我们执行完了箭头指的这一句代码： 123456public class Main &#123; public static void main(String[] args) &#123; --&gt;SubClass sc = new SubClass(); sc.printX(); &#125;&#125; 接下来执行的代码将打印mSubX的值, 自然就是1了。 我们基本上将这份代码所产生的字节码文件分析了一遍，相信大家应该有一份额外的感受—JVM真的是基于栈执行的啊！原来这就是基于栈的指令集。 好了，这篇博客到此结束，自认为干货满满，非常有成就感，如果大家在阅读的过程有什么疑惑，欢迎大家留言讨论交流~~ 参考阅读《深入理解Java虚拟机》– 周志明 java虚拟机指令dup的理解 Java重写方法与初始化的隐患]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>多态性</tag>
        <tag>JVM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM--浅谈垃圾收集机制]]></title>
    <url>%2F2017%2F12%2F20%2FJVM-%E6%B5%85%E8%B0%88%E5%9E%83%E5%9C%BE%E6%94%B6%E9%9B%86%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[谈起GC，应该是让Java程序员最激动的一项技术，我相信每个Java程序员都有探究GC本质的冲动！ 本篇博客围绕三个问题展开： 哪些内存需要回收？ 什么时候回收？ 如何回收？ 哪些内存需要回收首先回答第一个问题：不再使用的对象需要进行回收，不使用的类也有可能回收。 那么我们如何判断一个对象不再使用呢？主要有以下两种方法。 引用计数算法定义：给对象添加一个引用计数器，每当有一个地方引用它时，计数器就加1；当引用失效时，计数器就减一；任何时刻计数器为0的对象就是不会被使用的对象。 我们可以看出，引用技术方法实现简单。并且有一些GC中确实使用的是引用计数算法，但是在Java虚拟机中并没有使用这个方法进行内存管理，原因就是一个问题很难被解决—对象之间循环引用。 来看一个例子： 123456789101112class Node &#123; Node next ;&#125; Node a = new Node (); Node b = new Node (); a.next = b ; b.next = a ; a = null;b = null; 如上述代码，当我们执行最后两行代码的时候，堆中的对象因为还存在着循环引用，因此引用计数并不是0，导致GC并不会回收这两个对象的内存。 可达性分析算法Java、C#等语言都是使用这种算法来判定对象是否存活。 基本思想： 通过一系列的称为“GC Roots”的对象作为起始点，从这些节点开始向下搜索，搜索所走过的路径被称为引用链，当一个对象到“GC Roots”没有任何引用链相连的时候，就证明此对象是不可用的。 如图： 在Java语言中，可作为GC Root对象包括以下几种： 虚拟机栈（栈帧中的本地变量表）中的引用对象。 方法区中的静态属性或常量（final）引用的对象。 本地方法栈中JNI(即一般说的Native方法)引用的对象。 方法区的回收Java虚拟机规范中描述可以不要求虚拟机在方法区实现垃圾收集，因此很多人认为方法区中是没有垃圾收集的。 不要求虚拟机对方法区进行垃圾收集的原因主要是性价比比较低，在堆中，尤其是新生代中，进行一次垃圾收集一般会回收70%～95%的空间，但方法区的垃圾收集率远低于此。 即使这样，对方法区进行垃圾收集也并非没有必要，在大量使用反射、动态代理等这类频繁定义ClassLoader的场景都需要虚拟机卸载类的功能，以保证方法区不会溢出。 方法区的垃圾收集主要回收废弃常量与无用的类。 废弃常量的判定与回收比较简单：以“abc”这个常量为例，如果当前系统中没有任何对象引用这个常量，也没有任何其他地方（博主猜测是.class文件中有些地方对此常量的引用）引用这个字面量。此时如果发生内存回收，这个常量就会被清理出常量池。（常量池中其他类、接口、方法、字段的符号引用与此类似） 一个无用的类则需要满足以下三个条件： 该类不存在任何实例。 加载该类的ClassLoader已经被回收（条件比较苛刻）。 该类对应的Class对象没有在任何地方被引用，也就是无法使用反射机制。 虚拟机可以对满足上述三个条件的无用类进行回收。 枚举根节点我们在这个部分应该思考一个关于可达性分析算法的问题，我们应该如何找出那些GC Roots。 目前很多应用仅仅方法区就有数百兆，如果要逐个检查这里面的引用，必然会消耗很多时间。 要解决这个问题，我们首先明确准确式内存管理的概念：虚拟机可以知道内存中某个位置的数据具体是什么类型。基于这点实现，在HotSpot中，使用一组称为OopMap的数据结构来保存内存中对象引用所存储的位置。 一般是在类加载完成的时候，HotSpot就将对象内什么偏移量上是什么类型的数据计算出来，在JIT编译（运行期优化）过程中，也会在特定的位置记录下栈和寄存器中哪些位置是引用。 这样GC在扫描的时候就可以直接得到这些信息。 再谈引用引用分为强引用（Strong Reference）、软引用（Soft Reference）、弱引用（Weak Reference）、虚引用（Phantom Reference）4种，这4种引用强度依次逐渐减弱。 强引用就是指在程序代码之中普遍存在的，类似“Object obj = new Object()”这类的引用，只要强引用还存在，垃圾收集器永远不会回收掉被引用的对象。 软引用是用来描述一些还有用但并非必需的对象。对于软引用关联着的对象，在系统将要发生内存溢出异常之前，将会把这些对象列进回收范围之中进行第二次回收。如果这次回收还没有足够的内存，才会抛出内存溢出异常。在JDK 1.2之后，提供了SoftReference类来实现软引用。 弱引用也是用来描述非必需对象的，但是它的强度比软引用更弱一些，被弱引用关联的对象只能生存到下一次垃圾收集发生之前。当垃圾收集器工作时，无论当前内存是否足够，都会回收掉只被弱引用关联的对象。在JDK 1.2之后，提供了WeakReference类来实现弱引用。 虚引用也称为幽灵引用或者幻影引用，它是最弱的一种引用关系。一个对象是否有虚引用的存在，完全不会对其生存时间构成影响，也无法通过虚引用来取得一个对象实例。为一个对象设置虚引用关联的唯一目的就是能在这个对象被收集器回收时收到一个系统通知。在JDK 1.2之后，提供了PhantomReference类来实现虚引用。 什么时候回收finalize方法通过上面几种算法，虚拟机可以知道此时内存中有哪些需要被回收的对象，但是虚拟机什么时候会对这些对象进行回收呢？我们需要来谈一谈finalize方法。 在JVM中，当一个对象通过可达性分析算法被判定为垃圾的时候，JVM并不能直接对其进行回收，一是垃圾回收机制并不是实时进行，二是真正的回收一个对象之前还会判断是否要运行它的finalize方法。 当一个对象被判定为是垃圾之后，它将会被第一次标记并进行一次筛选，筛选的条件就是此对象是否有必要执行finalize方法。 如何判断一个对象是否有必要执行finalize方法呢？ 两种情况下虚拟机会视为“没有必要执行”： 对象没有覆盖finalize方法； finalize方法已经被虚拟机调用过（finalize方法只会被调用一次）。 如果这个对象被判定为有必要执行finalize方法，那么这个对象会被放置在一个叫做F-Queue的队列之中，并在稍后由一个被虚拟机创建的，低优先级的Finalizer线程去执行该对象的finalize()方法，并且对象在finalize()方法执行中如果出现执行缓慢或者发生死循环，将会导致F-Queue队列中其他对象永久处于等待。甚至导致整个内存回收系统崩溃。之后GC将会对F-Queue之中的对象进行第二次标记。如果在第二次标记前这些对象在自己的finalize()方法中可以拯救自己(重新与引用链上的任何一个对象建立关联即可)也是可以成功存活下来并被移除“即将回收”的集合的。 如果此时还没有逃脱，那就真的要被回收了。 注意：finalize()方法的运行代价高昂，不确定性大，无法保证各个对象的调用顺序。博主建议大家完全可以忘掉Java语言中有这个方法的存在。 如何回收标记清除算法算法分为标记和清除两个阶段：首先标记出所有需要回收的对象，在标记完成后统一回收所有被标记的对象，它的标记过程就是使用可达性算法进行标记的。 不足： 效率问题，标记和清除两个过程的效率都不高。空间问题，标记清除之后会产生大量不连续的内存碎片，导致以后分配较大对象时内存不足以至于不得不提前触发另一次垃圾收集动作。 标记，清除过程图解： 复制算法将现有的内存空间分为两块，每次只使用其中一块，在垃圾回收时将正在使用的内存中的存活对象复制到未被使用的内存块中，之后，清除正在使用的内存块中的所有对象，交换两个内存的角色，完成垃圾回收。 如果系统中的垃圾对象很多，复制算法需要复制的存活对象数量并不会太大。因此在真正需要垃圾回收的时刻，复制算法的效率是很高的。又由于对象在垃圾回收过程中统一被复制到新的内存空间中，因此，可确保回收后的内存空间是没有碎片的。该算法的缺点是将系统内存折半。 现在的商业虚拟机都采用这种收集算法回收新生代，同时我们显然不能忍受内存折半的损耗，好在IBM公司研究表明，新生代中98%的对象都是“朝生夕死”，所以并不需要按照1:1的比例来划分内存空间。 通常是将内存分为一块较大的Eden空间两块较小的Survivor空间，每次使用Eden和其中一块Survivor。HotSpot虚拟机默认Eden和Survivor的比例为8:1。 当每次进行回收时，将Eden和Survivor中还存活的对象一次性的复制到另一块Survivor空间上，然后清理掉Eden和刚才使用的Survivor空间。 当然会存在另一块Survivor空间不够用的情况，这时需要其他内存进行分配担保。关于分配担保的内容，我们稍后再说。 内存分配策略与分代收集算法也许你会疑惑什么是新生代以及什么是分配担保。 Java堆根据对象存活周期的不同将内存划分为新生代与老年代。新生代又被划分为三个区域：Eden、From Survivor、To Survivor。 堆的内存模型大致为： 当对象在 Eden ( 包括一个 Survivor 区域，这里假设是 from 区域 ) 出生后，在经过一次 Minor GC 后，如果对象还存活，并且能够被另外一块 Survivor 区域所容纳( 上面已经假设为 from 区域，这里应为 to 区域，即 to 区域有足够的内存空间来存储 Eden 和 from 区域中存活的对象 )，则使用复制算法将这些仍然还存活的对象复制到另外一块 Survivor 区域 ( 即 to 区域 ) 中，然后清理所使用过的 Eden 以及 Survivor 区域 ( 即 from 区域 )，并且将这些对象的年龄设置为1，以后对象在 Survivor 区每熬过一次 Minor GC，就将对象的年龄 + 1，当对象的年龄达到某个值时 ( 默认是 15 岁，可以通过参数 -XX:MaxTenuringThreshold 来设定 )，这些对象就会成为老年代（长期存活的对象进入老年代）。但这也不是一定的，对于一些较大的对象 ( 即需要分配一块较大的连续内存空间 ) 则是直接进入到老年代（PretenureSizeThreshold参数的设定）。 Minor GC（新生代GC）：在新生代中，每次垃圾收集时都发现有大批对象死去，只有少量存活，那就选用复制算法，只需付出少量存活对象的复制成本就可以完成收集。Minor GC非常频繁，并且回收速度也很快。 Full GC/Major GC（老年代GC）：老年代中对象存活率高、没有额外空间对它进行分配担保，就要使用“标记-清理”或“标记-整理”算法进行回收。回收速度比Minor GC慢上很多，发生也不频繁。 额外说一点：（动态对象年龄判定） 为了能更好地适应不同程序的内存状况，虚拟机并不是永远地要求对象的年龄必须达到了MaxTenuringThreshold才能晋升老年代，如果新生代中的Eden与from Survivor空间相同年龄对象的大小之和大于to Survivor空间中的一半，则大于或等于这个年龄的对象则无须等到MaxTenuringThreshold中要求的年龄，即可晋升老年代。 （JDK1.6环境下正常运行）。 分配担保机制我们之前说过在新生代GC的时候，会将Eden和Survivor中还存活的对象一次性的复制到另一块Survivor空间上，然后清理掉Eden和刚才使用的Survivor空间。 但是会存在另一块Survivor空间不够用的情况，这时就需要分配担保了。 其实在发生Minor GC之前，虚拟机会检查老年代最大可用的连续空间是否大于新生代所有对象的总大小，如果大于，则此次Minor GC是安全的。如果小于，则虚拟机会查看HandlePromotionFailure设置值是否允许开启分配担保机制。如果HandlePromotionFailure=true，说明开启了分配担保机制，那么会继续检查老年代最大可用连续空间是否大于历次晋升到老年代对象的平均大小，如果大于，则尝试进行一次Minor GC，但这次Minor GC依然是有风险的；如果小于或者HandlePromotionFailure=false，则改为进行一次Full GC。 上面提到了Minor GC依然会有风险，是因为新生代采用复制收集算法，假如大量对象在Minor GC后仍然存活（最极端情况为内存回收后新生代中所有对象均存活），而Survivor空间是比较小的，这时就需要老年代进行分配担保，把Survivor无法容纳的对象放到老年代。老年代要进行空间分配担保，前提是老年代得有足够空间来容纳这些对象，但一共有多少对象在内存回收后存活下来是不可预知的，因此只好取之前每次垃圾回收后晋升到老年代的对象大小的平均值作为参考。使用这个平均值与老年代剩余空间进行比较，来决定是否进行Full GC来让老年代腾出更多空间。 但取平均值仍然是一种概率性的事件，如果某次Minor GC后存活对象陡增，远高于平均值的话，必然导致担保失败。如果出现了分配担保失败，就只能在失败后重新发起一次Full GC。虽然存在发生这种情况的概率，但大部分时候都是能够成功分配担保的，这样就避免了过于频繁执行Full GC。 参考阅读《深入理解Java虚拟机》—周志明 深入理解Java虚拟机读书笔记 - 垃圾收集算法 Java GC、新生代、老年代 空间分配担保]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>垃圾收集机制</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM--剖析类与对象在JVM中从生存至死亡]]></title>
    <url>%2F2017%2F12%2F17%2FJVM-%E5%89%96%E6%9E%90%E7%B1%BB%E4%B8%8E%E5%AF%B9%E8%B1%A1%E5%9C%A8JVM%E4%B8%AD%E4%BB%8E%E7%94%9F%E5%AD%98%E8%87%B3%E6%AD%BB%E4%BA%A1%2F</url>
    <content type="text"><![CDATA[前面学习了Class文件结构、类的加载机制、字节码执行引擎、对象的创建与销毁，所以我准备从一个Java代码进行切入，详细剖析它的生命历程，将所学的知识真正的用起来，也算是对前面所学的知识进行一个系统的总结。 我们以这份Java代码为例，来剖析一个Java程序的生命历程： 1234567891011121314151617181920212223242526272829interface ClassName &#123; String getClassName();&#125;class Company implements ClassName &#123; String className; public Company(String className) &#123; this.className = className; &#125; @Override public String getClassName() &#123; return className; &#125;&#125;public class Main &#123; public static void main(String[] args) &#123; String className; Scanner scanner = new Scanner(System.in); while (scanner.hasNext()) &#123; className = scanner.next(); Company company = new Company(className); System.out.println("name=" + company.getClassName()); &#125; &#125;&#125; 可以看到，这份代码涉及到了接口，继承，对象的实例化，main方法，值得我们花费一些功夫去从JVM层面上了解这个程序从编译、运行到结束都发生了哪些事情。 所以，别急，让我们按顺序慢慢来分析。 编译阶段首先你要运行一个java程序，肯定要对其进行编译，生成我们前面说的Class文件，这段代码会生成3个Class文件。 Class文件中保存了魔数、版本符号、常量池、方法标志、类索引、父类索引、接口索引、字段表（有可能含有属性表）、方法表（有可能含有属性表）等信息。这些信息具体的组成结构，我在这里不再赘述。 我们可以通过字节码文件，清晰的描述出Java源码中有关类的所有信息。 在这里只以Main类为例，使用javap命令看一下生成的Class文件。 1javap -verbose Main; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121122123124125126127128129130131132133134 Last modified 2017-12-13; size 852 bytes MD5 checksum 0336fa14cc04a9c858c34cc016880c19 Compiled from "Main.java"public class Main minor version: 0 major version: 52 flags: ACC_PUBLIC, ACC_SUPERConstant pool: #1 = Methodref #18.#29 // java/lang/Object."&lt;init&gt;":()V #2 = Class #30 // java/util/Scanner #3 = Fieldref #31.#32 // java/lang/System.in:Ljava/io/InputStream; #4 = Methodref #2.#33 // java/util/Scanner."&lt;init&gt;":(Ljava/io/InputStream;)V #5 = Methodref #2.#34 // java/util/Scanner.hasNext:()Z #6 = Methodref #2.#35 // java/util/Scanner.next:()Ljava/lang/String; #7 = Class #36 // Company #8 = Methodref #7.#37 // Company."&lt;init&gt;":(Ljava/lang/String;)V #9 = Fieldref #31.#38 // java/lang/System.out:Ljava/io/PrintStream; #10 = Class #39 // java/lang/StringBuilder #11 = Methodref #10.#29 // java/lang/StringBuilder."&lt;init&gt;":()V #12 = String #40 // name= #13 = Methodref #10.#41 // java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; #14 = Methodref #7.#42 // Company.getClassName:()Ljava/lang/String; #15 = Methodref #10.#43 // java/lang/StringBuilder.toString:()Ljava/lang/String; #16 = Methodref #44.#45 // java/io/PrintStream.println:(Ljava/lang/String;)V #17 = Class #46 // Main #18 = Class #47 // java/lang/Object #19 = Utf8 &lt;init&gt; #20 = Utf8 ()V #21 = Utf8 Code #22 = Utf8 LineNumberTable #23 = Utf8 main #24 = Utf8 ([Ljava/lang/String;)V #25 = Utf8 StackMapTable #26 = Class #30 // java/util/Scanner #27 = Utf8 SourceFile #28 = Utf8 Main.java #29 = NameAndType #19:#20 // "&lt;init&gt;":()V #30 = Utf8 java/util/Scanner #31 = Class #48 // java/lang/System #32 = NameAndType #49:#50 // in:Ljava/io/InputStream; #33 = NameAndType #19:#51 // "&lt;init&gt;":(Ljava/io/InputStream;)V #34 = NameAndType #52:#53 // hasNext:()Z #35 = NameAndType #54:#55 // next:()Ljava/lang/String; #36 = Utf8 Company #37 = NameAndType #19:#56 // "&lt;init&gt;":(Ljava/lang/String;)V #38 = NameAndType #57:#58 // out:Ljava/io/PrintStream; #39 = Utf8 java/lang/StringBuilder #40 = Utf8 name= #41 = NameAndType #59:#60 // append:(Ljava/lang/String;)Ljava/lang/StringBuilder; #42 = NameAndType #61:#55 // getClassName:()Ljava/lang/String; #43 = NameAndType #62:#55 // toString:()Ljava/lang/String; #44 = Class #63 // java/io/PrintStream #45 = NameAndType #64:#56 // println:(Ljava/lang/String;)V #46 = Utf8 Main #47 = Utf8 java/lang/Object #48 = Utf8 java/lang/System #49 = Utf8 in #50 = Utf8 Ljava/io/InputStream; #51 = Utf8 (Ljava/io/InputStream;)V #52 = Utf8 hasNext #53 = Utf8 ()Z #54 = Utf8 next #55 = Utf8 ()Ljava/lang/String; #56 = Utf8 (Ljava/lang/String;)V #57 = Utf8 out #58 = Utf8 Ljava/io/PrintStream; #59 = Utf8 append #60 = Utf8 (Ljava/lang/String;)Ljava/lang/StringBuilder; #61 = Utf8 getClassName #62 = Utf8 toString #63 = Utf8 java/io/PrintStream #64 = Utf8 println&#123; public Main(); descriptor: ()V flags: ACC_PUBLIC Code: stack=1, locals=1, args_size=1 0: aload_0 1: invokespecial #1 // Method java/lang/Object."&lt;init&gt;":()V 4: return LineNumberTable: line 27: 0 public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V flags: ACC_PUBLIC, ACC_STATIC Code: stack=3, locals=4, args_size=1 0: new #2 // class java/util/Scanner 3: dup 4: getstatic #3 // Field java/lang/System.in:Ljava/io/InputStream; 7: invokespecial #4 // Method java/util/Scanner."&lt;init&gt;":(Ljava/io/InputStream;)V 10: astore_2 11: aload_2 12: invokevirtual #5 // Method java/util/Scanner.hasNext:()Z 15: ifeq 63 18: aload_2 19: invokevirtual #6 // Method java/util/Scanner.next:()Ljava/lang/String; 22: astore_1 23: new #7 // class Company 26: dup 27: aload_1 28: invokespecial #8 // Method Company."&lt;init&gt;":(Ljava/lang/String;)V 31: astore_3 32: getstatic #9 // Field java/lang/System.out:Ljava/io/PrintStream; 35: new #10 // class java/lang/StringBuilder 38: dup 39: invokespecial #11 // Method java/lang/StringBuilder."&lt;init&gt;":()V 42: ldc #12 // String name= 44: invokevirtual #13 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 47: aload_3 48: invokevirtual #14 // Method Company.getClassName:()Ljava/lang/String; 51: invokevirtual #13 // Method java/lang/StringBuilder.append:(Ljava/lang/String;)Ljava/lang/StringBuilder; 54: invokevirtual #15 // Method java/lang/StringBuilder.toString:()Ljava/lang/String; 57: invokevirtual #16 // Method java/io/PrintStream.println:(Ljava/lang/String;)V 60: goto 11 63: return LineNumberTable: line 31: 0 line 32: 11 line 33: 18 line 34: 23 line 35: 32 line 36: 60 line 37: 63 StackMapTable: number_of_entries = 2 frame_type = 253 /* append */ offset_delta = 11 locals = [ top, class java/util/Scanner ] frame_type = 51 /* same */&#125;SourceFile: "Main.java" 我们大概分析一下上面的输出结果吧： 直接从常量池开始分析，前面的几行信息我认为没有分析的必要。 1#1 = Methodref #18.#29 // java/lang/Object."&lt;init&gt;":()V 这是常量池里面的第一项数据，#1代表索引，Methodref告诉我们常量池中第一个索引表示的是一个方法引用，对这个方法引用的描述用常量池中第18项和第29项的内容可以进行描述，你们可以查一下常量池中第18项和第29项的内容，其实对应的就是后面注释的内容。它告诉了你这个方法所属的类是Object，方法的简单名称是&lt;init&gt;，方法的描述符是()V，也就是Object中的实例构造函数（对简单名称和描述符我在前面的博客中已经进行了说明）。 你也许想问，为什么Main类常量池中的第一项数据描述的是Object类中的无参构造函数？你可能忘了，所有类都应如此，Java中所有的类都继承自Object，常量池中也会保存他们父类的索引，因为在Java中，对象的初始化与实例化不是还有一条规则嘛—先初始化与实例化父类，然后才是子类（说的并不精确，明白我的意思就行）。 剩余的常量池分析与上面类似。 常量池下面的代码块，可以看到，一个Main类的默认构造函数，一个就是main方法了。关于这两个东西，我们等一下在字节码的执行阶段再说。 可以看到，Class文件中，包含着详细的信息，有大量的信息都是你无法从源码中直接得到的。 类加载阶段javac对源文件编译完成，我们使用java命令开始运行这个Main类。java命令只能运行包含main方法的类。 java命令一开始运行，JVM开始对Main类进行加载。 这时候就对应了我们学习类加载机制的每个阶段：加载（从.class文件中读取字节码）、验证（对字节码文件进行一系列的验证保证格式无误并且对JVM不会产生危害）、准备（为类变量分配内存并进行系统初始化）、解析（分为静态链接与动态链接，非虚方法的符号引用会在这一阶段被解析为直接引用）、初始化（执行类构造器）。 我再对其过程进行一点点补充，如果想要更加详细的说明，请移步至我的博客专栏。 JVM在加载这个Main类的时候，使用类加载器（双亲委派模型）对其进行加载，经历了加载、验证（在这个时候又会触发Object类的加载）阶段，由于在Main类中并没有类变量，也就相当于跳过了准备这一阶段，然后对字节码进行解析，由于main方法是静态方法，也就是非虚方法，开始静态链接，在字节码中直接将main方法的符号引用解析为直接引用（别忘了Main类反编译之后的默认构造器～），由于没有静态变量与静态语句块，所以初始化这一阶段也相当于是直接跳过，最后整个加载过程完毕，并在方法区中生成Main类所对应的Class对象。 由于我这个例子中不涉及多态，也就不涉及分派，但这部分知识请务必掌握。 方法执行阶段类中所有的信息已经在内存中加载完毕，JVM开始进行方法调用… … 方法调用：JVM开始执行main方法，这部分工作是由虚拟机中字节码执行引擎完成的。main方法会由一个线程进行调用。此线程会在虚拟机栈上为自己开辟一部分的栈空间，此后只要这个线程调用新的方法，这个方法便会被当作栈帧压入虚拟机栈的栈顶（作为当前栈帧）。这个方法中定义的局部变量会被存储进局部变量表，在JVM中，并不存储局部变量的名称，他们都是以局部变量表的相对偏移量来标识每个不同的局部变量。 我以main方法的Code属性再说明一下栈帧中的局部变量表以及操作数栈。（Class文件中方法表的Code属性保存的是Java方法体中的字节码） 12Code: stack=3, locals=4, args_size=1 可以看到，main方法在调用之前（实际上在编译阶段，它的局部变量表，操作数栈的大小都已确定），locals为4，也就是局部变量表的大小为4：this，className，scanner，company；stack为3，也就是操作数栈的大小3：className，scanner，company。 我们还可以在上面方法体的字节码当中看到许多指令，而这些指令就是方法在执行的过程中，JVM需要解释运行的，如下： 10: new #2 // class java/util/Scanner 前面的0表示的也是相对偏移量，而new指令就是新建一个对象，对应Java源码中的new，后面的#2代表的是new指令的参数，表达的意思是常量池中索引为2的数据项，也就是上述代码后面注释中的Scanner类。 值得一说的是，这里的Scanner由于是对类的实例化，因此JVM会首先判断Scanner这个类是否会被加载进内存，如果没有被加载进内存，JVM开始对这个类进行类加载，过程如上述步骤，然后进行对象的初始化。 指令一条条的向下面执行（程序计数器），进入循环体，最终执行到这一步：Company company = new Company(className);，也是一个对类的实例化，但它和上面的Scanner又有点不同，这个类不仅实现了ClassName接口，而且实例化的时候还进行了传参。那么何时会加载ClassName接口呢，博主根据查阅的资料，猜测是在Company类加载中的验证阶段会触发其接口的加载，具体大家可以Google、baidu。 那么如何实现参数的传递呢，相信大家应该是有印象的，它是在实例化Company类的过程中，执行了Company的构造方法，而构造方法本身又是一个方法，因此可以理解为实参先进入被调用方法的操作数栈中，然后将操作数栈中的引用出栈赋值给被调用方法的局部变量表。 最后，代码执行完毕，程序退出。 参考阅读JVM 内部原理（六）— Java 字节码基础之一]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>生命周期</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM--详解创建对象与类加载的区别与联系]]></title>
    <url>%2F2017%2F12%2F12%2FJVM-%E8%AF%A6%E8%A7%A3%E5%88%9B%E5%BB%BA%E5%AF%B9%E8%B1%A1%E4%B8%8E%E7%B1%BB%E5%8A%A0%E8%BD%BD%E7%9A%84%E5%8C%BA%E5%88%AB%E4%B8%8E%E8%81%94%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[在前几篇博客中，我们探究了.class文件的本质，类的加载机制，JVM运行时的栈帧结构以及字节码执行时对应操作数栈以及局部变量表的变化。 如果你已经掌握了这些东西，你现在应该会有一种感觉，给你一个Java代码，你可以从JVM的层面上将这个类从javac编译成.class文件开始，到使用java命令运行这个Class文件，然后这个类的运行过程是怎么样的，你可以解释清楚。 但是等等，好像少了点什么？我们好像没有谈及JVM中对象的创建？也就是说，在Java代码中，你new一个对象，这时候都发生哪些事情，这就是今天我所要说的。 对象创建的时机我们先不说对象创建的具体过程是啥，我们先来谈一谈什么时候JVM会创建对象。 以下5种方式，会使JVM帮助你创建一个对象： 使用new关键字创建对象1Student student = new Student(); 使用Class类的newInstance方法(反射机制)newInstance方法只能调用无参的构造器创建对象。 123Student student2 = (Student)Class.forName("Student类全限定名").newInstance(); // 或者Student stu = Student.class.newInstance(); 使用Constructor类的newInstance方法(反射机制)java.lang.relect.Constructor类里也有一个newInstance方法可以创建对象，该方法和Class类中的newInstance方法很像，但是相比之下，Constructor类的newInstance方法更加强大些，我们可以通过这个newInstance方法调用有参数的和私有的构造函数。 123456789101112131415public class Student &#123; private int id; public Student(Integer id) &#123; this.id = id; &#125; public static void main(String[] args) throws Exception &#123; // 首先得到要实例化类的构造器（有参） Constructor&lt;Student&gt; constructor = Student.class .getConstructor(Integer.class); Student stu3 = constructor.newInstance(123); &#125;&#125; 事实上Class的newInstance方法内部调用的也是Constructor的newInstance方法。 使用Clone方法创建对象无论何时我们调用一个对象的clone方法，JVM都会帮我们创建一个新的、一样的对象，特别需要说明的是，用clone方法创建对象的过程中并不会调用任何构造函数。 123456789101112131415161718192021public class Student implements Cloneable&#123; private int id; public Student(Integer id) &#123; this.id = id; &#125; @Override protected Object clone() throws CloneNotSupportedException &#123; // TODO Auto-generated method stub return super.clone(); &#125; public static void main(String[] args) throws Exception &#123; Constructor&lt;Student&gt; constructor = Student.class .getConstructor(Integer.class); Student stu3 = constructor.newInstance(123); Student stu4 = (Student) stu3.clone(); &#125;&#125; 使用(反)序列化机制创建对象当我们反序列化一个对象时，JVM会给我们创建一个单独的对象，在此过程中，JVM并不会调用任何构造函数。为了反序列化一个对象，我们需要让我们的类实现Serializable接口。 12345678910111213141516171819202122232425262728293031public class Student implements Cloneable, Serializable &#123; private int id; public Student(Integer id) &#123; this.id = id; &#125; @Override public String toString() &#123; return "Student [id=" + id + "]"; &#125; public static void main(String[] args) throws Exception &#123; Constructor&lt;Student&gt; constructor = Student.class .getConstructor(Integer.class); Student stu3 = constructor.newInstance(123); // 写对象 ObjectOutputStream output = new ObjectOutputStream( new FileOutputStream("student.bin")); output.writeObject(stu3); output.close(); // 读对象 ObjectInputStream input = new ObjectInputStream(new FileInputStream( "student.bin")); Student stu5 = (Student) input.readObject(); System.out.println(stu5); &#125;&#125; 创建对象与类加载的区别与联系在明白了对象何时会被创建之后，现在我们就说一说，对象的创建与类加载的区别与联系。 当碰到上面所述5种情况的任何一种，都会触发对象的创建。 对象创建的过程 首先是对象创建的时机，在碰到new关键字，使用反射机制（class的new Instance、constructor的new Instance），使用clone等，会触发对象的创建。 在分配内存之前，JVM首先会解析是否能在运行时常量池中定位到这个类的符号引用，定位之后会判断这个类是否已经被加载、解析、初始化。如果没有，则先进行类的加载。 在确定对象需要创建之后，给对象开始分配内存，在分配内存的过程中，需要注意使用的是哪一种垃圾收集算法，因为垃圾收集算法的不同会导致内存块是否规整，也就影响到分配内存的方式是使用指针碰撞还是使用空闲列表。 在进行内存分配的时候，如果使用的是指针碰撞方法，还需要注意并发情况下，内存的分配是否是线程安全的。一般使用加同步块的方式和本地线程分配缓冲这两种方式解决线程安全的问题。 内存分配完毕之后就是JVM对其内存块进行默认初始化，这也是对象的实例变量不需要显示初始化就可以直接使用的原因。 从JVM的角度来看，一个对象就此创建完毕，但是从程序的角度来看，一个对象的创建才刚刚开始，它还没有运行&lt;init&gt;（实例初始化方法），所有的字段都还为默认值。只有运行了&lt;init&gt;之后，一个真正可用的对象才算产生出来。 具体过程如下图： 对象的组成符号引用解析完毕之后，JVM会为对象在堆中分配内存，HotSpot虚拟机实现的Java对象包括三个部分：对象头、实例字段和对齐填充字段（非必须）。 对象头主要包括两部分： 用于存储对象自身的运行时数据（哈希码、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳） 类型指针，即对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。 实例字段包括自身定义的和从父类继承下来的（即使父类的实例字段被子类覆盖或者被private修饰，都照样为其分配内存）。相信很多人在刚接触面向对象语言时，总把继承看成简单的“复制”，这其实是完全错误的。JAVA中的继承仅仅是类之间的一种逻辑关系（具体如何保存记录这种逻辑关系，则设计到Class文件格式的知识，之前也有说过），唯有创建对象时的实例字段，可以简单的看成“复制”。 为对象分配完堆内存之后，JVM会将该内存（除了对象头区域）进行零值初始化，这也就解释了为什么Java的属性字段无需显示初始化就可以被使用，而方法的局部变量却必须要显示初始化后才可以访问。最后，JVM会调用对象的构造函数，当然，调用顺序会一直上溯到Object类。 关于对象的实例化过程我下面详细说明，如图可得它是一个递归的过程。 &lt;init&gt;方法我们在类的加载机制一文中曾经说过&lt;clinit&gt;（类构造器），这个方法会在类的初始化阶段发挥作用，主要是收集类变量的赋值动作与静态语句块。 &lt;init&gt;有类似的作用。它也会将实例变量的赋值动作与实例代码块进行收集。说的详细点，如果我们对实例变量直接赋值或者使用实例代码块赋值，那么编译器会将其中的代码放到类的构造函数中去，并且这些代码会被放在对超类构造函数的调用语句之后(Java要求构造函数的第一条语句必须是超类构造函数的调用语句)，构造函数本身的代码之前。 &lt;init&gt;()就是指收集类中的所有实例变量的赋值动作、实例代码块和构造函数合并产生的。 我们将类构造器和实例构造器的初始化过程做一个总结：父类的类构造器() -&gt; 子类的类构造器() -&gt; 父类成员变量的赋值和实例代码块 -&gt; 父类的构造函数 -&gt; 子类成员变量的赋值和实例代码块 -&gt; 子类的构造函数。 对象的引用至此，一个对象就被创建完毕，此时，一般会有一个引用指向这个对象。在Java中，存在两种数据类型，一种就是诸如int、double等基本类型，另一种就是引用类型，比如类、接口、内部类、枚举类、数组类型的引用等。引用的实现方式一般有两种，如下图。 参考阅读图解JAVA对象的创建过程 深入理解Java对象的创建过程：类的初始化与实例化]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>虚拟机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM--详解虚拟机字节码执行引擎之静态链接、动态链接与分派]]></title>
    <url>%2F2017%2F12%2F09%2FJVM-%E8%AF%A6%E8%A7%A3%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AD%97%E8%8A%82%E7%A0%81%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E%E4%B9%8B%E9%9D%99%E6%80%81%E9%93%BE%E6%8E%A5%E3%80%81%E5%8A%A8%E6%80%81%E9%93%BE%E6%8E%A5%E4%B8%8E%E5%88%86%E6%B4%BE%2F</url>
    <content type="text"><![CDATA[这篇博客主要带你认识何谓静态链接、动态链接。并且会讲述JVM中分派的知识，让你对Java中的多态实现机制有一个浅显的认识。 前言从接触Java语言的第一天起，往后，我相信你一定听过什么动态链接啊，动态扩展啊，静态链接啊，它和C++相比有哪些优缺点啊… …如果你只是听说而没有探究过他们，也许你现在仍没办法说出Java语言到底有什么优缺点。 我们知道class文件是源代码经过编译后得到的字节码，如果学过编译原理会知道，这个仅仅完成了一半的工作（词法分析、语法分析、语义分析、中间代码生成），接下来就是实际的运行了。而Java选择的是动态链接的方式，即用到某个类再加载进内存，而不是像C++那样使用静态链接：将所有类加载，不论是否使用到。当然了，孰优孰劣不好判断。静态链接优点在速度，动态链接优点在灵活。 静态链接那么，首先，咱们先来聊聊静态链接。 如上面的概念所述，在C/C++中静态链接就是在编译期将所有类加载并找到他们的直接引用，不论是否使用到。而在Java中我们知道，编译Java程序之后，会得到程序中每一个类或者接口的独立的class文件。虽然独立看上去毫无关联，但是他们之间通过接口(harbor)符号互相联系，或者与Java API的class文件相联系。 我们之前也讲述了类加载机制中的一个过程—解析，并在其中提到了解析就是将class文件中的一部分符号引用直接解析为直接引用的过程，但是当时我们并没有详细说明这种解析所发生的条件，现在我给大家进行补充： 方法在程序真正运行之前就有一个可确定的调用版本，并且这个方法的调用版本在运行期是不可改变的。可以概括为：编译期可知、运行期不可变。 符合上述条件的方法主要包括静态方法和私有方法两大类。前者与类型直接关联，后者在外部不可被访问，这两种方法的特点决定了它们都不可能通过继承或别的方式重写其他版本，因此它们适合在类加载阶段进行解析。 额外补充一点： 在Java虚拟机中提供了5条方法调用字节码指令，其中invokestatic和invokespecial指令调用的方法，都可以在解析阶段中确定唯一的调用版本，符合这个条件的有静态方法、私有方法、实例构造器、父类方法（不知道这是个什么玩意、不重要，先放下）4类。它们在类加载的时候就会把符号引用解析为该方法的直接引用，因此这些方法也被称为非虚方法（包括final方法），与之相反的称为虚方法。 解析调用一定是个静态过程，在编译期间就完全确定，在类加载的解析阶段就会把涉及的符号引用转化为可确定的直接引用，不会延迟到运行期再去完成，这也就是Java中的静态链接。 动态链接上面大概说完了静态链接，那么什么是动态链接、它有什么用？ 如上所述，在Class文件中的常量持中存有大量的符号引用。字节码中的方法调用指令就以常量池中指向方法的符号引用作为参数。这些符号引用一部分在类的加载阶段（解析）或第一次使用的时候就转化为了直接引用（指向数据所存地址的指针或句柄等），这种转化称为静态链接。而相反的，另一部分在运行期间转化为直接引用，就称为动态链接。 与那些在编译时进行链接的语言不同，Java类型的加载和链接过程都是在运行的时候进行的，这样虽然在类加载的时候稍微增加一些性能开销，但是却能为Java应用程序提供高度的灵活性，Java中天生可以动态扩展的语言特性就是依赖动态加载和动态链接这个特点实现的。 动态扩展就是在运行期可以动态修改字节码，也就是反射机制与cglib，有兴趣的朋友可以查一下。 分派我们先不谈分派是什么，先来说说学习分派对你有什么用。 分派会解释多态性特征的一些最基本的体现，如“重载”、“重写”在Java虚拟机中是如何实现的，当然这里的实现不是语法上该怎么写，我们关心的是虚拟机如何确定正确的目标方法。 静态分派这就是刚才不谈分派的原因，分派的概念比较泛，分为静态分派、动态分派、单分派、多分派。我们先来说说静态分派。 来看一下静态分派的概念：所有依赖静态类型来定位方法执行版本的分派动作称为静态分派。静态分派的典型应用是方法重载。 你应该会对静态类型这个名词感到疑惑。 再来解释一下： 1Human man = new Man(); 如上代码，Human被称为静态类型，Man被称为实际类型。 再来看一段代码： 12345678//实际类型变化Human man = new Man();man = new Woman();//静态类型变化StaticDispatch sr = new StaticDispatch();sr.sayHello((Human) man);sr.sayHello((Woman) man); 可以看到的静态类型和实际类型都会发生变化，但是有区别：静态类型的变化仅仅在使用时发生，变量本身的静态类型不会被改变，并且最终的静态类型是在编译期可知的，而实际类型变化的结果在运行期才可确定。 知道这些东西之后，我给大家贴上完整代码： 12345678910111213141516171819202122232425262728293031class Human &#123; &#125;class Man extends Human &#123; &#125; class Woman extends Human &#123; &#125; public class StaticDispatch &#123; public void sayHello(Human guy) &#123; System.out.println("hello, guy!"); &#125; public void sayHello(Man guy)&#123; System.out.println("hello, gentleman!"); &#125; public void sayHello(Woman guy)&#123; System.out.println("hello, lady!"); &#125; public static void main(String[] args)&#123; Human man = new Man(); Human woman = new Woman(); StaticDisPatch sr = new StaticDisPatch(); sr.sayHello(man); sr.sayHello(woman); &#125; &#125; 运行结果： 12hello, guy!hello, guy! 如上代码与运行结果，在调用 sayHello()方法时，方法的调用者都为sr的前提下，使用哪个重载版本，完全取决于传入参数的数量和数据类型。代码中刻意定义了两个静态类型相同、实际类型不同的变量，可见编译器（不是虚拟机，因为如果是根据静态类型做出的判断，那么在编译期就确定了）在重载时是通过参数的静态类型而不是实际类型作为判定依据的。并且静态类型是编译期可知的，所以在编译阶段，javac 编译器就根据参数的静态类型决定使用哪个重载版本。这就是静态分派最典型的应用。 在静态分派中还有一个重载方法匹配优先级的问题，因为觉得并不是我要分享的重点，所以在这里我就不贴了，有兴趣的同学可以查阅《深入理解Java虚拟机》第249页或自行百度。 动态分派动态分派与多态性的另一个重要体现——方法重写有着很紧密的关系。向上转型后调用子类覆写的方法便是一个很好地说明动态分派的例子。这种情况很常见，因此这里不再用示例程序进行分析。很显然，在判断执行父类中的方法还是子类中覆盖的方法时，如果用静态类型来判断，那么无论怎么进行向上转型，都只会调用父类中的方法，但实际情况是，根据对父类实例化的子类的不同，调用的是不同子类中覆写的方法，很明显，这里是要根据变量的实际类型来分派方法的执行版本。而实际类型的确定需要在程序运行时才能确定下来，这种在运行期根据实际类型确定方法执行版本的分派过程称为动态分派。 关于动态分派与多态实现机制更详细的内容，建议参考博主的另一篇博客：JVM–从JVM层面深入解析对象实例化、多态性实现机制 总结一下静态分派：注意静态类型，编译阶段。动态分派注意局部变量表、操作数栈、invokevirtual指令的解析过程。 单分派与多分派先给出宗量的定义：方法的接受者（亦即方法的调用者）与方法的参数统称为方法的宗量。单分派是根据一个宗量对目标方法进行选择，多分派是根据多于一个宗量对目标方法进行选择。 为了方便理解： 12345678910111213141516171819202122232425262728293031323334class Eat &#123; &#125;class Drink &#123; &#125; class Father &#123; public void doSomething(Eat arg) &#123; System.out.println("爸爸在吃饭"); &#125; public void doSomething(Drink arg) &#123; System.out.println("爸爸在喝水"); &#125; &#125; class Child extends Father &#123; public void doSomething(Eat arg) &#123; System.out.println("儿子在吃饭"); &#125; public void doSomething(Drink arg) &#123; System.out.println("儿子在喝水"); &#125; &#125; public class SingleDoublePai &#123; public static void main(String[] args) &#123; Father father = new Father(); Father child = new Child(); father.doSomething(new Eat()); child.doSomething(new Drink()); &#125; &#125; 运行结果应该非常容易判断： 12爸爸在吃饭儿子在喝水 我们首先来看编译阶段编译器的选择过程，即静态分派过程。这时候选择目标方法的依据有两点：一是方法的接受者（即调用者）的静态类型是 Father 还是 Child，二是方法参数类型是 Eat 还是 Drink。因为是根据两个宗量进行选择，所以 Java 语言的静态分派属于多分派类型。 再来看运行阶段虚拟机的选择，即动态分派过程。由于编译期已经了确定了目标方法的参数类型（编译期根据参数的静态类型进行静态分派），因此唯一可以影响到虚拟机选择的因素只有此方法的接受者的实际类型是 Father 还是 Child。因为只有一个宗量作为选择依据，所以 Java 语言的动态分派属于单分派类型。 根据以上论证，我们可以总结如下：目前的 Java 语言（JDK1.6）是一门静态多分派（方法重载）、动态单分派（方法重写）的语言。 参考阅读《深入理解Java虚拟机》—周志明 Understanding the JVM：虚拟机类加载机制 JVM-动态链接(Dynamic Linking and Resolution) 多态性实现机制——静态分派与动态分派]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>虚拟机</tag>
        <tag>静态链接</tag>
        <tag>动态链接</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM--详解虚拟机字节码执行引擎之栈帧结构]]></title>
    <url>%2F2017%2F12%2F08%2FJVM-%E8%AF%A6%E8%A7%A3%E8%99%9A%E6%8B%9F%E6%9C%BA%E5%AD%97%E8%8A%82%E7%A0%81%E6%89%A7%E8%A1%8C%E5%BC%95%E6%93%8E%E4%B9%8B%E6%A0%88%E5%B8%A7%E7%BB%93%E6%9E%84%2F</url>
    <content type="text"><![CDATA[本篇博客信息量依旧庞大！ 前言在讨论本文的主题之前，我们先来思考一下：什么是虚拟机字节码执行引擎？它有什么作用？ 在说明这个问题之前，我们先来想想之前我们已经学习过的class文件结构与类加载机制。 当一个Java程序写好之后，我们使用javac命令对其进行编译，产生的虚拟机字节码存储在class文件中，我在JVM–解析Class类文件结构一文中详细分析了class文件中字节码的存储格式及其组成。然后我们紧接着说明了JVM–详解类加载机制，你应该了解到—类加载机制就是将class文件中的字节码加载进JVM的方法区并生成这个类的class对象的过程（再次强调不是生成这个类的实例化对象的过程）。 虚拟机字节码执行引擎那么，现在我们言归正传，解决上面的两个问题： 假设我们现在有这样一个类： 12345public class Demo &#123; public static void main(String[] args) &#123; System.out.println("hello world"); &#125;&#125; 在这个Java类写好之后并在虚拟机运行它之前，首先要将它加载进JVM中，而加载的就是class文件中的字节码，具体的加载过程我不在强调。 那么现在万事具备，JVM以main方法作为入口，而这些方法在运行之前首先要进行调用，那么虚拟机字节码执行引擎就是负责对方法进行调用并运行方法的JVM组成部分之一。 其实我现在觉得吧，class文件结构那一章对class文件进行了详细的分析；类加载机制那一章告诉了你类生命周期中的各个阶段并额外讲述了一点类加载器的相关知识；而这一章就是告诉你虚拟机是如何使用（执行）这些加载进来的字节码。 至于这个东西有什么作用？我上面概括的那些话就是作用—学完这三部分知识，你应该对一个Java程序从源码至运行时JVM层面发生的一系列过程有了一个挺详细的认知。 这一部分难点、重点比较多，但是也不要怕～我将这部分知识分解为三部分，绝对让你们轻松吸收～ 这篇博客我们主要探究何为栈帧～～～ 运行时栈帧结构要了解JVM对方法进行调用的过程，我们首先要掌握一个概念：栈帧。 它的概念很好理解，只是名字比较高大上而已。栈帧是一种数据结构，栈帧中包括了局部变量表、操作数栈、动态连接、返回地址等信息。 关于栈帧中所包括的信息，我等下再进行详细的说明。 我们先来看一下栈帧的概念结构图： 这个图已经描述的很清晰了，我将其中的重点再罗列出来： 栈帧存在于虚拟机栈中，并且是虚拟机栈中的单位元素。 每个线程中的不同栈帧对应这个线程调用的不同方法，可以看到栈帧很多，也就是调用的方法链会很多。 在活动线程中，只有当前栈帧有效，与之对应的也就是当前正在执行的方法，此方法被成为当前方法。 每调用一个新的方法，此方法对应的栈帧就会被放到栈顶（入栈），也就是成为新的当前栈帧；当一个方法退出的时候，此方法对应的栈帧也相应销毁（出栈）。 这就是栈帧。概念不多，而且栈帧中的出栈入栈对应数据结构中的栈操作（先入后出）。 额外需要补充的是，栈帧中需要多大的局部变量表，多深的操作数栈在 编译成class文件 的时候都是已经确定好的，这些信息都存储在 方法表中的code属性 中，因此每个栈帧需要分配多少内存，不会受到程序运行期变量数据的影响。 好吧，有可能你们对方法表中的code属性有点懵逼，那么我们先来回顾一下方法表是什么东西（知识本身就是一个不断回顾的过程，不要着急～）： 方法表的定义： class文件中的方法表包含了此方法的一些信息：访问标志（public、private等）、名称索引（指向常量池）、描述符索引（指向常量池，描述符用来描述方法的参数列表以及返回值）、属性表集合。 code属性的定义： code属性存储在属性表中，而属性表是多种属性的集合。我们这里只谈code属性，code属性存放的就是经过编译器编译成字节码指令的Java方法里面的代码（里面记录了局部变量表的大小与操作数栈的深度）。 所以我们之前说方法表中不一定需要属性表，是因为如果这是一个抽象方法，那么这个方法生成的方法表中就不需要存在属性表（这个Java方法没有被定义，属性表中的其他属性也无法被生成）。 局部变量表既然每个栈帧对应了每个调用过的方法，那么栈帧中存储的理应是我们平常方法体中所写的Java代码。 那么局部变量表作为组成栈帧的一份子： 用于存储方法参数和方法内部定义的局部变量，在Java程序编译成class文件的时候，就在code属性中的max_locals数据项中确定了该方法所要分配的局部变量表的最大容量。 局部变量区被组织为以一个slot（变量槽）为单位、从0开始计数的数组。虚拟机规范中并没有明确规定slot的大小，只是说明每个slot都应该能存放一个boolean、byte、char、short、int、float、reference、或returnAddress类型的数据。 类型为short、byte和char的值在存入数组前要被转换成int值，而long和 double在数组中占据连续的两项，在访问局部变量中的long或double时，只需取出连续两项的第一项的索引值即可，如某个long值在局部变量区中占据的索引是3、4项，取值时，指令只需取索引为3的long值即可。 再补充一些东西～～～ 先来上两份代码： 123public static int runClassMethod(int i,long l,float f,double d,Object o,byte b &#123; return 0;&#125; 123public int runInstanceMethod(char c,double d,short s,boolean b) &#123; return 0;&#125; 我们再来看一个图（可以对应上面我所说的）： 首先，可以看到虚拟机是使用局部变量表完成参数值到参数变量列表的传递过程的。并且runInstanceMethod（实例方法）的局部变量区第一项是个reference（引用），它指定的就是对象本身的引用，也就是我们常用的this，但是在RunClassMethod方法中，没这个引用，那是因为runClassMethod是个静态方法。 关于reference类型，代表的是一个对象实例的引用，这个引用应当可以做到两点： 从此引用中直接（直接引用）或间接（句柄池）地查找到对象在Java堆中的数据存放的起始地址索引。 从此引用中直接（对象头）或间接（句柄池）的查找到对象所属类型在方法区中存储的类型信息。 关于局部变量表的知识还有一个“slot的复用”，但是我在这里并不打算再进行讲解，原因就是它只能算是对GC收集的一种奇淫技巧，如果大家有兴趣的话可以参考《深入理解Java虚拟机》–周志明一书中的239页。 操作数栈同样，操作数栈也是一个先入后出的栈结构。同局部变量表一样，操作数栈的最大深度也在编译的时候写入到了code属性的max_stacks数据项中，运行期并不会改变。 操作数栈和局部变量区一样，也被组织成一个数组，操作数栈中的每个元素可以是任意的Java数据类型，32位数据类型所占栈容量为1，64位数据类型所占栈容量为2。但和前者不同的是，它不是通过索引来访问的，而是通过入栈和出栈来访问的。可把操作数栈理解为存储计算时，临时数据的存储区域。 下面我们通过一幅图片来了解下操作数栈的作用： 从图中可以得出：操作数栈其实就是个临时数据存储区域，它是通过入栈和出栈来进行操作的。 如果上图中的指令你们无法理解的话，也不要着急，更详细的内容会在之后的第三部分讲解～～ 动态连接这部分的内容会在之后的第二部分专门介绍，现在先不讲。 方法返回地址在一个方法开始执行之后，将来这个方法肯定是会退出的。方法的退出分为正常结束和异常终止。如果是通过return正常结束，则当前栈帧从Java栈中弹出，恢复发起调用的方法的栈。如果方法有返回值，JVM会 把返回值压入到发起调用方法的操作数栈。 为了处理Java方法中的异常情况，栈帧中还必须保存一个对此方法异常引用表的引用。当异常抛出时，JVM给catch块中的代码。如果没发现，方法立即终止，然后JVM用帧区数据的信息恢复发起调用的方法的帧，然后再发起调用方法的上下文重新抛出同样的异常。 虚拟机栈的整个结构来做个总结吧：栈是由栈帧组成，每当线程调用一个Java方法时，JVM就会在该线程对应的栈中压入一个帧，而帧是由局部变量区、操作数栈和 栈帧信息（一般会把动态链接、方法返回地址和其它附加信息归为一类，称为栈帧信息）组成。 参考阅读《深入理解Java虚拟机》—周志明 深入JVM——栈和局部变量]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>虚拟机</tag>
        <tag>栈帧</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[GitHub--重装系统后关联以前的GitHub]]></title>
    <url>%2F2017%2F12%2F07%2FGitHub-%E9%87%8D%E8%A3%85%E7%B3%BB%E7%BB%9F%E5%90%8E%E5%85%B3%E8%81%94%E4%BB%A5%E5%89%8D%E7%9A%84GitHub%2F</url>
    <content type="text"><![CDATA[由于系统重装了，当然本地的仓储和SSH生成的密钥就都没有了，这时如何在本地pull自己在GitHub上的仓储呢？ 由于系统重装，~/.ssh文件肯定就没有了，这时候就需要重新生成公钥和密钥。 ssh-keygen -t rsa -C “your_email@example.com“ Enter file in which to save the key (/home/XXX/.ssh/id_rsa):Created directory ‘/home/XXX/.ssh’.Enter passphrase (empty for no passphrase):Enter same passphrase again:Your identification has been saved in /home/XXX/.ssh/id_rsa.Your public key has been saved in /home/XXX/.ssh/id_rsa.pub.The key fingerprint is:79:3c:8c:e7:3e:57:8a:59:36:4d:4c:09:59:d3:8b:df your_email@example.comThe key’s randomart image is: 1234567891011+--[ RSA 2048]----+| .+oo || . o..|| o. .|| = .o. || S * o. .|| + .+ o.E|| .= + || .+ o || .o |+-----------------+ 需要输入时直接Enter，命令行中的邮箱是你在GitHub上留的邮箱，这样就生成类型为rsa的密钥对。然后把id_rsa.pub里的公开密钥拷贝到GitHub上即可。 Account Setting—&gt;SSH Keys—&gt;Add SSH Key即可。最后在目前目录下用ssh -T git@github.com验证。 输出Hi XXX! You’ve successfully authenticated, but GitHub does not provide shell access.即说明设置成功。 当我们需要GitHub上的仓储时，我们新建一个文件夹，首先git init初始化git，然后git pull https://github.com/XXX/Snake.git/，这样就会把仓储拷贝下来，这样就可以操作了。]]></content>
      <categories>
        <category>GitHub</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>SSH</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM--详解类加载机制]]></title>
    <url>%2F2017%2F12%2F01%2FJVM-%E8%AF%A6%E8%A7%A3%E7%B1%BB%E5%8A%A0%E8%BD%BD%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[这篇博客—干货很多！！！ Java虚拟机的体系结构前面我们探讨了Class文件的结构，如果你还没有学习，将不利于这部分知识的吸收与掌握，所以请移步：JVM–Class类文件结构（一） 学习一个东西之前，我们务必要知道，这东西大概是干什么的，有什么作用。 为了更清楚的阐释类加载机制到底是干什么的，我先将JVM的结构图贴给大家： 如上图，我们要学的类加载机制就是要搞清楚类加载器是如何找到指定的Class文件以及怎样将Class文件装载进内存，以便执行引擎执行Class文件中存在的数据和指令，从而使你的Java程序跑起来。 上面的黑体字就是这玩意大概是干啥的，至于学习它有什么作用，有助于你了解Java源代码是怎么从一个普通的文件变成一个可以正在运行的程序这其中的过程。而且，学习了这部分知识，你再回过头看反射机制，会有一种醍醐灌顶的感觉。 类的生命周期先来看一下类的生命周期吧： 结合上图，类加载机制主要学习加载、验证、准备、解析、初识化这些过程，然后就是需要了解真正可以将类加载进内存的一个玩意（还是代码实现）—类加载器！ 其实，有了前面Class文件结构的基础，这些东西都很简单，不要怕～ 额外补充： 上图中解析和初始化的位置是可以互换的，如果解析一旦在初始化之后开始，这就是我们经常所说的“动态绑定”～～ 除此之外，这些阶段通常都是互相交叉的混合式进行，各个阶段只保证按部就班的开始，并不保证按部就班的进行或完成。 类加载的过程我们根据上面所说的类的生命周期来一点点剖析类的加载过程。 加载我们首先要明白一件事情：什么开始进行类加载过程的第一阶段：加载？Java虚拟机没有进行强制约束，交由虚拟机的具体实现自由把握。 看完上面的话，我们来看在加载阶段，虚拟机需要完成哪些事情： 通过一个类的全限定名来获取定义此类的二进制字节流 将获取到的二进制字节流转化成一种数据结构并放进方法区 在内存中生成一个代表此类的java.lang.Class对象，作为访问方法区中各种数据的接口 我们需要注意一些事情： 对于方法区的认识：被加载的类的信息存储在方法区中，可以被线程所共享，也就是说，加载阶段完成后，虚拟机外部的二进制字节流就按照虚拟机所需的格式存储在了方法区之中。然后你能想起来那个二进制流中都存储了哪些信息吗？ 对于Class对象认识：Class对象虽然是在内存中，但并未明确规定是在Java堆中，对于HotSpot来说，Class对象存储在方法区中。它作为程序访问方法区中二进制字节流中所存储各种数据的接口。你能大概想到反射机制中的Class对象是怎么一回事了吗？为什么可以在运行期通过反射机制得到那么多的类信息你能猜测到吗？ 验证从上面类的生命周期一图中我们可以看出，验证是连接的第一步，这一阶段的目的主要是为了确保Class文件的字节流中包含的信息符合当前虚拟机的要求，从而不会危害虚拟机自身安全。也就是说，当加载阶段将字节流加载进方法区之后，JVM需要做的第一件事就是对字节流进行安全校验，以保证格式正确，使自己之后能正确的解析到数据并保证这些数据不会对自身造成危害。 验证阶段主要分成四个子阶段： 文件格式验证 元数据验证 字节码验证 符号引用验证 我不在这里详细的说明每一阶段的校验主要干了什么事情，有兴趣的同学可以自行百度。 挑点重点来说吧，对字节流进行校验是由一个叫做Class文件检验器的东西所完成，其实还是代码实现。 而什么叫做元数据呢？ 所谓的元数据是指用来描述数据的数据，更通俗一点就是描述代码间关系，或者代码与其它资源（例如数据库表）之间内在联系的数据，你也可以更简单的认为成框架中的各种@注解，因为这些@注解很简介的描述了大量有关各个类、方法、字段额外的信息或之间的联系。 元数据验证也就是验证这些额外的信息或它们之间的联系是否正确。 我们还得注意字节码验证，在字节码验证中涉及到了一个概念：字节码流。 字节码流 = 操作码 + 操作数。 操作码就是伪指令，操作数就是普通的Java数据，如int，float等等。 所以对字节码验证的过程就是对字节码流验证的过程，也就是验证操作码是否合法，操作数是否合法。 而符号引用验证涉及到常量池解析的知识，在下文中我们顺带着将符号引用验证带过就行，现在先不说。 准备准备阶段你只要掌握两个知识点： 准备阶段的目的：正式为类变量分配内存并设置类变量初始值的阶段，这些变量所使用的内存将在方法区中分配。 注意我的重点：是类变量（static）不是实例变量，还有，我们又知道了在JVM的方法区中不仅存储着Class字节流（按照运行时方法区的数据结构进行存储，上述的二进制字节流是不严谨的说法，只是为了大家好理解），还有我们的类变量。 这里的类变量初始值通常是指数据类型的零值。比如int的零值为0，long为0L，boolean为false… …真正的初始化赋值是在初始化阶段进行的。 额外一点，如果你设置的类变量还具有final字段，如下： 1public static final int value = 123; 那么在准备阶段变量的初始值就会被直接初始化为123，具体原因是由于拥有final字段的变量在它的字段属性表中会出现ConstantValue属性。 解析这一阶段我个人觉得不太好理解并且非常重要，但我还是会一点点剖析难点，保证你能听懂，所以开始吧～～ 先来看一下解析阶段的目的：虚拟机将常量池内的符号引用替换为直接引用。 然后说一下解析阶段最大的特点：发生时间不可预料，有可能和初始化阶段互相交换位置。至于原因，我们等下再说。 先来说看完解析阶段的目的吧，你有可能有三个疑问。哪个常量池？什么符号引用？什么直接引用？Ok，搞清这三个问题，解析这部分你也就学会了。 首先来说常量池：在Class的文件结构中我们就花了大量的篇幅去介绍了常量池，我们再来总结一下：常量池(constant pool)指的是在编译期被确定，并被保存在已编译的.class文件中的一些数据。它包括了关于类、方法、接口等中的常量，也包括字符串常量。 然后这段话中的常量池指的就是存在于.class文件中的常量池，结果在运行期被JVM装载，并且可以扩充的存在于方法区中的运行时常量池。 然后来看符号引用：在Class文件中我们也讲述了什么是符号引用。总的来说就是常量池中存储的那些描述类、方法、接口的字面量，你可以简单的理解为就是那些所需要信息的全限定名，目的就是为了虚拟机在使用的时候可以定位到所需要的目标。 最后来看直接引用：直接指向目标的指针、相对偏移量或能间接定位到目标的句柄。 现在我们对上面那句话进行重新解读：虚拟机将运行时常量池中那些仅代表其他信息的符号引用解析为直接指向所需信息所在地址的指针。 大概就是这样，我觉得你应该已经完全明白了。 解决一个遗留的问题：还记得刚才没有说到的符号引用吗？ 这一阶段就是发生在JVM将符号引用转换为直接引用的时候，它的作用就是对类自身以外（常量池中的各种符号引用）的信息进行匹配性校验，以确保解析动作能够正常执行！ 在解析阶段主要有以下不同的动作，我只给大家罗列出来，不细讲，有兴趣的同学可以自行百度： 类或接口的解析（注意数组类和非数组类） 字段（简单名称+字段描述符）解析（注意递归搜索） 类方法解析（注意递归搜索） 接口方法解析（注意递归搜索） 在解析阶段还有一个很有意思的东西：动态连接！ 它也是上面解析阶段发生时间不确定的直接原因：大部分JVM的实现都是延迟加载或者叫做动态连接。它的意思就是JVM装载某个类A时，如果类A中有引用其他类B，虚拟机并不会将这个类B也同时装载进JVM内存，而是等到执行的时候才去装载。 而这个被引用的B类在引用它的类A中的表现形式主要被登记在了符号表中，而解析的过程就是当需要用到被引用类B的时候，将引用类B在引用类A的符号引用名改为内存里的直接引用。这就是解析发生时间不可预料的原因，而且这个阶段是发生在方法区中的。 初始化虚拟机规范定义了5种情况，会触发类的初始化阶段，也正是这个阶段，JVM才真正开始执行类中定义的Java程序代码： new一个对象、读取一个类静态字段、调用一个类的静态方法的时候 对类进行反射调用的时候 初始化一个类，发现父类还没有初始化，则先初始化父类 main方法开始执行时所在的类 最后一种情况我也不懂，就不贴了 额外补充： 有三种引用类的方式不会触发初始化（也就是类的加载），为以下三种： 通过子类引用父类的静态字段，不会导致子类初始化 通过数组定义来引用类，不会触发此类的初始化 引用另一个类中的常量不会触发另一个类的初始化，原因在于“常量传播优化” 来说一说常量传播优化吧（先看一份代码）： 1234567891011121314public class ConstClass &#123; static &#123; System.out.println("ConstClass init!"); &#125; public static final String HELLOWORLD = "hello world";&#125;public class NotInitialization &#123; public static void main(String[] args) &#123; System.out.println(ConstClass.HELLOWORLD); &#125;&#125; 这种调用方式不会触发ConstClass的初始化，因为常量传播优化，常量“hello world”已经被存储到了NotInitialization类的常量池中，以后NotInitialization对常量ConstClass.HELLOWORLD的引用实际上都被转化为NotInitialization对自身常量池的引用。 然后在初识化阶段我们重点掌握的知识就是类构造器&lt;clinit&gt;()了。 这个东西我也只是提几点重要的： &lt;clinit&gt;()是编译器自动收集类中的所有类变量的赋值动作和静态语句块合并产生的。 父类中定义的静态语句块要优先于子类的变量赋值操作。 虚拟机保证一个类的&lt;clinit&gt;()方法在多线程环境中被正确的加锁、同步。 方法区使用实例上面讲了那么多，不知道大家吸收了没有，学习的过程中一定要学会总结和抓中重点哦～ 咱们来看一个例子将上面类加载的过程来串一下吧，加深一下自己的印象：（其中还涉及了一点执行引擎的知识，没关系，很容易理解） 123456789101112131415class Lava &#123; private int speed = 5; void flow() &#123; &#125;&#125;public class Volcano &#123; public static void main(String[] args) &#123; Lava lava = new Lava(); lava.flow(); &#125;&#125; 不同的虚拟机实现可能会用完全不同的方法来操作，下面描述的只是其中一种可能——但并不是仅有的一种。 加载：读取一个类的class文件并将其中的二进制字节流组织成正确的数据结构放进运行时方法区中： 要运行Volcano程序，首先得以某种“依赖于实现的”方式告诉虚拟机“Volcano”这个名字。之后，虚拟机将找到并读入相应的class文件“Volcano.class”，然后它会从导入的class文件里的二进制数据中提取类型信息并放到方法区中。通过执行保存在方法区中的字节码，虚拟机开始执行main()方法，在执行时，它会一直持有指向当前类（Volcano类）的常量池（方法区中的一个数据结构）的指针。 注意：虚拟机开始执行Volcano类中main()方法的字节码的时候，尽管Lava类还没被装载，但是和大多数（也许所有）虚拟机实现一样，它不会等到把程序中用到的所有类都装载后才开始运行。恰好相反，它只会需要时才装载相应的类。（延迟加载、动态连接） main()的第一条指令告知虚拟机为列在常量池第一项的类分配足够的内存。所以虚拟机使用指向Volcano常量池的指针找到第一项，发现它是一个对Lava类的符号引用，然后它就检查方法区，看Lava类是否已经被加载了。 这个符号引用仅仅是一个给出了类Lava的全限定名“Lava”的字符串。为了能让虚拟机尽可能快地从一个名称找到类，虚拟机的设计者应当选择最佳的数据结构和算法。 当虚拟机发现还没有装载过名为“Lava”的类时，它就开始查找并装载文件“Lava.class”，并把从读入的二进制数据中提取的类型信息放在方法区中。 解析： 紧接着，虚拟机以一个直接指向方法区Lava类数据的指针来替换常量池第一项（就是那个字符串“Lava”），以后就可以用这个指针来快速地访问Lava类了。这个替换过程称为常量池解析，即把常量池中的符号引用替换为直接引用。 终于，虚拟机准备为一个新的Lava对象分配内存。此时它又需要方法区中的信息。还记得刚刚放到Volcano类常量池第一项的指针吗？现在虚拟机用它来访问Lava类型信息，找出其中记录的这样一条信息：一个Lava对象需要分配多少堆空间。 JAVA虚拟机总能够通过存储在方法区的类型信息来确定一个对象需要多少内存，当JAVA虚拟机确定了一个Lava对象的大小后，它就在堆上分配这么大的空间，并把这个对象实例的变量speed初始化为默认初始值0。 当把新生成的Lava对象的引用压到栈中，main()方法的第一条指令也完成了。接下来的指令通过这个引用调用Java代码（该代码把speed变量初始化为正确初始值5）。另一条指令将用这个引用调用Lava对象引用的flow()方法。 类加载器其实这一部分的知识并不多，你需要了解、掌握的知识只有两点： 类加载器的命名空间 双亲委派模型 说一点啊，看到这些高大上的名词你们不要怕，又不让你拿代码去实现，其实其中的原理都是很简单的。 上面说了那么多，类加载器就是用于实现类加载动作的一段代码实现。好了，明白了它的作用，我们来看看什么是它命名空间。 类加载器的命名空间：对于任意一个类，都需要由加载它的类加载器和这个类本身一同确立其在Java虚拟机中的唯一性，每一个类加载器，都拥有一个独立的类命名空间。也就是说，你现在要比较两个类是否相等，只有在这两个类是同一个类加载器加载的前提下才有意义。 这就是类加载器的命名空间，不难吧～但是读完上面这段话我们可以提取出另一个重要的信息：你上面所说的话中好像包含这样一种意思：类加载器在JVM中不止一个？你很聪明！好了，接下来说明何谓双亲委派模型。 双亲委派模型：首先你得知道在JVM中有三种系统提供的类加载器：启动类加载器，扩展类加载器、应用程序类加载器。关于这三种加载器的描述大家自行百度，这也不是重点。 贴一张图： 如图，这种层次结构就是双亲委派模型。 好了，为了让大家印象深刻，我在给大家描述一下双亲委派模型的工作过程吧： 它是一个递归调用类加载器的模型，也就是说如果一个类加载器收到了类加载的请求，它首先不会自己去加载这个类，而是不断请求父加载器，如果父加载器可以完成这个加载请求，那么就由父加载器进行加载，如果父加载器不能完成加载请求（它的搜索范围中没有找到所需的类），子加载器才会尝试自己去加载。 那么使用这种模型有什么好处？ Java类随着类加载器一起具备了带有优先级的层级关系。例如java.lang.Object，在程序的各种类加载器环境中都是同一个类。 关于双亲委派模型的实现代码非常简单，我就不再贴出，有兴趣的同学可以研读《深入理解Java虚拟机》第二版中232页的内容。 参考阅读JAVA虚拟机体系结构 《深入理解Java虚拟机》—周至明]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>虚拟机</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL数据库--外键约束及外键使用]]></title>
    <url>%2F2017%2F11%2F18%2FMySQL%E6%95%B0%E6%8D%AE%E5%BA%93-%E5%A4%96%E9%94%AE%E7%BA%A6%E6%9D%9F%E5%8F%8A%E5%A4%96%E9%94%AE%E4%BD%BF%E7%94%A8%2F</url>
    <content type="text"><![CDATA[什么是主键、外键关系型数据库中的一条记录中有若干个属性，若其中某一个属性组(注意是组)能唯一标识一条记录，该属性组就可以成为一个主键。 比如： 学生表(学号，姓名，性别，班级)其中每个学生的学号是唯一的，学号就是一个主键 课程表(课程编号,课程名,学分)其中课程编号是唯一的,课程编号就是一个主键 成绩表(学号,课程号,成绩)成绩表中单一一个属性无法唯一标识一条记录，学号和课程号的组合才可以唯一标识一条记录，所以学号和课程号的属性组是一个主键 成绩表中的学号不是成绩表的主键，但它和学生表中的学号相对应，并且学生表中的学号是学生表的主键，则称成绩表中的学号是学生表的外键。 同理：成绩表中的课程号是课程表的外键。 定义主键和外键主要是为了维护关系数据库的完整性，总结一下： 主键是能确定一条记录的唯一标识，比如，一条记录包括身份正号，姓名，年龄。身份证号是唯一能确定你这个人的，其他都可能有重复，所以，身份证号是主键。 外键用于与另一张表的关联。是能确定另一张表记录的字段，用于保持数据的一致性。比如，A表中的一个字段，是B表的主键，那他就可以是A表的外键。 主键、外键和索引的区别 主键 外键 索引 定义 唯一标识一条记录，不能有重复的，不允许为NULL 表的外键是另一表的主键, 外键可以有重复的, 可以是NULL 没有重复值，可以为NULL(会使索引无效) 作用 用来保证数据完整性 用来和其他表建立联系用的 提高查询排序的速度 个数 主键只能有一个 一个表可以有多个外键 一个表可以有多个惟一索引 外键约束在上面“什么是主键、外键” 一小节中，我给大家灌输的思维是，学生表使用学号作为主键，课程表使用课程ID作为主键，成绩表使用学号、课程ID作为联合主键（联合主键（使用组合索引进行替代）以后压根就别用，主键的设计原则就是字段数目越少越好），这样就产成了一个问题，外键的参考键必须是另一个表的主键吗？ 答案当然不是，但是参考键必须是唯一性索引。主键约束和唯一性约束都是唯一性索引。 错误的设计方式—[1215] Cannot add foreign key constraint出现这种问题的原因一般有两个： 两张表里要设主键和外键的字段的数据类型或者数据长度不一样。 某个表里已经有记录了。 我当时属于第一个。 如何设计良好的数据库主键摘抄一位知乎用户的回答：知乎链接—纪路 主键的话我的建议是自增整形，不要使用与业务相关的名字，仅用id即可，而效率问题都可以用索引来解决。因为主键的不可变的特性，如果选择不慎，会在未来产生难以预期的问题。比如你用int型做文章的id，但是如果在未来某一天文章数超过了无符号整形的最大值，你将没法将主键修改成bigint。或者为了给用户起一个唯一id用了自增主键，但是如果未来有其他的项目用户要合并进来，他也是这么做的。这时候为了区分不同的项目可能要在这个用户id前加一个前缀，这时候也没法修改主键的值。主键之所以叫做主键就是到什么时候都不能改，所以最好的方案就是使用自增数字id做主键，并且不要给这个主键赋予一个业务相关的意义。 总结上面前辈的一句话就是，不要将表中与业务相关的字段设置为主键，即使它可以唯一标识这一行，比如身份证号，学号等等，主键越没有意义，说明主键设置的越好。 主键、外键的使用创建表就按照我们上面的例子来建立三张表吧：student、course、score表。 创建student表： 123456789create table student( pk_id bigint unsigned not null auto_increment primary key, uk_sno int(10) unsigned not null, name char(60) not null, sex char(10) not null, class char(60) not null, constraint uk_sno unique (sno))enige = InnoDB, charset = utf8; 创建course表： 12345678create table course( pk_id bigint unsigned not null auto_increment primary key, uk_course_id int(10) unsigned not null, course_name char(30) not null, credit int not null, constraint uk_course_id unique (course_id))enige = InnoDB, charset=utf8; 创建score表： 123456789create table score( pk_id bigint not null auto_increment primary key, fk_sno int(10) unsigned not null, fk_course_id int(10) unsigned not null, result int not null, constraint fk_sno foreign key (fk_sno) references &lt;databasename&gt;.student (sno), constraint fk_course_id foreign key (fk_course_id) references &lt;databasename&gt;.course (course_id))enige = InnoDB, charset=utf8; 值得一说的是，创建外键的时候也会自动创建普通索引，所以fk_sno、fk_course_id其实是两个普通索引的名称。 对于使用IDEA的同学，我们会发现在设置外键的时候还有Update rule 和 Delete rule规则，对于这两个选项的解释，我们下面再说。 外键的使用–更新与删除表已经建立成功，现在我们插入数据： student表： 1INSERT INTO student(uk_sno, name, sex, class) VALUES(123456, "spider_hgyi", "male", "cs"); crouse表： 1INSERT INTO course(uk_course_id, course_name, credit) VALUES(1, "csapp", 10); score表： 1INSERT INTO score(fk_sno, fk_course_id, result) VALUES(123456, 1, 100); 好了，现在三个表里都已经有了数据，现在我们尝试更新学生表中学号的信息： 1UPDATE student SET uk_sno=12345678 WHERE uk_sno=123456; MySQL报错： 1(1451, 'Cannot delete or update a parent row: a foreign key constraint fails (`bookmanager`.`score`, CONSTRAINT `fk_sno` FOREIGN KEY (`fk_sno`) REFERENCES `student` (`uk_sno`))') 看看错误告诉我们什么：不能删除或更新这一行，存在外键约束，score表中的fk_sno列是当前要更新的uk_sno的外键，也就是说，你要更新学生表中的学号，但是成绩表中的学号是你的外键，你不能不管它呀，删除也是同理。 要怎么解决？ 还记得刚才我贴的那张IDEA的图片吗？那两个规则就可以帮助我们解决这个问题。 级联删除与更新我们在更新与删除时遇到的外键约束解决方案分别对应设置Update rule与Delete rule。有如下四个选项： CASCADE：从父表删除或更新且自动删除或更新子表中匹配的行。 SET NULL：从父表删除或更新行，并设置子表中的外键列为NULL。如果使用该选项，必须保证子表列没有指定NOT NULL。 RESTRICT：拒绝对父表的删除或更新操作。 NO ACTION：标准SQL的关键字，在MySQL中与RESTRICT相同。 可以看到我在创建外键的时候选择的是NO ACTION，也就是第四个选项。我们只需要选择CASCADE就可以啦。具体效果就不进行演示了。 如果你不用IDEA也没关系，接下来我给出SQL语句的实现（重新创建score表）： 123456789create table score( pk_id bigint not null auto_increment primary key, fk_sno int(10) unsigned not null, fk_course_id int(10) unsigned not null, result int not null, constraint fk_sno foreign key (fk_sno) references &lt;databasename&gt;.student (sno) on update cascade on delete cascade, constraint fk_course_id foreign key (fk_course_id) references &lt;databasename&gt;.course (course_id) on update cascade on delete cascade)enige = InnoDB, charset=utf8; 补充博主在学习阿里的Java开发手册时，他们对于外键与级联是这样描述的： 【强制】不得使用外键与级联,一切外键概念必须在应用层解决。 说明：以学生和成绩的关系为例,学生表中的 student _ id 是主键,那么成绩表中的 student _ id则为外键。如果更新学生表中的 student _ id ,同时触发成绩表中的 student _ id 更新,即为级联更新。外键与级联更新适用于单机低并发,不适合分布式、高并发集群 ; 级联更新是强阻塞,存在数据库更新风暴的风险;外键影响数据库的插入速度。 本来我是打算以后在脑海中抛弃外键与级联这部分知识的，但经过学长的敲打，不得不说我对阿里的盲目崇拜。 外键约束、级联更新与删除对于开发者是非常有用的，它确保了数据删除与更新的完整性。至于阿里所说的影响性能，学长反问我：“你的应用有多少人在用？阿里的应用有多少人在用？”。 说这这些话的原因也是这次提醒我在软件开发的过程中应想好受众的大小，灵活运用所学的知识，不能盲目追求课本以及参考资料。 参考阅读外键必须是另一个表的主键吗 关于数据库主键和外键（终于弄懂啦） 快速理解MySQL中主键与外键的实例教程 MySQL使用外键实现级联删除与更新的方法 阿里巴巴Java开发手册–MySQL数据库]]></content>
      <categories>
        <category>MySQL数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>外键</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL数据库--索引的说明、使用、及其注意事项]]></title>
    <url>%2F2017%2F11%2F16%2FMySQL%E6%95%B0%E6%8D%AE%E5%BA%93-%E7%B4%A2%E5%BC%95%E7%9A%84%E8%AF%B4%E6%98%8E%E3%80%81%E4%BD%BF%E7%94%A8%E3%80%81%E5%8F%8A%E5%85%B6%E6%B3%A8%E6%84%8F%E4%BA%8B%E9%A1%B9%2F</url>
    <content type="text"><![CDATA[什么是索引索引用来快速地寻找那些具有特定值的记录，所有MySQL索引都以B-树的形式保存。如果没有索引，执行查询时MySQL必须从第一个记录开始扫描整个表的所有记录，直至找到符合要求的记录。表里面的记录数量越多，这个操作的代价就越高。如果作为搜索条件的列上已经创建了索引，MySQL无需扫描任何记录即可迅速得到目标记录所在的位置。如果表有1000个记录，通过索引查找记录至少要比顺序扫描记录快100倍。 假设我们创建了一个名为people的表： 1CREATE TABLE people (peopleid SMALLINT NOT NULL, name CHAR(50) NOT NULL); 然后，我们完全随机把1000个不同name值插入到people表。 如果我们创建了name列的索引，MySQL将在索引中排序name列。 对于索引中的每一项，MySQL在内部为它保存一个数据文件中实际记录所在位置的“指针”。因此，如果我们要查找name等于“Mike”记录的peopleid（SQL命令为“SELECT peopleid FROM people WHERE name=’Mike’;”），MySQL能够在name的索引中查找“Mike”值，然后通过指针直接转到数据文件中相应的行，准确地返回该行的peopleid（999）。在这个过程中，MySQL只需处理一个行就可以返回结果。如果没有“name”列的索引，MySQL要扫描数据文件中的所有记录，即1000个记录！显然，需要MySQL处理的记录数量越少，则它完成任务的速度就越快。 索引的类型及使用普通索引普通索引可以通过以下几种方式创建： 12345创建索引：CREATE INDEX &lt;indexname&gt; ON &lt;tablename&gt;(列);修改表：ALTER TABLE &lt;tablename&gt; ADD INDEX &lt;indexname&gt;(列);创建表的时候指定索引：CREATE TABLE &lt;tablename&gt;([...], INDEX &lt;indexname&gt;(列)); 唯一性索引这种索引和前面的“普通索引”基本相同，但有一个区别：索引列的所有值都只能出现一次，即必须唯一。唯一性索引可以用以下几种方式创建： 12345创建索引：CREATE UNIQUE INDEX &lt;indexname&gt; ON tablename(列);修改表：例如ALTER TABLE &lt;tablename&gt; ADD UNIQUE &lt;indexname&gt;(列);创建表的时候指定索引：CREATE TABLE &lt;tablename&gt; ([...], UNIQUE &lt;indexname&gt;(列)); 主键索引它是一种特殊的唯一索引，不允许有空值。一般是在建表的时候同时创建主键索引： 123创建表的时候指定索引：CREATE TABLE &lt;tablename&gt;([...], PRIMARY KEY(列)); 修改表：ALTER TABLE &lt;tablename&gt; ADD PRIMARY KEY(列); 组合索引为了形象地对比单列索引和组合索引，建立一个拥有多个字段的表： 1CREATE TABLE &lt;tablename&gt;(id INT NOT NULL, username VARCHAR(16) NOT NULL, city VARCHAR(50) NOT NULL, age INT NOT NULL); 为了进一步榨取MySQL的效率，就要考虑建立组合索引。就是将 name, city, age建到一个索引里： 1ALTER TABLE &lt;tablename&gt; ADD INDEX &lt;indexname&gt;(username(10), city, age); 那么，如果在username、city、age这三个列上分别创建单列索引，效果是否和创建一个username、city、age的多列索引一样呢？答案是否定的，两者完全不同。当我们执行查询的时候，MySQL只能使用一个索引。如果你有三个单列的索引，MySQL会试图选择一个限制最严格的索引。但是，即使是限制最严格的单列索引，它的限制能力也肯定远远低于firstname、lastname、age这三个列上的多列索引。 建立这样的组合索引，其实是相当于分别建立了下面三组组合索引： 123usernname, city, ageusernname, cityusernname 为什么没有city，age这样的组合索引呢？这是因为MySQL组合索引“最左前缀”的结果。下面我们来了解一下最左前缀。 最左前缀多列索引还有另外一个优点，它通过称为最左前缀（Leftmost Prefixing）的概念体现出来。继续考虑前面的例子，现在我们有一个username、city、age列上的多列索引，我们称这个索引为uname_city_age。当搜索条件是以下各种列的组合时，MySQL将使用uname_city_age索引： 123username，city，ageusername，cityusername 从另一方面理解，它相当于我们创建了(username，city，age)、(username，city)以及(username)这些列组合上的索引。下面这些查询都能够使用这个uname_city_age索引： 123SELECT id FROM &lt;tablename&gt; WHERE username='Mike' AND city='xian' AND age='17';SELECT id FROM &lt;tablename&gt; WHERE username='Mike' AND city='xian';SELECT id FROM &lt;tablename&gt; WHERE username='Mike'; 相反的，其他的查询则不能使用索引。 什么时候不应建立索引虽然使用索引可以提高查询速度，但它使用了额外的空间换取了查询的时间，并且如果滥用索引的话，不仅不会提高数据库的性能，反而会增大插入、更新数据的难度，从而拖累数据库的性能。因此使用索引时，以下情况可以不用建立索引： 表记录太少。 经常插入、删除、修改的表，对一些经常处理的业务表应在查询允许的情况下尽量减少索引。 数据重复且分布平均的表字段，假如一个表有10万行记录，有一个字段A只有T和F两种值，且每个值的分布概率大约为50%，那么对这种表A字段建索引一般不会提高数据库的查询速度。 创建索引时应注意的问题 索引不会包含有NULL值的列，只要列中包含有NULL值都将不会被包含在索引中，复合索引中只要有一列含有NULL值，那么这一列对于此复合索引就是无效的。所以我们在数据库设计时不要让字段的默认值为NULL。 不要过度索引，不是什么情况都非得建索引不可，比如性别可能就只有两个值，建索引不仅没什么优势，还会影响到更新速度，这被称为过度索引。 使用短索引，对串列进行索引，如果可能应该指定一个前缀长度。例如，如果有一个CHAR(255)的 列，如果在前10 个或20 个字符内，多数值是惟一的，那么就不要对整个列进行索引。短索引不仅可以提高查询速度而且可以节省磁盘空间和I/O操作。 排序的索引问题，MySQL查询只使用一个索引，因此如果WHERE子句中已经使用了索引的话，那么ORDER BY中的列是不会使用索引的。因此数据库默认排序可以符合要求的情况下不要使用排序操作；尽量不要包含多个列的排序，如果需要最好给这些列创建复合索引。 LIKE语句操作，一般情况下不鼓励使用LIKE操作，如果非使用不可，如何使用也是一个问题。like “%aaa%” 不会使用索引而like “aaa%”可以使用索引。 不要在列上进行运算，SELECT * FROM users WHERE YEAR(adddate)。 附阿里巴巴Java开发手册MySQL数据库规范索引规约【强制】使用 ISNULL() 来判断是否为NULL值。 说明: NULL 与任何值的直接比较都为 NULL。 【推荐】建组合索引的时候,区分度最高的在最左边。 说明:存在非等号和等号混合判断条件时,在建索引时,请把等号条件的列前置。如: where a &gt; ? and b = ? 那么即使 a 的区分度更高,也必须把 b 放在索引的最前列。 参考阅读MySQL索引类型总结和使用技巧以及注意事项 MySQL 索引分析和优化 如何创建索引、什么时候该创建、什么时候不应该创建 阿里巴巴Java开发手册–MySQL数据库]]></content>
      <categories>
        <category>MySQL数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>索引</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[计算机网络--Web缓存机制]]></title>
    <url>%2F2017%2F11%2F01%2F%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-Web%E7%BC%93%E5%AD%98%E6%9C%BA%E5%88%B6%2F</url>
    <content type="text"><![CDATA[注：文中的客户端指浏览器（浏览器缓存）与缓存服务器，实际上缓存服务器是代理服务器的一种，在本文中将缓存服务器归结至客户端，服务端则指源服务器。 什么是缓存Cache? 为什么人们要使用它?一个使用缓存Cache的站点会监听客户端向服务端发出的请求，并保存服务端的回应——比如HTML页面、图片等文件。接着，如果再次使用相同URL发送请求，他能够使用之前已经保存下来的反馈文件，而不是再次向服务端发出请求。 有两个主要的理由让人们使用缓存: 减少延迟 — 因为所发出的网页请求是指向更接近客户端的缓存而不再是服务端，因此请求所花费时间更短，这让网站看上去反应更快。 降低网络负荷 — 因为缓存文件可以重复使用,节省了不少的带宽（单位时间内能传输的数据量）。这也给用户省了不少流量。 缓存Caches种类缓存分为浏览器缓存（浏览器前进与后退功能的实现）和代理缓存。 浏览器缓存 Caches浏览器缓存机制，其实主要就是HTTP协议定义的缓存机制。 浏览器缓存机制主要有两种策略：Expires策略与Cache-Control策略。这两种策略可以用来判断缓存的资源是否已经过期。 Expires策略（忽略）Expires是Web服务器响应消息中的头字段，在响应HTTP请求时告诉浏览器在过期时间前可以直接从浏览器缓存或服务器缓存取数据，而无需再次请求。 Expires响应头包含日期/时间，即在此日期之后，响应过期。 示例：Expires: Wed, 21 Oct 2015 07:28:00 GMT 通过比较Expires和请求头中Date属性的值，来判断缓存是否失效。 不过Expires是HTTP 1.0的东西，现在默认浏览器均默认使用HTTP 1.1，所以它的作用基本忽略。 Cache-Control策略（重点关注）Cache-Control与Expires的作用一致，都是指明当前资源的有效期，控制浏览器是否直接从客户端取数据或是重新发请求到服务端。只不过Cache-Control的选择更多，设置更细致，如果同时设置的话，其优先级高于Expires。 首先我们来看一下HTTP协议头Cache-Control中容易混淆的几个部分： 缓存请求（Request header）指令： no-cache：告诉客户端，不管副本是否过期，使用资源副本前，一定要到服务端进行副本有效性校验（ETag）。 no-store：告诉客户端不应该缓存这个请求的Response。 max-age：指定资源的缓存时长，覆盖响应头中首次指定的缓存时长。使用请求头中的缓存时长与已缓存时长进行比较从而判断资源是否过期。 min-fresh：在指定时间内，客户端缓存不会过期。 max-stale：接收已过期响应，在max-stale过期时间内仍然有效。 only-if-cached：只请求客户端已缓存的数据，若客户端无缓存数据则返回504。 缓存响应（Response header）指令： public：该响应可以被任何中间人（比如中间代理、CDN等）缓存。 private：该响应只能应用于浏览器私有缓存中。 no-cache：提示客户端在重新验证这个缓存之前不应该使用。 no-store：指示客户端应该删除这个缓存。 max-age：资源在客户端上缓存的最长时间（相对时间）。 下面我们对上述所牵扯的一些字段再做一些补充。关于有效性校验（ETag）在下文会进行说明。 Cache-Control: no-cache客户端发送请求中如果包含no cache指令，表示浏览器不会接受缓存内容。客户端的请求必须转发给服务端。 服务端的响应中如果包含no cache指令，表示客户端不能对资源进行缓存。服务端也将不再对客户端请求中提出的资源有效性进行确认。 Cache-Control: max-age=604800客户端发送请求中包含max-age指令时，如果判定资源的缓存时间比指定时间的数值小，那么客户端可以接受缓存的资源。另外，如果指定max-age=0，那么客户端必须将请求转发给服务端。 服务端的响应中包含max-age指令时，客户端在指定的时间内将不对资源有效性再做确认，而max-age数值代表资源保存为缓存的最长时间。 重要的一点就是max-age代表的是相对时长而不是绝对时长，如下图（图片截选自：HTTP 缓存）： 上图也展示了如何使用max-age进行资源是否过期的判断。 Cache-Control: min-fresh=60要求服务端返回还未过指定时间的缓存资源。比如，当指定min-fresh=60后，过了60s的资源都无法作为响应返回。 Cache-Control: max-stale=3600可指示缓存资源，即使过期也照常接受。如果指令未指定任何参数，那么无论经过多久，客户端都会接受响应。如果指令中指定了具体数值，那么即使资源过期，但只要处于max-stale指定时间内，仍然可以被客户端接受。 Cache-Control: only-if-cached使用only-if-cached指令表示客户端仅在缓存了目标资源的情况下才会返回响应。该指令要求客户端不重新加载响应，也不会再次确认资源有效性。若发生请求客户端的缓存无响应，则返回状态码504（网关超时）。 Last-Modified与If-Modified-SinceLast-Modified：标示这个响应资源的最后修改时间。服务端在响应请求时，告诉客户端资源的最后修改时间。 If-Modified-Since：使用If-Modified-Since头将Last-Modified所标识的时间发送至服务端。服务端收到请求后与被请求资源的最后修改时间进行比对。若最后修改时间较新，说明资源又被改动过，则响应整片资源内容，返回200；若最后修改时间较旧，说明资源没有修改，则响应HTTP 304 (无需包体，节省浏览，缓存的页面仍然有效)，告知浏览器继续使用所保存的缓存。 ETag与If-None-MatchETag：服务端响应请求时，告诉客户端当前资源在服务端的唯一标识（生成规则由服务端决定）。Apache中ETag的值默认是对文件的索引节（INode），大小（Size）和最后修改时间（MTime）进行Hash后得到的。 If-None-Match：当资源过期时（使用Cache-Control标识的max-age），发现资源具有ETag声明，则再次向服务端请求时带上头If-None-Match（存储了ETag的值）。服务端收到请求后发现有头If-None-Match 则与被请求资源的相应校验串进行比对，决定返回200或304。 既生Last-Modified何生ETag？你可能会觉得使用Last-Modified已经足以让浏览器知道本地缓存的副本是否足够新，为什么还需要ETag（实体标识）呢？HTTP1.1中ETag的出现主要是为了解决几个Last-Modified比较难解决的问题： Last-Modified标注的最后修改只能精确到秒级。 如果某些文件会被定期生成，然而内容并没有任何变化，此时Last-Modified却改变了，导致文件没法使用缓存而一直向服务端请求最新的资源。 服务端没有准确获取文件修改时间。 代理缓存代理缓存就是我们所说的缓存服务器，它使用相同的原理，但却有大得多的规模，代理可以用相同的方法为几百甚至几千的使用者服务。 代理缓存是共享缓存的一种，不是只有一个人正在使用它们，而是同时有大量的用户，因此它们非常好的节约了带宽和网页延迟。 关于代理缓存的内容，可以参考Web代理与CDN缓存，更详细的内容我就不在这里细说了。 网站缓存的工作原理所有的缓存都有一整套工作机制，其中一些规则来自于HTTP协议,另一些则来自管理员。 通常来说，它们有一些共有的规则： header响应头部分可以设置是否进行缓存。 如果请求是经过HTTP认证或是SSL安全链接, 缓存无法工作。 如果符合以下条件，缓存机制的启用是通过页面的刷新引起： 时间没有超过已设置的缓存页面过期时间。 缓存是最近请求时保存的，或是缓存修改的时间也是新的。 当资源过期时（使用Cache-Control标识的max-age），发现资源具有Last-Modified声明，则再次向服务端请求时带上头If-Modified-Since，表示客户端请求时间。服务端收到请求后发现有头If-Modified-Since则与被请求资源的最后修改时间进行比对。若最后修改时间较新，说明资源又被改动过，则响应整片资源内容；若最后修改时间较旧，说明资源无新修改，则响应HTTP 304，告知客户端继续使用所保存的cache。 特定情况——比如与服务端断开连接缓存会直接作为请求的响应。 如何设置网站的缓存 CachesHTML Meta标签Tags VS HTTP报头Headers我们可以在HTML文件中的部分写入tag标签来描述该文件的属性。meta tags常常可以用来标签文件是否启用缓存和设置缓存的过期时间。Meta tags很容易使用，但是却很没效率。那是因为它们只对浏览器缓存有用，而对缓存服务器无用（代理从来不会读取HTML文件）。 总结 熟悉Cache-Control策略； 熟悉HTTP协议头Cache-Control中常用字段的意义； 熟悉Last-Modified和ETag的区别（这两个也是进行缓存验证的方法）； 掌握常用的HTTP状态码含义。 参考阅读缓存Cache详解—付潭 HTTP 通用首部字段—Unitless HTTP 缓存—mozilla MDN web docs]]></content>
      <categories>
        <category>计算机网络</category>
      </categories>
      <tags>
        <tag>Web</tag>
        <tag>缓存</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java网络爬虫--PhantomJs的使用及性能优化]]></title>
    <url>%2F2017%2F10%2F10%2FJava%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB-PhantomJs%E7%9A%84%E4%BD%BF%E7%94%A8%E5%8F%8A%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96%2F</url>
    <content type="text"><![CDATA[先说点题外话吧，在我刚开始学习爬虫的时候，有一次一个学长给了我一个需求，让我把京东图书的相关信息抓取下来。恩，因为真的是刚开始学习爬虫，并且是用豆瓣练得手，抓取了大概500篇左右的影评吧，然后存放到了mysql中，当时觉得自己厉害的不行，于是轻松的接下了这个需求。。。 然后信心满满的开始干活。。首先查看网页源代码。。。？？？我需要的东西源代码里面没有！！！然后去问了学长。学长给我说，这是AJAX产生的数据，大概听完之后我就去查了资料。发现网上大片的资料都在阐述一个道理，对于动态页面，使用PhantomJs进行抓取，但是这样效率很低。作为一个优秀的程序员，当时看见效率很低这四个字，那在我心里是绝对不能被允许的，所以我就采用了抓包的方式，查看AJAX数据所在的URL，对于这个模拟浏览器的方法也就一直搁置到现在。 但是既然知道了这个东西，哪有不去学习的道理。所以我抽出了一点时间看了一下关于Java方面使用PhantomJs的资料，现在分享给大家。 对了，其实做网络爬虫，页面上90%的数据都可以使用抓包进行获取。所以我还是鼓励大家直接请求自己所需数据所在的URL。毕竟这种方式虽然方便，但是效率低下。 JS渲染与AJAX在学习这个东西之前我们首先得了解什么是JS渲染、什么是AJAX以及为何这两种数据我们在网页源码里面获取不到。 依照我的理解，JS渲染与AJAX是一种相辅相成的关系，AJAX负责异步从服务器端获取数据，拿到数据后再使用JS进行渲染，最后呈现给用户。由于在Java中，HttpClient只能请求简单的静态页面，并不能请求到页面完全加载好后由JS调用相关代码产生的异步数据，所以我们不能直接通过HttpClient获取AJAX与JS渲染产生的数据。此时按上面所说的推荐大家直接进行网络抓包拿到AJAX数据所在的URL，或者使用本文所说的PhantomJs渲染引擎。 三大JS渲染引擎的比较在网上进行资料查阅的时候，我们经常会因为五花八门的答案而不知所措，这时候一是要保持一颗平静的心情，二就要考虑搜索问题的相关姿势，必要的时候还需要科学上网。 先不说本文所说的JS渲染引擎，单说在Java爬虫中HTTP请求的库简直就可以用五花八门来形容，Java的原生HttpURLConnection类，HttpClient第三方库等等… …当然网络上提供了这么多方法，我们必须要进行选择，那么我们肯定想选择功能强大的，使用简单的类库。此时我们就应该在网上搜索对两个类库做相关比较的问题，来进行更好的选择，而不是随便挑一个学习就完事了，这样很有可能投入的学习成本与回报不成正比。 那么相信大家在准备使用JS引擎模拟浏览器的时候，在网上看过不仅有PhantomJs，还听说过Selenium，HtmlUnit这两个具有相同功能的东西。那么我们该如何选择呢？下图截选自其他网友的博客： HtmlUnit Selenium PhantomJs 内置Rhinojs浏览器引擎，没有哪一款浏览器使用该内核，解析速度一般，解析JS/CSS差，无浏览器界面。 Seleninum + WebDriver = Selenium基于本地安装的浏览器，需打开浏览器，需要引用相应的WebDriver，正确配置webdriver的路径参数，在爬取大量js渲染页面时明显不合适。 神器，短小精悍，可本地化运行，也可作为服务端运行，基于webkit内核，性能及表现良好，完美解析绝大部分页面。 这也是我选择讲述PhantomJs的原因。 网上PhantomJs和Selenium还经常成对出现，原因是Selenium封装了PhantomJs的一部分功能，Selenium又提供了Python的接口模块，在Python语言中可以很好地去使用Selenium，间接地就可以使用PhantomJs。然而，是时候抛弃Selenium+PhantomJs了，原因之一此封装的接口很久没有更新了（没人维护了），原因之二Selenium只实现了一部分PhantomJs功能，且很不完善。 PhantomJs的使用我使用的Ubuntu16.04的开发环境，至于PhantomJs + Selenium的环境部署，网络上有大篇资料，我就在这里给大家引入一个链接，也不详细说明了：ubuntu安装phantomjs 关于PhantomJs和Selenium的介绍我也就不再详说，大家直接百度就可以了。我们直接来看一下在Java中应该怎么使用PhantomJs～ 如果你没有使用Maven的话，就在网上下载第三方jar包。我们所需要的Maven依赖如下: 12345678910111213&lt;!-- https://mvnrepository.com/artifact/org.seleniumhq.selenium/selenium-java --&gt;&lt;dependency&gt; &lt;groupId&gt;org.seleniumhq.selenium&lt;/groupId&gt; &lt;artifactId&gt;selenium-java&lt;/artifactId&gt; &lt;version&gt;2.53.1&lt;/version&gt; &lt;/dependency&gt; &lt;!-- https://mvnrepository.com/artifact/com.github.detro.ghostdriver/phantomjsdriver --&gt; &lt;dependency&gt; &lt;groupId&gt;com.github.detro.ghostdriver&lt;/groupId&gt; &lt;artifactId&gt;phantomjsdriver&lt;/artifactId&gt; &lt;version&gt;1.1.0&lt;/version&gt; &lt;/dependency&gt; 接下来我们来看一下程序到底应该怎么写： 设置请求头123456789101112//设置必要参数DesiredCapabilities dcaps = new DesiredCapabilities();//ssl证书支持dcaps.setCapability("acceptSslCerts", true);//截屏支持dcaps.setCapability("takesScreenshot", true);//css搜索支持dcaps.setCapability("cssSelectorsEnabled", true);//js支持dcaps.setJavascriptEnabled(true);//驱动支持（第二参数表明的是你的phantomjs引擎所在的路径，使用whereis phantomjs可以查看）dcaps.setCapability(PhantomJSDriverService.PHANTOMJS_EXECUTABLE_PATH_PROPERTY, "/usr/local/bin/phantomjs"); 创建phantomjs浏览器对象12//创建无界面浏览器对象PhantomJSDriver driver = new PhantomJSDriver(dcaps); 设置隐性等待12//设置隐性等待driver.manage().timeouts().implicitlyWait(1, TimeUnit.SECONDS); 因为Load页面需要一段时间，如果页面还没加载完就查找元素，必然是查找不到的。最好的方式，就是设置一个默认等待时间，在查找页面元素的时候如果找不到就等待一段时间再找，直到超时。 以上三点是使用PhantomJs时需要注意的地方，大家可以看一下大致的整体程序： 12345678910111213141516171819202122232425262728293031323334353637383940import org.openqa.selenium.By;import org.openqa.selenium.WebDriver;import org.openqa.selenium.WebElement;import org.openqa.selenium.phantomjs.PhantomJSDriver;import org.openqa.selenium.phantomjs.PhantomJSDriverService;import org.openqa.selenium.remote.DesiredCapabilities;import java.util.concurrent.TimeUnit;/** * Created by hg_yi on 17-10-11. */public class phantomjs &#123; public static void main(String[] args) &#123; //设置必要参数 DesiredCapabilities dcaps = new DesiredCapabilities(); //ssl证书支持 dcaps.setCapability("acceptSslCerts", true); //截屏支持 dcaps.setCapability("takesScreenshot", true); //css搜索支持 dcaps.setCapability("cssSelectorsEnabled", true); //js支持 dcaps.setJavascriptEnabled(true); //驱动支持（第二参数表明的是你的phantomjs引擎所在的路径） dcaps.setCapability(PhantomJSDriverService.PHANTOMJS_EXECUTABLE_PATH_PROPERTY, "/usr/bin/phantomjs-2.1.1-linux-x86_64/bin/phantomjs"); //创建无界面浏览器对象 PhantomJSDriver driver = new PhantomJSDriver(dcaps); //设置隐性等待（作用于全局） driver.manage().timeouts().implicitlyWait(1, TimeUnit.SECONDS); //打开页面 driver.get("--------------------------------"); //查找元素 WebElement element = driver.findElement(By.id("img_valiCode")); System.out.println(element.getAttribute("src")); &#125;&#125; 我成功的抓取到了网页源码里面没有的数据～ 关于上面使用到的PhantomJSDriver类的相关API，大家直接看这篇资料即可，应该可以满足你的日常需求了：webdriver API中文版（WebDriver的API同样适用于PhantomJSDriver）。 PhantomJs的性能优化我们都知道使用PhantomJs这种无头浏览器进行网页源码的抓取是非常费时的，所以当我们决定使用这个工具并且对抓取速度还有一定要求的时候，就需要掌握对PhantomJs进行性能优化的能力。 设置参数Google，Baidu半天，还看了一点官方文档，还是找不到PhantomJs相关的Java调用API文档，好吧，先扔一篇Python的，以后找到这方面的内容再进行补充吧～～～ 【phantomjs系列】Selenium+Phantomjs性能优化]]></content>
      <categories>
        <category>Java网络爬虫</category>
      </categories>
      <tags>
        <tag>网络爬虫</tag>
        <tag>PhantomJs</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发--DCL双检查锁机制中的volatile]]></title>
    <url>%2F2017%2F08%2F29%2FJava%E5%B9%B6%E5%8F%91-DCL%E5%8F%8C%E6%A3%80%E6%9F%A5%E9%94%81%E6%9C%BA%E5%88%B6%E4%B8%AD%E7%9A%84volatile%2F</url>
    <content type="text"><![CDATA[作为被面试官最喜欢问到的23种设计模式之一，我们不得不熟练掌握单例模式以及洞悉多线程环境下，单例模式所存在的非线程安全问题以及它的解决方式。 注：这篇文章主要讲述多线程环境下单例模式存在的非线程安全问题，并不详细讲述单例模式。 何为单例模式首先我们先大概了解一下单例模式的定义： 单例类只能有一个实例。 单例类必须自己创建自己的唯一实例。 单例类必须给所有其他对象提供这一实例。 单例模式的应用非常广泛，例如在计算机系统中，线程池、缓存、日志对象、对话框、打印机、显卡的驱动程序对象常被设计成单例。这些应用都或多或少具有资源管理器的功能。每台计算机可以有若干个打印机，但只能有一个Printer Spooler，以避免两个打印作业同时输出到打印机中。选择单例模式就是为了避免不一致状态。 单例模式的实现有三种方式：饿汉式（天生线程安全），懒汉式，登记式（可忽略）。 对于上面单例模式的实现方式我在这里不做过多介绍，我们着重来看一下懒汉式在多线程环境下出现的问题以及它的解决策略。 设计线程安全的单例模式DCL双检查锁机制其实我觉得能看这篇文章的伙伴们对设计线程安全的单例模式都是有一定的了解，所以对于解决非线程安全的单例模式的3种方式也应该有些了解。我们再来总结一下这三种方式：声明synchronized关键字（同步代码块），DCL双检查锁机制，静态内置类的实现。 关于第一种方式，我觉得大家应该没有什么疑惑，所以我在这里也就不再讲述了，咱们来看一下我在学习双检查锁机制过程中遇到的问题，是否和你一样。 这是单例类，注意private volatile static MyObject myObject这句话。 123456789101112131415161718192021222324252627public class MyObject &#123; private volatile static MyObject myObject; private MyObject() &#123; &#125; public static MyObject getInstance() &#123; try &#123; if (myObject != null) &#123; &#125; else &#123; // 模拟在创建对象之前做的一些准备工作 Thread.sleep(3000); synchronized (MyObject.class) &#123; if (myObject == null) &#123; myObject = new MyObject(); &#125; &#125; &#125; &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; return myObject; &#125;&#125; 线程类： 123456public class MyThread extends Thread &#123; @Override public void run() &#123; out.println(MyObject.getInstance().hashCode()); &#125;&#125; 测试类： 1234567891011public class Run &#123; public static void main(String[] args) &#123; MyThread thread1 = new MyThread(); MyThread thread2 = new MyThread(); MyThread thread3 = new MyThread(); thread1.start(); thread2.start(); thread3.start(); &#125;&#125; 最终结果： 123773715418773715418773715418 我们可以看到，使用了Double-Check，使得在多线程环境下，也只能取得类的唯一实例。但是不知道你有没有和我一样的疑惑，看我上面着重提出来的那句话，我们为什么在声明MyObject对象的时候还要给它加上volatile关键字？我们在Double-Check下已经加入了synchronized关键字，既然synchronized已经起到了多线程下原子性、有序性、可见性的作用，为什么还要加volatile呢？要解决这个问题，我们需要深入了解volatile关键字的特性，它不仅可以使变量在多个线程之间可见，而且它还具有禁止JVM进行指令重排序的功能，具体请参见JVM–从volatile深入理解Java内存模型这篇文章。 首先，我们需要明白的是：创建一个对象可以分解为如下的3行伪代码： 12345memory=allocate(); // 1.分配对象的内存空间ctorInstance(memory); // 2.初始化对象instance=memory; // 3.设置instance指向刚分配的内存地址。// 上面3行代码中的2和3之间，可能会被重排序导致先3后2 也就是说，myObject = new MyObject()这句话并不是一个原子性操作，在多线程环境下有可能出现非线程安全的情况。 现在我们先假设一下，如果此时不设置volatile关键字会发生什么。 假设两个线程A、B，都是第一次调用该单例方法，线程A先执行myObject = new MyObject()，该构造方法是一个非原子操作，编译后生成多条字节码指令，由于JAVA的指令重排序，可能会先执行myObject的赋值操作，该操作实际只是在内存中开辟一片存储对象的区域后直接返回内存的引用，之后myObject便不为空了，但是实际的初始化操作却还没有执行，如果就在此时线程B进入，就会看到一个不为空的但是不完整（没有完成初始化）的MyObject对象，所以需要加入volatile关键字，禁止指令重排序优化，从而安全的实现单例。 因此我们以后应该记得，在使用Double-Check的时候，那个volatile至关重要。并不是可要可不要的。]]></content>
      <categories>
        <category>Java并发</category>
      </categories>
      <tags>
        <tag>并发</tag>
        <tag>单例模式</tag>
        <tag>DCL双检查</tag>
        <tag>volatile关键字</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM--从volatile深入理解Java内存模型]]></title>
    <url>%2F2017%2F08%2F16%2FJVM-%E4%BB%8Evolatile%E6%B7%B1%E5%85%A5%E7%90%86%E8%A7%A3Java%E5%86%85%E5%AD%98%E6%A8%A1%E5%9E%8B%2F</url>
    <content type="text"><![CDATA[在上一篇博客JVM–解析Java内存区域及数据的内存分配与线程安全之间的一些联系中也说到了，想要理解volatile关键字，我们需要掌握Java虚拟机运行时数据区的相关知识，但是这还不够，只有理解了Java的内存模型，我们才能开始讲述volatile，而Java虚拟机运行时数据区是掌握Java内存模型的基础，所以如果你还没有看上一篇博客，请点击上方链接～～～ 引言既然本节讲述volatile关键字，那么就先抛个砖引个玉（以下代码在64位jdk1.8下进行测试，不同jdk版本运行结果有可能不一样）： 123456789101112131415161718192021public class RunThread implements Runnable &#123; private boolean isRunning = true; public boolean isRunning() &#123; return isRunning; &#125; public void setRunning(boolean isRunning) &#123; this.isRunning = isRunning; &#125; @Override public void run() &#123; out.println("进入run了"); while (isRunning) &#123; &#125; out.println("线程被停止~"); &#125;&#125; 123456789101112131415public class Run &#123; public static void main(String[] args) &#123; RunThread Runthread = new RunThread(); Thread thread = new Thread(Runthread); thread.start(); try &#123; Thread.sleep(1000); Runthread.setRunning(false); out.println("已经赋值为false"); &#125; catch (InterruptedException e) &#123; e.printStackTrace(); &#125; &#125;&#125; 大家认为上面的代码能够停下来吗？答案是不行。 我先给大家说明几点原因吧，具体的细节我接下来会慢慢讲述。首先来解释一下为什么我会说在64位jdk1.8下面测试有效，而其他版本运行结果会有所不同。 这里涉及到一点JVM的知识，但是并不难懂，你只要记住就行了。出现这样结果的直接原因，并不是根本原因，是在于我们使用64位jdk1.8的时候只能运行在Server模式下。你说什么是Server模式？别急，看下面： JVM Server模式与client模式启动，最主要的差别在于：-Server模式启动时，速度较慢，但是一旦运行起来后，性能将会有很大的提升。原因是：当虚拟机运行在-client模式的时候，使用的是一个代号为C1的轻量级编译器，而-server模式启动的虚拟机采用相对重量级，代号为C2的编译器。 C2比C1编译器编译的相对彻底，服务起来之后，性能更高。然后如果是64位的jdk 1.8，只能运行在Server模式下。 这种模式为什么会造成程序没办法停止呢？一个线程明明对共享变量作出了修改，其它线程却没办法看到，这不是有悖于上一节所说的吗。 在这里先浅显的说明一下Server模式到底会对线程造成什么样的影响。当程序被启动时，变量private boolean isRunning存在于共享堆与线程的私有栈之中。并且当JVM被设置为-server模式时，为了线程的运行效率，线程会一直在私有堆栈中取得isRunning的值是true。而代码Runthread.setRunning(false)虽然被执行，更新的却是共享变量也就是公共堆里面的isRunning，因此一直都是死循环状态，线程无法停止。 看完上面这句话，也许你已经大概知道了为什么线程没有停止，但是等等，好像有什么不对，为什么在程序启动时，共享变量private boolean isRunning会同时存在于共享堆与线程的私有栈之中呢？好了，要明白这个问题，就是这篇博客将要讲述的重点，我们也会在搞清楚这个知识点之后，再回头看volatile。 什么是Java内存模型既然我们要讲Java的内存模型，那么首先肯定要知道它是什么。 首先来说一说“内存模型”这个抽象的概念。 我们知道如今计算机处理的任务都不可能是单靠处理器就能完成的，它至少要完成与内存的交互，如读取数据，存储运算结果等。但是存储设备与处理器的运算速度都是几个数量级的差距，所以当计算机在进行I/O操作的时候，处理器势必会等待这样缓慢的内存读写。于是聪明的人们就在计算机中加入了一层读写速度尽可能接近处理器的高速缓存。它的运行机制以及功能我就不进行描述了，直接说它所带来的问题，虽然它很好的解决了处理器与存储的速度矛盾，但是它也为计算机系统带来更高的复杂度以及一个新问题：缓存一致性。 缓存一致性： 在多处理器系统中，每个处理器都有自己的高速缓存，而它们又共享同一主内存，当多个处理器的运算任务都涉及同一块主内存区域时，而它们各自的缓存数据又不一致，那么同步回主内存时以谁的缓存数据为主呢？ 为了解决这个问题，需要各个处理器访问缓存时都遵循一些协议，在读写时要根据协议来进行操作，这些协议的种类很多，我就不举例子了。而内存模型就可以理解为在特定的操作协议下，对特定的内存或高速缓存进行读写访问的过程抽象。 上图说明了处理器，高速缓存，主内存之间的交互关系。 Java虚拟机规范中试图定义一种Java内存模型（JMM）可以用来屏蔽掉各种硬件和操作系统的内存访问差异，以实现让Java程序在各种平台下都能达到一致的内存存储效果。 主内存与工作内存我们已经了解了Java的内存模型是什么以及它有什么用，现在就来谈一谈主内存与工作内存，这也是理解volatile关键字的关键所在。 Java内存模型规定了所有变量都存储在主内存中，注意，这里说的变量与平常Java编程中说的变量有所区别，它包括了实例字段，静态字段和构成数组对象的元素，它不包括局部变量与方法参数，因为后者是线程私有的。也就是说，我们可以这样理解，除过线程私有的局部变量和方法参数之外，所有的变量都存在于主内存中。（本篇博客中的所有变量都特指这种共享变量） 忘了说一点，我们现在讨论的主内存，只是虚拟机内存的一部分，而虚拟机内存也只是物理内存的一部分。 上面说了主内存，那么再来谈一谈工作内存，上面讲的主内存可以和计算机中的物理内存进行类比，而工作内存可与高速缓存类比。工作内存是 JMM 的一个抽象概念，并不真实存在。它涵盖了缓存，写缓冲区，寄存器以及其它的硬件和编译器优化。 关于上面说到的缓存和缓冲区的区别，我特地百度了一下，发现了一名知乎用户的回答Cache 和 Buffer 都是缓存，主要区别是什么？ 每个线程都有一个自己的工作内存，该内存空间保存了被该线程使用到的变量的主内存副本，线程对变量的所有操作（读取，赋值等）都必须在工作内存中进行，而不直接读写主内存中的变量。看了这段话也许你会问，那假如线程访问一个10MB的对象，难道也会把这10MB的内存复制一份拷贝出来？这当然是不可能的，它有可能会将对象的引用，对象中某个线程访问到的字段拷贝出来，但绝不会将整个对象拷贝一次。 上图是线程，工作内存，主内存三者之间的交互关系。 我觉得你现在一定有一个疑惑，那就是JMM和Java虚拟机运行时的数据区到底有什么区别。 引用一段《深入理解Java虚拟机》上的解释： 这里所讲的主内存，工作内存与Java内存区域中的Java堆，栈，方法区等并不是同一个层次的划分，这两者基本上是没有关系的。如果两者一定要勉强对应起来，那么变量，主内存，工作内存依次对应Java堆中对象实例数据部分，工作内存对应虚拟机栈中的部分区域。从更低层次上来说，主内存直接对应于物理硬件的内存，工作内存优先存储于寄存器以及高速缓存。 结合上面的这些官方定义，我们大致总结起来其实就一句话，对于Java内存模型来说，只不过就是它在每个线程访问共享变量的时候，为了提高处理器处理数据的效率，增加了一个并不真实存在的，概念上的工作内存，每个线程对共享变量的访问相当于都是先将主内存中的变量拷贝到自己的工作内存中，然后对自己工作内存中存在的变量进行读写，操作完之后将它同步回主内存罢了。 内存交互操作既然上面讲到了主内存和工作内存，现在我们再来详细讨论一下一个变量是怎么从主内存拷贝到工作内存的，而工作内存的变量又是怎么同步回主内存的呢？ 我们先来了解一下JMM定义的8种原子性操作，看一下图解： 上图说明了工作内存和主内存之间交互的步骤，还有图上缺少的两种原子性操作分别是lock锁定，unlock解锁。由于这两个操作和内存之间的交互并没有关系，所以分开来说。 我们先来说一下图中的每个操作都是干嘛的： read（读取）：作用于主内存变量，把变量的值从主内存传输到线程的工作内存 load（载入）：作用于主内存变量，把read操作从主内存中得到的变量值放入工作内存的变量副本中 use（使用）：作用于工作内存变量 assign（赋值）：作用于工作内存变量 store（存储）：作用于工作内存变量，将工作内存中一个变量的值传送回主内存 write（写入）：作用于主内存变量，将工作内存中得到的变量值放入主内存的变量中 在《深入理解Java虚拟机》一书中，对于use和assign的描述涉及到了执行引擎，所以我在上面并没有详细的说明。另外我发现《Java多线程编程核心技术》这本书也对上面的原子性操作做了一个简明的说明，所以再来看看它是怎么说的： read与load：从主存复制变量到当前线程工作内存 use与assign：执行代码，改变共享变量值 store与write：用工作内存数据刷新主存对应变量的值 另外关于上面所说的lock与unlock，它实际就是我们平常在代码中写的同步块synchronized，说点题外话，同步块既保证多线程安全时所需要的原子性，而且也保证了可见性与有序性，所以我们经常可以看到程序员在滥用synchronized，虽然它的确比较“万能”，但是越“万能”的并发控制，通常也会伴随越大的性能影响。扯远了。。。 volatile现在我们对Java内存模型已经有了一定的认识，这个时候我们再来谈谈volatile这个轻量级同步机制。 首先来看volatile的作用： 强制从公共堆中取得变量的值，而不是从线程的私有堆栈中取得变量的值。如果我们需要用一张图来描述这个过程的话，就是这样： 从图中可以看到，volatile保证了变量的新值能立即同步到主内存，以及每次使用之前立即从主内存刷新。因此可以说volatile保证了多线程操作时变量的可见性，而普通变量不能保证这一点。 说了这么多，现在的你应该可以明白引言中的代码为何不会停止，并且就算以后碰到上面那种格式的代码也应该知道错误出在哪然后加以改正。 指令重排序与内存屏障解决了上面的问题，我们并不能结束，因为Java的内存模型还有很多东西都没有提到，当然，博主在学习的过程中看到有人说过光是JMM的知识就可以写一本书，所以在这里也只是给大家提一下，并不能完全剖析JMM，其中有错误的地方还是欢迎大家指出。而且，volatile也并没有讲述完毕，现在只是将上面那个代码的问题解决了而已。 如果你还想要探究volatile的其它特性，这些东西你不得不去掌握。 初看这两个词语，完全不知道它说的是什么意思。这算是Java比较底层的相关知识了，没听过也很正常，但也不用怕，让我们来一点点攻克这两个看起来很不友好的东西。 指令重排序 指CPU采用了允许将多条指令不按程序规定的顺序分开发送给各相应电路单元处理。但并不是说指令任意重排，CPU需要能正确处理指令依赖情况以保证程序能得出正确的执行结果。 什么叫指令依赖，举个例子： 假设指令1将地址A中的值加10，指令2将地址A中的值乘以2,指令3将地址B中的值减去3，这时指令1和指令2是有依赖的，它们之间不能重排，明显(A+10)2与A2+10不等，但是指令3可以重排到指令1，2之前或中间。 这就是指令重排序，设计它的目的就是为了提高程序并发能力，具体参见指令重排序。这篇博客详细但很浅显的讲述了指令重排序，我也就不过多叙述了。 内存屏障这个东西还是给大家放一个比较可靠的链接吧，由于博主水平有限，对于自己不太清楚的东西也不敢给大家胡乱总结，所以还是将我学习内存屏障中读起来不错的博客链接分享给大家：内存屏障 看完这篇博客我的感觉就是内存屏障其实还是一种用于保证变量可见性的技术手段，它通过store屏障和load屏障保证更改后的数据能及时的刷进缓冲区，保证各个线程可以从缓冲区中读到最新的数据。 通过对volatile进行反汇编，我们可以看到volatile实际上就使用到了内存屏障技术来保证其变量的修改对其他CPU立即可见。 happends-before（先行发生）原则按照上面的惯例，这些东西只要分开来写都可以单独写成一篇文章，加上我自觉目前不会比别人写的更通俗易懂并且保证博客中不出现错误，所以我还是扔一篇我学习之后觉得还不错的博文：俗解happends-before 在《深入理解Java虚拟机》这本书中，感觉也没太说清楚这个原则到底是怎么一回事，它还列举了JMM中存在的8条“天然的”先行发生关系，并且说如果两个操作之间的关系无法从这8条规则推导出来，那么它们就没有顺序性保障，虚拟机可以对它们随意进行重排序。说实话对于那八条规则目前来说我懒得去记，因为我觉得就算我死记下来那些也不是我的东西，如果大家有兴趣的话，可以下去找找看那八条规则，他说可以通过这8条规则一揽子解决并发环境下两个操作之间是否可能存在冲突的所有问题，我看了一道例题，觉得过于抽象并且死板，我觉得是我目前还没有领会到精髓吧。 再谈volatile关于volatile，还有一些细节值得我们去考虑，比如volatile只能保证数据的可见性，不能保证原子性；而synchronized可以保证原子性，也可以间接保证可见性，因为它会将工作内存和主内存中的数据做同步。 关于volatile不保证原子性，最明显的例子就是i++这样的表达式了。我们在来说一下i++的操作步骤： 从内存中读取i的值； 计算i的值； 将i的值写到内存中。 我们可以看到，在多线程环境下，对于i++这种操作，即使对i使用volatile，也只是表示在read与load之后加载内存中的最新值，但如果主内存中的i还在发生修改，然而线程工作内存中的值已经加载，不会产生对应的变化，也就是说线程的工作内存和主内存中的变量不同步，所以计算出来的结果还是会和预期不一样，因此volatile无法保证操作的原子性。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>JVM</tag>
        <tag>Java内存模型</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[JVM--解析Java内存区域及数据的内存分配与线程安全之间的一些联系]]></title>
    <url>%2F2017%2F08%2F08%2FJVM-%E8%A7%A3%E6%9E%90Java%E5%86%85%E5%AD%98%E5%8C%BA%E5%9F%9F%E5%8F%8A%E6%95%B0%E6%8D%AE%E7%9A%84%E5%86%85%E5%AD%98%E5%88%86%E9%85%8D%E4%B8%8E%E7%BA%BF%E7%A8%8B%E5%AE%89%E5%85%A8%E4%B9%8B%E9%97%B4%E7%9A%84%E4%B8%80%E4%BA%9B%E8%81%94%E7%B3%BB%2F</url>
    <content type="text"><![CDATA[最近一直在看《Java多线程编程核心技术》的第二章，主要讲的是线程共享变量与线程私有变量以及如何写出线程安全的代码。看这部分一开始没太注意，只是记住了一条规则，“类中的成员变量，也叫实例变量，也叫全局变量，它是非线程安全，是所有线程共享的变量，定义在方法中的私有变量是线程安全的，是每个线程私有的”。很好理解不是吗，然后一帆风顺的看到了关于volatile这部分的知识，看过之后我陷入了凌乱。。。关于这部分我之后进行总结，而现在我觉得你如果真的想写出线程安全的代码，那么Java的内存分配以及布局就是我们需要掌握的基础。为此，我粗略的看了一下《深入理解Java虚拟机》这本书的第二章，并且查阅了一些资料，现在汇总整理如下。 注：学习这部分内容之前如果你对进程的内存映像或数据在内存中的分配有大概的了解，建议你先忘记它们，因为这是讲Java虚拟机运行时的数据区，和之前的知识并不相同，所以学习的时候不要拿自己以前所了解的知识进行比较与衡量。 Java虚拟机运行时的数据区先来看一张图片： 在这里我们只需要关注线程共享区中的堆，以及线程独占区中的虚拟机栈，就是我们平时说的栈，只是这种说法在JVM中并不严谨，正确的应该说是虚拟机栈中局部变量表部分。（被static关键字声明的东西就是存储在方法区中） 其次，我在查阅资料的时候，看到网上很多资料都说栈中的数据共享而堆中的数据不共享，起初我还以为是博主写错了，了解之后发现我们考虑问题的立场不同，我们今天要讨论的是堆或栈内的数据对多线程是否共享，而他们所说的栈内数据共享则是在创建新变量时为节省内存空间而采取的一种措施，被称为Slot复用，两个完全是不同的东西。具体的区别在后面给大家说明。 虚拟机栈与堆看了上面的图片，对JVM的数据区也有了个大概的认识，我们来详细说一下虚拟机栈和堆中到底都存储的是哪些数据。 下面这段话摘自网上他人博客： 基本类型(primitive types), 共有8种，即int, short, long, byte, float, double, boolean, char(注意，并没有string的基本类型)。这种类型的定义是通过诸如int a = 3; long b = 255L;的形式来定义的，称为自动变量。值得注意的是，自动变量存的是字面值，不是类的实例，即不是类的引用，这里并没有类的存在。如int a = 3; 这里的a是一个指向int类型的引用，指向3这个字面值。这些字面值的数据，由于大小可知，生存期可知(这些字面值固定定义在某个程序块里面，程序块退出后，字段值就消失了)，出于追求速度的原因，就存在于栈中。 最后需要补充的是所有对象的引用也都存在于栈中，而实际的对象本身是存储在堆中的，我们这时候倒可以将引用理解为一个指针，它指向了我们在堆中创建的对象。 再来说明前面说的栈内数据共享是什么东西，还是摘自他人博客： 另外，栈有一个很重要的特殊性，就是存在栈中的数据可以共享。假设我们同时定义int a = 3; int b = 3; 编译器先处理int a = 3；首先它会在栈中创建一个变量为a的引用，然后查找有没有字面值为3的地址，没找到，就开辟一个存放3这个字面值的地址，然后将a指向3的地址。接着处理int b = 3；在创建完b的引用变量后，由于在栈中已经有3这个字面值，便将b直接指向3的地址。这样，就出现了a与b同时均指向3的情况。 特别注意的是，这种字面值的引用与类对象的引用不同。假定两个类对象的引用同时指向一个对象，如果一个对象引用变量修改了这个对象的内部状态，那么另一个对象引用变量也即刻反映出这个变化。相反，通过字面值的引用来修改其值，不会导致另一个指向此字面值的引用的值也跟着改变的情况。 至于堆中存储的数据，只要记住一句话，所有new出来的变量都存储在堆中。但是在这里我们还要考虑String类型的特殊性和自动拆箱与装箱的相关概念，好吧，概念实在太多了。我们之后再说这些问题。我们先来看两个问题：（下面讲述的问题只针对同一个对象，在多个不同的对象中，堆中的数据也是不共享的） 基本数据类型的成员变量放在jvm的哪块内存区域里？看了上面的概念之后，我们知道基本数据类型应该是存放在虚拟机栈中的。如果你也认为是虚拟机栈中，那么根据上面的图片，它应该属于线程独占区啊，怎么会属于共享变量呢？ 与上面的问题对应，方法中新建的非基本数据类型放在jvm的哪块内存区域里？如果我们在方法中new了一个对象，按道理来说，new 出来的对象都是存放在堆上的，而根据上图我们又发现… …Java堆是属于线程共享区的。这是怎么一回事呢？ 首先回答第一个问题： 123class &#123; private int i;&#125; 基本数据类型放在栈中，这一概念的确没有错，但是这个说法又不是很准确，如上面的代码，基本数据类型的全局变量i，它是存放在java堆中。因为它不是静态的变量，不会独立于类的实例而存在，而该类实例化之后，放在堆中，当然也包含了它的属性i。因此成员变量就算是基本数据类型也共享。 是不是觉得有点绕？然而事实的确是这样。 再来看第二个问题，对象的确是存放在堆中，但我们也说了线程不安全只针对单例模式的成员变量，而此时如果对象被定义在了方法之中，那么当每个线程调用一次方法都会新创建一个对象，这些对象都属于每个线程所私有，所以虽然对象本身存在于堆中，但也并不共享。至于基本类型由于每个线程执行时将会把局部变量放在各自栈帧的工作内存中，线程间不共享，故不存在线程安全问题。 最后结合Java虚拟机运行时的数据区总结一下，就是对于同一对象（单例模式），成员变量共享，局部变量不共享。 运行时常量池运行时常量池存在于方法区中，常量池里面主要存储字符串常量和基本类型常量（public static final）。 对于字符串：其对象的引用都是存储在栈中的，如果是编译期已经创建好(直接用双引号定义的)的就存储在常量池中，如果是运行期（new出来的）才能确定的就存储在堆中。对于equals相等的字符串，在常量池中永远只有一份，在堆中有多份。 如以下代码： 123456String s1 = "china";String s2 = "china";String s3 = "china";String ss1 = new String("china");String ss2 = new String("china");String ss3 = new String("china"); 这里解释一下，对于通过 new 产生一个字符串（假设为 ”china” ）时，会先去常量池中查找是否已经有了 ”china” 对象，如果没有则在常量池中创建一个此字符串对象，然后堆中再创建一个常量池中此 ”china” 对象的拷贝对象。 也就是有道面试题： String s = new String(“xyz”); 产生几个对象？ 一个或两个。如果常量池中原来没有 ”xyz”, 就是两个。如果原来的常量池中存在“xyz”时，就是一个。 对于基础类型的变量和常量：变量存储在栈中，常量存储在常量池中。 如以下代码： 123456int i1 = 9; int i2 = 9; int i3 = 9; public static final int INT1 = 9; public static final int INT2 = 9; public static final int INT3 = 9; 自动拆箱与自动装箱12345678910111213141516171819public class JVM &#123; public static void main(String[] args) &#123; Integer a = 1; Integer b = 2; Integer c = 3; Integer d = 3; Integer e = 321; Integer f = 321; Long g = 3L; out.println(c == d); out.println(e == f); out.println(c == (a+b)); out.println(c.equals(a+b)); out.println(g == (a+b)); out.println(g.equals(a+b)); &#125;&#125; 来看一下程序的运行结果是否符合你的预期： 123456truefalsetruetruetruefalse 首先我们先来了解一下包装类数据： 包装类数据，如Integer, String, Double，Long等将相应的基本数据类型包装起来的类。这些类数据全部存在于堆中。 然后我们在对自动装箱与拆箱做个了结： 包装类的“==”运算在不遇到算术运算的情况下不会自动拆箱，equals()方法不处理数据转型的关系。 在自动装箱时，把int变成Integer的时候，是有规则的，当你的int的值在-128-IntegerCache.high(127) 时，返回的不是一个新new出来的Integer对象，而是一个已经缓存在堆中的Integer对象，（我们可以这样理解，系统已经把-128到127之 间的Integer缓存到一个Integer数组中去了，如果你要把一个int变成一个Integer对象，首先去缓存中找，找到的话直接返回引用给你就 行了，不必再新new一个），如果不在-128-IntegerCache.high(127) 时会返回一个新new出来的Integer对象。 对于第二点，不仅Integer有这种特性，其它包装类数据也具有，我们可以看一下Long的源码： 12345678910private static class LongCache &#123; private LongCache()&#123;&#125; static final Long cache[] = new Long[-(-128) + 127 + 1]; static &#123; for(int i = 0; i &lt; cache.length; i++) cache[i] = new Long(i - 128); &#125;&#125; 在了解了上面的概念之后，我相信你已经基本能够正确解释代码运行的结果了。 最后我来解释一下最后两个的运行结果为什么是true与false，先来看一下后两句话在进行编译之后在.class文件中的样子吧： 12System.out.println(g.longValue() == (long)(a.intValue() + b.intValue()));System.out.println(g.equals(Integer.valueOf(a.intValue() + b.intValue()))); 我们可以看到编译器对out.println(g == (a+b));进行编译的时候，进行了拆箱与向上转型的操作，所以此时比较的仅仅是两个变量的字面值，与基本数据类型的比较是一样的，所以是true，而最后仍然比较的是对象中的数据并且对a没有进行向上转型，Long中存在的数据肯定就和Integer中存在的数据不等了，所以为false。 再说一点，我们能将字面值直接赋给Integer类是因为Java语法糖的存在，实际上Integer a = 1在经过编译之后是这样的：Integer a = new Integer(1)，语法糖帮助我们简化了语法。]]></content>
      <categories>
        <category>JVM</category>
      </categories>
      <tags>
        <tag>虚拟机</tag>
        <tag>内存区域</tag>
        <tag>线程安全</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java并发--详解this与Thread.currentThread()的区别]]></title>
    <url>%2F2017%2F08%2F04%2FJava%E5%B9%B6%E5%8F%91-%E8%AF%A6%E8%A7%A3this%E4%B8%8EThread-currentThread-%E7%9A%84%E5%8C%BA%E5%88%AB%2F</url>
    <content type="text"><![CDATA[注：本系列博客参考《Java多线程编程核心技术》，主要是对书上的知识点进行总结，并记录学习过程。 一直对并发这块比较感兴趣，也到了系统学习Java多线程的时间。目前所学习的书籍是《Java多线程编程核心技术》，买回来之后听说这本书不怎么样，豆瓣评分也就7点几，目前读完了第一章，感觉确实不是很好，但是也不算太坑，总的来说还是可以入手的。好了，废话不多说，开始正题。 首先我们来看一份代码： 123456789101112131415161718public class CountOperate extends Thread &#123; public CountOperate() &#123; out.println("CountOperate---begin"); out.println("Thread.currentThread.getName()=" + Thread.currentThread().getName()); out.println("this.name()=" + this.getName()); out.println("Thread.currentThread()==this :"+ (Thread.currentThread() == this)); out.println("CountOperate---end"); &#125; @Override public void run() &#123; out.println("run begin"); out.println("Thread.currentThread().getName()=" + Thread.currentThread().getName()); out.println("this.getName=" + this.getName()); out.println("Thread.currentThread()==this :"+ (Thread.currentThread() == this)); out.println("run---end"); &#125;&#125; 123456789public class Run &#123; public static void main(String[] args) &#123; CountOperate countOperate = new CountOperate(); Thread thread = new Thread(countOperate); thread.setName("A"); thread.start(); &#125;&#125; 来看一下运行结果是否符合你的预期： 1234567891011CountOperate---beginThread.currentThread.getName()=mainthis.name()=Thread-0Thread.currentThread()==this :falseCountOperate---endrun beginThread.currentThread().getName()=Athis.getName()=Thread-0Thread.currentThread()==this :falserun---end 在还没有启动CountOperate线程的时候，调用这段代码的是main线程，所以： 1Thread.currentThread.getName()=main 这是正常的，但是 this.name()=Thread-0 这是个什么东西？看一下Thread源码吧： 123public Thread() &#123; init(null, null, "Thread-" + nextThreadNum(), 0);&#125; 参数的含义不明确？好，我们再来看一下init方法的声明： 1private void init(ThreadGroup g, Runnable target, String name, long stackSize); 来解释一下各个参数的含义： ThreadGroup: 线程组 Runnable target: the object whose {@code run} method is invoked when this thread is started. If {@code null}, this thread’s run method is invoked. （源码解释，建议百度翻译，也是产生this和Thread.currentThread区别的原因）因为在Thread源码中，Thread实际上操作了Runable，所以此参数也接受Thread对象，也就是说此参数也可以是继承了Thread的线程类 String name: 线程名 long stackSize: 新线程所需的堆栈大小，或0表示该参数将被忽略。 这下，我们知道Thread-0是怎么来的了，String name生成名称的规则是：“Thread-”加上创建的线程的个数（第几个）。默认从0开始，main线程是默认就有的，所以并不计数。 然后 Thread.currentThread()==this :false 这个也好理解，this代表的是CountOperate对象实例，而Thread.currentThread() 得到的是main，所以为false。 代码继续向下执行… … 重点来了： 123Thread.currentThread().getName()=Athis.getName()=Thread-0Thread.currentThread()==this : false Thread.currentThread().getName() 得到的是执行这段代码的线程名，我们已经设置为了A，没问题。但是这里的this.getName()=Thread-0为什么没变？this代表的实例是什么？别着急，我们再来看一看源码： 123public Thread(Runnable target) &#123; init(null, target, "Thread-" + nextThreadNum(), 0);&#125; 然后我们结合上面源码中target的注释再来看一看run方法的源码： 12345public void run() &#123; if (target != null) &#123; target.run(); &#125; &#125; 明白了吧？只要target不为空，那么它最后还是会调用target的run方法。是的，现在问题已经解决了！没反应过来？那我们回过头来看一下Thread thread = new Thread(countOperate)，从源码来讲，它最后还是会执行countOperate.run()，而this取得的就是代表当前实例的引用，所以，this.getName() 还是会打印Thread-0。那么最后一个false也就明白了，两个取得的都不是同一个对象，自然Thread.currentThread()和this也就不等了。]]></content>
      <categories>
        <category>Java并发</category>
      </categories>
      <tags>
        <tag>并发</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查找--二叉查找树分析与实现]]></title>
    <url>%2F2017%2F08%2F03%2F%E6%9F%A5%E6%89%BE-%E4%BA%8C%E5%8F%89%E6%9F%A5%E6%89%BE%E6%A0%91%E5%88%86%E6%9E%90%E4%B8%8E%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[为什么我们需要掌握这些“高端”的树型结构事实上，大型数据库的组织结构一般采用树型结构，我们必须要解决频繁更新数据的能力，要求支持高效的动态查找能力，包括记录的插入，删除，精确匹配查询，范围查询和最大值、最小值查询。但是由于数据库中包含了大量的记录，所以线性表的查询本身会因为记录太大而无法存储到主存之中，另外对于记录的插入和删除操作更需要移动大量的元素，这本身的效率是非常低下的。 二叉查找树（BST）的定义二叉查找树要么是一颗空树，要么满足以下的定义： 若它的左子树不为空，那么它左子树上所有节点的值均小于等于根节点。 若它的右子树不为空，那么它右子树上所有节点的值均大于根节点。 它的左右子树均是二叉查找树。 这明显是一个递归定义，因此我们对于BST的操作大多是建立在递归之上的。 二叉查找树有一个重要的特征：对一颗二叉查找树进行中序遍历，可以得到一个递增序列，那么我们只要将中序遍历的遍历顺序反过来，那么我们就会得到一个递减序列，这也正是二叉查找树得名的原因。 BST的数据结构123456typedef int keyType;typedef struct Node &#123; keyType data; struct Node *lchild, *rchild;&#125; BSTNode, BSTree; BST的建立首先我们讨论一下怎么建立一颗二叉查找树，根据二叉查找树的定义，如果现在有一串序列，那么它的建立过程如下： 首先建立一颗空树； 使节点插入正确的位置（BST的建立正是建立在BST的插入算法之上的）； 序列为空为止。 那么我们现在应该如何给一颗二叉查找树中插入新的节点呢？ 从树的根节点开始查找； 插入的关键字小于等于根节点则进入左子树，否则进入右子树； 由于新插入的节点必定是新的叶子节点，所以当此时的节点指针为空的时候，进行插入。 插入算法： 123456789101112131415void insert(BSTree **bstree, keyType data) &#123; BSTNode *bstnode; if(*bstree == NULL) &#123; bstnode = (BSTNode *)malloc(sizeof(BSTNode)); bstnode -&gt; data = data; bstnode -&gt; lchild = NULL; bstnode -&gt; rchild = NULL; *bstree = bstnode; &#125; else if(data &lt;= (*bstree) -&gt; data) &#123; insert(&amp;((*bstree)-&gt;lchild), data); &#125; else &#123; insert(&amp;((*bstree)-&gt;rchild), data); &#125;&#125; 在进行节点插入的时候，我们并不希望指向根节点的指针随着遍历的进行也跟着移动，并且因为是对于指针的参数传递，这里必然使用二级指针，这样头结点才会被真正的初始化。然后我们在进行递归遍历节点的时候，我们也不能影响二级指针的指向，我之所以在这里着重强调，说实话，我这个bug还是小伙伴帮我改出来的，对于二级指针，一直都是感觉自己懂了，一碰到繁杂的操作，就又糊涂了。。。 建立算法： 123456void create_BSTree(int array[], BSTree **bstree) &#123; //二叉查找树的创建是建立在插入上面的 for(int i = 0; i &lt; N; i++) &#123; insert(bstree, array[i]); &#125;&#125; 随后我们便可以对二叉查找树进行树状打印了： 123456789101112131415161718void print_tree(BSTree *bstree, int h) &#123; if(bstree == NULL) &#123; return ; &#125; if(bstree -&gt; rchild != NULL) &#123; print_tree(bstree -&gt; rchild, h+1); &#125; for(int i = 0; i &lt; h; i++) &#123; cout &lt;&lt; " "; &#125; cout &lt;&lt; bstree -&gt; data &lt;&lt; endl; if(bstree -&gt; lchild != NULL) &#123; print_tree(bstree -&gt; lchild, h+1); &#125;&#125; 仔细看上面的打印代码，没错，将中序遍历的顺序颠倒一下就行了。 BST的查找BST的查找我们可以用递归和循环分别实现。 非递归实现： 12345678910111213BSTNode *find(BSTree *bstree, keyType key) &#123; while(bstree != NULL) &#123; if(bstree -&gt; data == key) &#123; return bstree; &#125; else if(bstree -&gt; data &gt;= key)&#123; bstree = bstree -&gt; lchild; &#125; else &#123; bstree = bstree -&gt; rchild; &#125; &#125; return bstree;&#125; 非递归不再分析。 递归实现： 12345678910111213BSTNode *find_recursive(BSTree *bstree, keyType key) &#123; if(bstree == NULL) &#123; return NULL; &#125; if(bstree -&gt; data == key) &#123; return bstree; &#125; else if(key &lt;= bstree -&gt; data) &#123; return find(bstree -&gt; lchild, key); &#125; else &#123; return find(bstree -&gt; rchild, key); &#125;&#125; 由于我们对二叉查找树进行查找的时候不需要回溯，所以我们每次进入新子树的时候，直接这样： 1return find(bstree -&gt; lchild, key); return 语句阻止了回溯的发生。 BST的删除BST节点的删除，情况比较复杂，我在这里分三种情况说明： 在进行节点删除的时候，我们需要两个指针，它们指向待删节点的父节点与待删节点。 123// 定义父节点和孩子节点（待删节点）BSTree *root, *child;child = root = bstree; 首先我们考虑特殊情况，代码如下： 12345678910// 首先考虑特殊情况（根节点为空|只有根节点）if(root == NULL) &#123; cout &lt;&lt; "这是棵空树" &lt;&lt; endl;&#125; else if (root -&gt; lchild == NULL &amp;&amp; root -&gt; rchild == NULL) &#123; if(root -&gt; data == key) &#123; root = NULL; &#125; else &#123; cout &lt;&lt; "没要找到您要删除的节点" &lt;&lt; endl; &#125;&#125; 待删节点是叶子节点 找待删节点，并不断记录它的父节点。 将父节点的相应孩子域进行置空并free待删节点所占的空间。 先进行待删节点的查找： 12345678910111213while(child != NULL &amp;&amp; child -&gt; data != key) &#123; if(key &lt;= child -&gt; data) &#123; root = child; hild = root -&gt; lchild; &#125; else &#123; root = child; child = root -&gt; rchild; &#125;&#125;if(child == NULL) &#123; cout &lt;&lt; "没有找到您要删除的节点" &lt;&lt; endl;&#125; 进行叶子节点的删除： 12345678910// 待删节点是叶子节点if(child -&gt; lchild == NULL &amp;&amp; child -&gt; rchild == NULL) &#123; if(root -&gt; lchild == child) &#123; root -&gt; lchild = NULL; free(child); &#125; else &#123; root -&gt; rchild = NULL; free(child); &#125;&#125; 待删节点只有左子树或只有右子树 找到待删节点并判断此节点是否只有左子树或右子树。 将待删节点父节点的相应孩子域设置为该待删节点对应的左子树或右子树。 将待删节点置空。 12345678910111213141516// 待删节点只有左子树或者右子树else if(child -&gt; lchild == NULL || child -&gt; rchild == NULL) &#123; if(root -&gt; lchild == child &amp;&amp; child -&gt; lchild != NULL) &#123; root -&gt; lchild = child -&gt; lchild; free(child); &#125; else if(root -&gt; lchild == child &amp;&amp; child -&gt; rchild != NULL) &#123; root -&gt; lchild = child -&gt; rchild; free(child); &#125; else if(root -&gt; rchild == child &amp;&amp; child -&gt; lchild != NULL) &#123; root -&gt; rchild = child -&gt; lchild; free(child); &#125; else &#123; root -&gt; rchild = child -&gt; rchild; free(child); &#125; &#125; 待删节点既有左子树也有右子树当碰到这种情况的时候，我们需要额外引入两个指针，一个用来记录找到的替换节点，一个用来记录替换节点的父节点（替换节点就是用来替换被删除的节点）。 那么我们应该如何找到替换节点呢？来看一张图： 我们知道对二叉查找树进行中序遍历所得到的是递增序列。 假设我们现在要删除节点5，这棵BST的中序遍历结果为：1，2，3，4，5，6，7，根据中序遍历的性质可知，5之左是它的左子树，5之右是它的右子树，现在我们要删除5，那么我们就要找可以替换5的4或6，也就是和5相邻的两个数，我们再来看一下这两个节点在树中的位置，4的位置在左子树中最右下角，6的位置在右子树的最左下角，我们选择查找任一节点进行待删节点的替换就行。仔细观察会发现这一规律对任意节点适用。 理解了上面寻找替换节点的两种方式之后，来看一下实现代码，我寻找的是左子树中最右下角的替换节点并同时记录替换节点的父节点： 12345678BSTree *alter;BSTNode *alterParent = child;alter = alterParent;while(alter -&gt; rchild != NULL) &#123; alterParent = alter; alter = alterParent -&gt; rchild;&#125; 先重申一点，我们现在拥有四个指针，root-&gt;用来指向待删节点的父节点，child-&gt;用来指向待删节点，alter-&gt;用来指向我们找到的替换节点，alterParent-&gt;用来指向我们找到的替换节点的父节点。下面我们具体讨论四种情况： 对这四种情况，我希望大家可以认真考虑，画出一棵二叉查找树，然后去一个个进行实现。 1. 替换节点的父节点等于待删节点且删除节点是根节点的时候 123456789if(alterParent == child &amp;&amp; child == bstree) &#123;/**由于我查找替换节点的方式是左子树的最右边，所以当满足上面的if语句之后，我的替换节点必定在待删除节点的左边*/ alter -&gt; lchild = child -&gt; lchild -&gt; lchild; alter -&gt; rchild = child -&gt; rchild; //更新根节点 bstree = alter; free(child);&#125; 2. 替换节点的父节点等于待删节点但删除节点不是根节点的时候 123456789101112else if(alterParent == child &amp;&amp; child != bstree) &#123; alter -&gt; lchild = child -&gt; lchild -&gt; lchild; alter -&gt; rchild = child -&gt; rchild; //我们要判断待删节点是在待删节点父节点的哪颗子树上 if(child == root -&gt; lchild) &#123; root -&gt; lchild = alter; free(child); &#125; else &#123; root -&gt; rchild = alter; free(child); &#125;&#125; 3. 替换节点的父节点不等于待删节点但删除节点是根节点的时候 123456789else if(alterParent != child &amp;&amp; child == bstree) &#123; //如果是这种情况，那么替换节点就在替换父节点的右子树上 alter -&gt; lchild = child -&gt; lchild; alter -&gt; rchild = child -&gt; rchild; free(child); //更新根节点 bstree = alter; alterParent -&gt; rchild = NULL;&#125; 4. 替换节点的父节点不等于待删节点并且删除节点不是根节点的时候 123456789101112else &#123; alter -&gt; lchild = child -&gt; lchild; alter -&gt; rchild = child -&gt; rchild; if(child == root -&gt; lchild)&#123; root -&gt; lchild = alter; free(child); &#125; else &#123; root -&gt; rchild = alter; free(child); &#125; alterParent -&gt; rchild = NULL;&#125; 如上，可以看到每种情况的处理过程并不复杂，只要将每种情况都处理到，二叉查找树的删除问题也就迎刃而解了。 我把每种删除情况都测试了一下，都删除成功，具体的源码链接我放在下面，有兴趣的小伙伴可以研究研究。 BST性能分析二叉查找树查找的最差情况为ASL（平均查找长度）=((1+n)*n/2)/n = (n+1)/2，和顺序查找相同，如图： 最好的情况与折半查找相同，ASL为log2N（图片不再贴出）。 对于BST的插入和删除来说，只需修改某些节点的指针域，不需要大量移动其它记录，动态查找的效率很高。 源码链接戳我得到源码哦～～]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>BST</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序--堆排序分析与实现]]></title>
    <url>%2F2017%2F07%2F27%2F%E6%8E%92%E5%BA%8F-%E5%A0%86%E6%8E%92%E5%BA%8F%E5%88%86%E6%9E%90%E4%B8%8E%E5%AE%9E%E7%8E%B0%2F</url>
    <content type="text"><![CDATA[何为堆一个数组序列我们可以将其用完全二叉树或近似完全二叉树（不是满二叉树的完全二叉树）表示出来，当数组下标为i时，它的父节点为(i-1)/2，左孩子为(2i+1)，右孩子为(2i+2)，这种对应关系说明数组下标为0的地方也要存储数据。（关于完全二叉树和满二叉树我在这里不做介绍） 堆是在完全二叉树的基础上递归定义的，堆分为大顶堆和小顶堆。 大顶堆：根节点的数值大于孩子节点，完全二叉树的左右子树同时满足这个条件。小顶堆：根节点的数值小于孩子节点，完全二叉树的左右子树同时满足这个条件。 从这种数据结构中我们可以发现：大顶堆的根节点也就是数组的第一个元素必定是最大值，而小顶堆必定是最小值，看到这，我想大家已经大概能感觉的到堆这种数据结构为什么可以用来排序了。 在来看个大顶堆和小顶堆的图解吧： 堆排序的过程要想写出堆排序的代码，首先我们一定要清楚堆排序的过程，根据堆这种数据结构的特性，我总结了一下堆排序的过程： 首先我们需要将一个数组初始化为堆； 在初始化堆的过程中我们必定要移动数组中元素的位置； 初始化完成之后，如果我们建立的是大顶堆，那么数组中的第一个元素就是数组的最大值，小顶堆就是最小值； 然后我们将最大值（最小值）和堆中的最后一个叶子节点（数组中的最后一个元素）进行交换，是不是类似于选择排序。可以预测到，如果是大顶堆，那么我们将会进行升序排序，如果是小顶堆，我们将会进行降序排序； 在进行了上一个步骤之后我们需要重新初始化堆，然后重复以上步骤，直到循环结束。 在上面的排序过程中，有很多细节没有说，只是在脑中大致建立起一个堆排序的过程，下面我们仔细研究一下其中的细节。 堆的初始化堆的初始化实际上就是数组元素的移动与交换，只不过这种交换发生在孩子节点与父节点之间。假设我们要建立的是大顶堆，我们只要保证每棵左右子树都是堆并且都是大顶堆那么最后整棵完全二叉树必然是大顶堆。根据完全二叉树的结构我们可以得到，假设我们的数组有n个元素，那么对应的完全二叉树的叶子节点就有(n+1)/2个，每棵子树的根节点的下标（0单元进行存储）都是从(n/2)-1开始。叶子节点已经有序，可以单独看做已经初始化好的子堆，也就说我们只要从节点(n/2)-1处开始，分别计算出当前节点的左右孩子，先拿出值最大的孩子，然后将此孩子与父节点进行比较，如果孩子节点小于父节点，说明此子树已经是一子堆，直接考虑前一个非叶子节点，如果此孩子大于父节点，则需要将孩子节点与父节点互换后再考虑后面的结点，直至以这个节点为根的子树是一个堆！就相当于将这个比较小的节点不断下沉的画面。然后再考虑前一个非叶子节点。我再给大家一张图，很直观： 如图，我们不对叶子节点进行考虑，直接从36处进行调整，而我上面着重标注的那段话，就是图d和图e。 根节点的删除与其叫做根节点的删除，不如说是根节点与n-i (i=1,2,3… …)处节点的互换，这样我们就相当于每次将当前数组的最大值放到数组的最后面，也就是实现了升序。可以看到，每建立一次堆，下次重新初始化堆的时候节点数量都会少一，那么当整个数组有序的时候也就是当只有一个节点进行堆的初始化的时候。 代码实现好了，堆排序的思想至此已经完全清楚了，按照这个思路我实现了大顶堆的排序： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374757677787980818283#include&lt;iostream&gt;using namespace std;#define N 10class Heap &#123;public: void sort(int array[], int size); void createHeap(int array[], int i, int size); void swap(int array[], int local);&#125;;void Heap :: swap(int array[], int local) &#123; int temp = array[local]; array[local] = array[0]; array[0] = temp;&#125;void Heap :: createHeap(int array[], int i, int size) &#123; //先找到当前节点的左右孩子节点 int l = 2*i+1; int r = l+1; int k; //保存当前节点的值 int temp = array[i]; cout &lt;&lt; "l: " &lt;&lt; l &lt;&lt; " r: " &lt;&lt; r &lt;&lt; endl; while(l &lt; size) &#123; //先找到数值较大的孩子 if(l == size-1) &#123; k = l; &#125; else &#123; k = (array[l] &gt;= array[r] ? l : r); &#125; //将孩子和父节点进行比较 if(array[k] &lt;= temp) &#123; break ; &#125; else &#123; array[i] = array[k]; i = k; l = 2*i+1; r = l+1; &#125; array[k] = temp; &#125;&#125;void Heap :: sort(int array[], int size) &#123; //先找到第一个非叶子节点 int not_leafP = size/2-1; int local = size; //初始化堆 for(int i = not_leafP; i &gt;= 0; i--) &#123; //建立子堆 createHeap(array, i, size); &#125; //将堆顶元素插入到数组尾的有序区间中 swap(array, --local);&#125;int main()&#123; int array[N]; Heap heap; for(int i = 0; i &lt; N; i++) &#123; cin &gt;&gt; array[i]; &#125; //当只有一个节点进行初始化堆的时候，数组有序 for(int size = N; size &gt; 1; size--) &#123; heap.sort(array, size); &#125; for(int i = 0; i &lt; N; i++) &#123; cout &lt;&lt; array[i] &lt;&lt; ' '; &#125; cout &lt;&lt; endl; return 0;&#125; 小顶堆的实现代码和大顶堆没有区别，故不在列出。 效率分析时间复杂度：堆排序的时间代价主要花费在建立初始堆和调整为新堆时所反复进行的“筛选上”，由代码可知，我们总共建立了n-1次堆，建立新堆时总共进行的比较次数最多为{2[log2(n-1)+log2(n-2)+log2(n-3)… … +log2]} &lt; 2n[(log2(n)]，所以堆排序的时间复杂度为O(nlog2(n))。 空间复杂度：只需要一个辅助空间，为O(1)。 最后，堆排是一种不稳定的排序。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>堆排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序--快速排序分析]]></title>
    <url>%2F2017%2F07%2F22%2F%E6%8E%92%E5%BA%8F-%E5%BF%AB%E9%80%9F%E6%8E%92%E5%BA%8F%E5%88%86%E6%9E%90%2F</url>
    <content type="text"><![CDATA[快速排序实现代码：快速排序 可以看到我的代码有一个错误版，我在这里给大家分析一下为什么会出现错误，并且将之记录以便今后进行查阅。 快速排序（错误版分析）12345678910111213141516int Quick :: process(int array[], int l, int r) &#123; int temp = array[l]; while(l != r) &#123; while(array[r] &gt;= temp) r--; array[l] = array[r]; l++; while(array[l] &lt;= temp) l++; array[r] = array[l]; r--; &#125; array[l] = temp; return l;&#125; 如上是我自己实现的单趟快速排序。 快排的算法思想从待排序列中任意选择一个记录，以该记录为关键字，凡数组中元素小于该关键字的都移动至该关键字前面，反之移动到后面。致使一趟快速排序之后，以关键字为中点将数组分割成左右两个序列。然后分别对两个子序列递归进行快速排序，直至每个子序列中只含有一个元素为止。 为什么上面这个代码会出现段错误，首先我们来看： 1while(array[r] &gt;= temp) r--; 我在上面的while循环里面设定了l != r，也就是说一趟快速排序结束的标志就是当l == r的时候，然后在进行元素和关键字记录比较的时候，并没有重新写上l &lt; r,考虑这种情况： 0 9 8 7 6 5 4 3 2 1 当我以0为关键字的时候，r再一直减小，然后r会越界，直到拿到一个垃圾数据比关键字0还要小的时候第一个循环才会停下来，这是一个错误。 第二个错误就是我多余的进行了l++,r–操作： 1234567while(array[r] &gt;= temp) r--;array[l] = array[r];l++;while(array[l] &lt;= temp) l++;array[r] = array[l];r--; 依旧是上面的序列，当前三句运行完之后，l的值会变为1，此时l与r已经相等，然而我又在后三句的最后进行了r–操作，此时会让l &gt; r,最外层的while循环： 1while(l != r) 就会失效，然后也就发生了段错误，改正之后，运行正确。 效率分析快速排序的时间代价取决与关键字的选择，最简单的方法就是选择第一个记录或最后一个记录，但这样的弊端已然很明显，在每次进行递归分割的时候，如果是正序，那么就会将剩余记录全部分到一个序列当中而另一个序列为空。假如有10个数，第一趟经过9次比较，左半序列为空，第二趟经过8次比较，左半序列为空… …以次类推共需要n-1趟排序，比较次数为O(n2)，等同于冒泡排序。 为避免这种情况，我们可以选取中间位置对应记录的关键字。 最好的情况下，每次分割都将序列分为两个长度相等的子序列，则总共就需要分割log2N次，因此算法的时间代价在最好情况下就是O(N*log2N)，比较幸运的是，快速排序的平均时间复杂度就是这个值，所以快速排序仍然是一种非常高效的排序方法。 空间复杂度：快排需要分割log2N次，每次都需要一个辅助空间，因此快速排序的空间复杂度为O(log2N)。 最后，快速排序并不稳定。]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>快速排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[排序--归并排序]]></title>
    <url>%2F2017%2F07%2F21%2F%E6%8E%92%E5%BA%8F-%E5%BD%92%E5%B9%B6%E6%8E%92%E5%BA%8F%2F</url>
    <content type="text"><![CDATA[了解归并归并排序算法和快速排序算法是java.util.Arrays中使用的排序算法。对于一般的基本数据类型，Arrays.sort函数使用双轴快速排序算法，而对于对象类型使用归并排序（准确的说使用的是TimSort排序算法，它是归并排序的优化版本）。这样做的原因有两点，第一个原因，归并排序是稳定的，而快速排序不是稳定的。第二个原因，对于基本数据类型，排序的稳定性意义不大，但对于复合数据类型（比如对象）排序的稳定性就能帮助我们保持排序结果的某些性质。 归并排序也是一种基于分治法的排序。它是将原始无序序列划分成两个子序列，然后分别对每个子序列递归的进行排序，最后再将有序子序列进行合并。 自顶向下归并排序自顶向下主要使用递归的思想，要保证整体序列有序，我们就应该同时保证原数组的左子序与右子序同时有序，然后将两个子序进行合并，然而要保证左子序和右子序有序，我们就要保证左子序的左右子序有序，然后进行合并；右子序的左右子序有序，然后进行合并… ….处理这种重复的工作，我们需要的就是递归。 图解自顶向下归并排序： 算法思想 既然我们要拆分数组，我们需要一个中枢值，它就是原数组的中点或近似中点的地方； 我们需要一个辅助空间，它用来保存当前需要合并的左右子序； 从图解中我们可以很清楚的看到，我们刚开始不断地将原数组拆分，直到保证每个子数组都有序，很显然，这个子数组就是单个元素； 合并子数组，将辅助数组中合并后产生的新数组覆盖至原数组，我们需要对原数组进行刷新。 下面是自顶向下的实现代码： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576777879808182#include&lt;iostream&gt;using namespace std;#define SIZE 11class MergeTD &#123;private: int *array_temp;public: MergeTD(int array_temp[]) &#123; this -&gt; array_temp = array_temp; &#125; //我对sort函数进行了重载 void sort(int array[]); void merge(int array[], int mid, int l, int r); void sort(int array[], int l, int r);&#125;;void MergeTD :: sort(int array[]) &#123; //进行归并排序，需要排序的数组和左右两个指标 sort(array, 0, SIZE-1);&#125;void MergeTD :: sort(int array[], int l, int r) &#123; //当已经拆分为单个元素或l已经大于r的时候退出当前函数 if(l &gt;= r) return ; //计算中点值 int mid = l+(r-l)/2; //对左边进行拆分 sort(array, l, mid); //对右边进行拆分 sort(array, mid+1, r); //进行归并操作 merge(array, mid, l, r);&#125;void MergeTD :: merge(int array[], int mid, int l, int r) &#123; int m = l, n = mid+1; //将归并的数组存放到临时数组中 for(int k = l; k &lt;= r; k++) &#123; array_temp[k] = array[k]; &#125; //归并操作在原数组上只对拆分的这一部分有效 for(int i = l; i &lt;= r; i++) &#123; if(m &gt; mid) array[i] = array_temp[n++]; else if(n &gt; r) array[i] = array_temp[m++]; else if(array_temp[n] &lt; array_temp[m]) array[i] = array_temp[n++]; else array[i] = array_temp[m++]; &#125;&#125;int main()&#123; int array[SIZE]; //辅助数组，用来存储每个有序的小数组 int array_temp[SIZE]; MergeTD TDMerge(array_temp); for(int i = 0; i &lt; SIZE; i++) &#123; cin &gt;&gt; array[i]; &#125; //归并排序 TDMerge.sort(array); for(int i = 0; i &lt; SIZE; i++) &#123; cout &lt;&lt; array[i] &lt;&lt; ' '; &#125; return 0;&#125; 效率分析归并排序的时间主要花在了划分序列，子序列的排序过程以及合并过程，由于划分序列的时间为常数，对一个长度为n的记录序列进行归并排序，调用一趟归并排序的操作是调用n/2h次合并算法，时间复杂度为O(n)。整个归并排序需要进行logN趟归并操作，时间复杂度O(n) = NlogN。 空间复杂度：需要一个空间大小为N的辅助数组来存放待排序记录，空间杂度为O(n)。 最后，归并排序是稳定的。 自底向上归并排序直接看图解吧： 可以看到在自顶向下归并排序中，我们先是对数组进行了不断的拆分，然后又合并，而在自底向上的归并中，我们不再对数组进行拆分，因为单个元素始终有序，所以我们只要刚开始控制子数组的长度为1，然后进行一一合并，合并后的子数组长度为2，那么我们就进行两两合并… …可以看到，自底向上的代码是比自顶向下的代码要好写的。 在这里我们需要注意的一点是数组按照归并长度划分，最后一个子数组可能不满足长度要求，这个情况需要特殊处理。自底向上一般使用循环： 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960#include&lt;iostream&gt;#include&lt;cstdlib&gt;#include&lt;cstring&gt;using namespace std;void sort(int array[], int size) &#123; int aux[size]; //刚开始子数组长度为1，然后进行一一归并，两两归并，四四归并... ...当子数组长度大于等于原数组长度时，整个数组有序 for(int i = 1; i &lt; size; i *= 2) &#123; //将原数组复制到一个临时数组之中 memcpy(aux, array, 4*size); //进行归并操作，首先要判断好每次归并的start，mid，end for(int start = 0; start &lt; size; start += i*2) &#123; int end = start + i*2-1; int mid = (start+end)/2; int m = start, n = mid+1; //当我们对数组进行划分的时候，最后一个子数组的end有时候会大于数组长度，此时我们应该选取整体数组的长度 if(end &gt;= size) &#123; end = size-1; &#125; //开始进行单趟归并 for(int k = start; k &lt;= end; k++) &#123; if(m &gt; mid) &#123; array[k] = aux[n++]; &#125; else if(n &gt; end) &#123; array[k] = aux[m++]; &#125; else if(aux[m] &gt; aux[n]) &#123; array[k] = aux[n++]; &#125; else &#123; array[k] = aux[m++]; &#125; &#125; &#125; &#125;&#125;int main()&#123; int *array; int size; cin &gt;&gt; size; array = (int *)malloc(sizeof(int)*size); for(int i = 0; i &lt; size; i++) &#123; cin &gt;&gt; array[i]; &#125; sort(array, size); for(int i = 0; i &lt; size; i++) &#123; cout &lt;&lt; array[i] &lt;&lt; " "; &#125; return 0;&#125;]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>归并排序</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java网络爬虫--海量URL去重之布隆过滤器]]></title>
    <url>%2F2017%2F06%2F06%2FJava%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB-%E6%B5%B7%E9%87%8FURL%E5%8E%BB%E9%87%8D%E4%B9%8B%E5%B8%83%E9%9A%86%E8%BF%87%E6%BB%A4%E5%99%A8%2F</url>
    <content type="text"><![CDATA[简介布隆过滤器当我们要对海量URL进行抓取的时候，我们常常关心一件事，就是URL的去重问题，对已经抓取过的URL我们不需要在进行重新抓取。在进行URL去重的时候，我们的基本思路是将拿到的URL与已经抓取过的URL队列进行比对，看当前URL是否在此队列中，如果在已抓取过的队列中，则将此URL进行舍弃，如果没有在，则对此URL进行抓取。看到这，如果有哈希表基础的同学，很自然的就会想到那么如果用哈希表对URL进行存储管理的话，那么我们对于URL去重直接使用HashSet进行URL存储不就行了。事实上，在URL非海量的情况下，这的确是一种很不错的方法，但哈希表的缺点很明显：费存储空间。 对于像Gmail那样公众电子邮件提供商来说，总是需要过滤掉来自发送垃圾邮件的人和来及邮件的E-mail地址。然而全世界少说也有几十亿个发垃圾邮件的地址，将他们都存储起来需要大量的网络服务器。如果用哈希表，每存储一亿个E-mail地址，就需要1.6GB的内存（用哈希表实现的具体实现方式是将每一个E-mail地址对应成一个八字节的信息指纹，然后将这个信息存储在哈希表中，但是由于哈希表的存储效率一般只有50%，一旦存储空间大于表长的50%，查找速度就会明显的下降（容易发生冲突），即存储一个E-mail我们需要给它分配十六字节的大小，一亿个地址的大小大约就要1.6GB内存）。因此存储几十亿的地址就要需要大约上百GB的内存，除非是超级计算机，一般服务器是无法存储的。 关于哈希表的相关知识，请戳这篇博客—查找–理解哈希算法并实现哈希表 具体实现思想在这种情况下，巴顿·布隆在1970年提出了布隆过滤器，它只需要哈希表的1/8到1/4的大小就可以解决同样的问题。我们来看一下其工作原理： 首先我们需要一串很长的二进制向量，与其说是二进制向量，我觉得不如说是一串很长的“位空间”，其具体原理大家可以了解一下Java中BitSet类的算法思想。它用位空间来存储我们平常的整数，可以将数据的存储空间急剧压缩。然后需要一系列随机映射函数（哈希函数）来将我们的URL映射成一系列的数，我们将其称为一系列的“信息指纹”。 然后我们需要将刚才产生的一系列信息指纹对应至布隆过滤器中，也就是我们刚才设置的那一串很长的位空间（二进制向量）中。位空间中各个位的初始值为0。我们需要将每个信息指纹都与其布隆过滤器中的对应位进行比较，看看其标志位是否已经被设置过，如果判断之后发现一系列的信息指纹都已被设置，那么就将此URL进行过滤（说明此URL可能存在于布隆过滤器中）。事实上，我们将每个URL用随机映射函数来产生一系列的数之所以能被称之为信息之纹，就是因为这一系列的数基本上是独一无二的，每个URL都有其独特的指纹。虽然布隆过滤器还有极小的可能将一个没有抓取过的URL误判为已经抓取过，但它绝对不会对已经抓取过的URL进行重新抓取。然后刚才的误判率一般来说我们基本上可以忽略不计，等下我给大家列出一张表格大家直观感受一下。 对于为什么会出现误判的情况，请参考此篇博客—布隆过滤器(Bloom Filter)的原理和实现 算法总结现在我们来总结一下该怎么设计一个布隆过滤器： 创建一个布隆过滤器，开辟一个足够的位空间（二进制向量）； 设计一些种子数，用来产生一系列不同的映射函数（哈希函数）； 使用一系列的哈希函数对此URL中的每一个元素（字符）进行计算，产生一系列的随机数，也就是一系列的信息指纹； 将一系列的信息指纹在布隆过滤器中的相应位，置为1。 代码实现（Java）123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566import static java.lang.System.out;public class SimpleBloomFilter &#123; // 设置布隆过滤器的大小 private static final int DEFAULT_SIZE = 2 &lt;&lt; 24; // 产生随机数的种子，可产生6个不同的随机数产生器 private static final int[] seeds = new int[] &#123;7, 11, 13, 31, 37, 61&#125;; // Java中的按位存储的思想，其算法的具体实现（布隆过滤器） private BitSet bits = new BitSet(DEFAULT_SIZE); // 根据随机数的种子，创建6个哈希函数 private SimpleHash[] func = new SimpleHash[seeds.length]; // 设置布隆过滤器所对应k（6）个哈希函数 public SimpleBloomFilter() &#123; for (int i = 0; i &lt; seeds.length; i++) &#123; func[i] = new SimpleHash(DEFAULT_SIZE, seeds[i]); &#125; &#125; public static void main(String[] args) &#123; String value = "stone2083@yahoo.cn"; SimpleBloomFilter filter = new SimpleBloomFilter(); out.println(filter.contains(value)); &#125; public static class SimpleHash &#123; private int cap; private int seed; // 默认构造器，哈希表长默认为DEFAULT_SIZE大小，此哈希函数的种子为seed public SimpleHash(int cap, int seed) &#123; this.cap = cap; this.seed = seed; &#125; public int hash(String value) &#123; int result = 0; int len = value.length(); for (int i = 0; i &lt; len; i++) &#123; // 将此URL用哈希函数产生一个值（使用到了集合中的每一个元素） result = seed * result + value.charAt(i); &#125; // 产生单个信息指纹 return (cap - 1) &amp; result; &#125; &#125; // 是否已经包含该URL public boolean contains(String value) &#123; if (value == null) &#123; return false; &#125; boolean ret = true; // 根据此URL得到在布隆过滤器中的对应位，并判断其标志位（6个不同的哈希函数产生6种不同的映射） for (SimpleHash f : func) &#123; ret = ret &amp;&amp; bits.get(f.hash(value)); &#125; return ret; &#125;&#125; 代码的注解已经足够详细，如果大家还有什么疑惑，可以在评论区进行讨论交流～～ 布隆过滤器误判率表]]></content>
      <categories>
        <category>Java网络爬虫</category>
      </categories>
      <tags>
        <tag>网络爬虫</tag>
        <tag>布隆过滤器</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[查找--理解哈希算法并实现哈希表]]></title>
    <url>%2F2017%2F06%2F01%2F%E6%9F%A5%E6%89%BE-%E7%90%86%E8%A7%A3%E5%93%88%E5%B8%8C%E7%AE%97%E6%B3%95%E5%B9%B6%E5%AE%9E%E7%8E%B0%E5%93%88%E5%B8%8C%E8%A1%A8%2F</url>
    <content type="text"><![CDATA[我们喜欢使用数组进行数据的查找，就是因为数组是一种“随机存取”的数据结构，我们根据数组的起始地址和数组元素的下标值就可以直接计算出每一个数组元素的存储位置，所以它的查找时间是O(1)，而与数组的个数无关。 我们在这个思想的基础上，可以联想到，如果有一种数据结构，让我们在进行关键字查找的时候，也可以像数组一样，进行随机存储，使其时间复杂度从O(n)降到O(1)，那就可以大大提高查找的效率。我们的前辈们基于这种想法发明了散列方法，也就是哈希或关键字地址计算方法。 基本思想我们试图寻找一种关系，可以根据我们要存储的关键字（key）然后使用这种关系直接计算出它应该存储的位置（p），一旦建立起这种关系，那么我们在之后一旦需要查找此关键字的话，只需计算此对应关系所产生的值就可以直接得到关键字所在的地址，那么查找的时间复杂度也就降到了O(1)，我们将刚才所说的转换为一种数学关系：p（位置）= H（key）。 其中H就是对应关系，我们称之为哈希函数，p被成为散列地址。因此，哈希算法的核心就是找到哈希函数（H），通过这个函数来组织存储并进行查找。 Hash函数的构造方法我们先来看一个问题： 假如我们现在有一些单词（关键字）：and，cell，do，flag … … 等等，如果我们的哈希函数取值为关键字的第一个字母在字母表中的字典顺序，那么关键字会依次分布在散列地址中。然而当我们还使用这套规则进行and，ant，apple，cell，do … …等等字母的存储，那么就会产生“地址冲突”的问题，因为前三个单词在字母表中的顺序是一样的。 由于Hash函数是一个压缩映像，因此在实际应用中，很少存在不产生冲突的hash函数，因此，如何构造恰当的Hash函数，使得节点“均匀分布”，尽量少的产生冲突就是我们必须要解决的问题之一了。 hash函数的构造原则为简单和均匀： hash函数本身运算简单，便于计算； hash函数值必须在散列地址范围内，且分布均匀，地址冲突尽可能少。 有几种常用的hash函数的构造方法： 1. 除留余数法 该方法是最为简单常用的一种方法，假设表长为m（散列地址的长度），p为小于等于表长m的最大素数，则hash函数为H（key）= key % p。H就是在散列地址中位置。 p的取值应该是一个质因子，这样才能减少“地址冲突”的可能性。 2. 数字分析法 假设关键字集合中的每个关键字都是由s位数字组成（k1, k2, … , kn），如果可以预先估计出全体关键字的每一位上各种数字出现的频度时，分析关键字集中的全体，并从中提取出分布均匀的若干位或它们的组合作为hash地址。 例如：H（49646542）= 465， H（49673242）= 732 … … 经过分析，各个关键字中第4~6位中的取值比较均匀，则hash函数为H（key）= d4d5d6。 3. 平方取中法 由于整数相除的运行速度通常比相乘要慢，所以我们需要有意识的避免使用除余法可以提高散列算法的运行时间。平方取中法：首先通过求关键字的平方值扩大相近数的差别，然后根据表长度取中间的几位数作为hash函数值。又因为一个乘积的中间几位数和乘数的每一位都相关，因此由此产生的散列地址较为均匀。 4. 分段叠加法 有时关键字所含的位数很多，采用平方取中法计算太复杂，则可将关键字分割成位数相同的几部分（最后一部分的位数可以不同），然后取这几部分进行叠加，叠加和（舍去进位）作为散列地址。 具体的叠加方式有移位叠加和折叠叠加。 5. 基数转换法 首先将关键字看做是另一种进制的数，然后在转换成原来进制的数，再选择其中几位作为散列地址。 例如：把十进制（362081）看做13进制的数，最后结果再转换为十进制（1289744），假设散列长度是10000，则可取低四位9744作为散列地址。 一般来说我们还是应该根据实际情况采用恰当的哈希算法，并测试它的性能，一般考虑下列因素： 计算hash函数所需的时间； 关键字的长度； 散列表的大小； 关键字分布的情况； 记录查找的频率。 处理冲突的方法在上面我们说过，实际情况中我们是不可能不产生地址冲突的，所以，一旦我们有地址冲突，我们应该怎么办？我们自然而然的想到，那就为产生冲突的地址寻找下一个散列地址。 开放定址（再散列）法基本思想： 当关键字key的初始散列地址h0=H（key）出现冲突时，以h0为基础查找下一个地址h1，如果h1仍然冲突，再以h0为基础，产生另一个散列地址h2… … 直到找出一个不冲突的地址hi，将相应元素存入其中，这种方法有一个通用的再散列函数形式：hi=（（H（key）+ di）% m。 其中h0=H（key），m为表长，di为增量序列。增量序列的取值方式不同，对应不同的再散列方式： 1. 线性探测再散列 di = c × i 最简单的情况：c = 1。 特点：冲突放生时，顺序查看表中下一个单元，直到找到一个空单元或查遍全表。值得注意的是：由于使用的是%（取余）运算符，所以它和循环队列有点相似，表尾的后边是表头，表头的前边是表尾。 2. 二次探测再散列 di = 1^2，-1^2，2^2，-2^2, … … ，k^2，-k^2 （k&lt;=(m/2)） 特点：冲突发生时分别在表的右，左进行跳跃式探测，较为灵活，不易产生聚集，缺点是不能探查到整个散列地址空间。 3.随机探测再散列 di = 伪随机数 特点：建立一个随机数发生器，并给定一个随机数作为起始点。 链地址法基本思想：把所有具有地址冲突的关键字链在同一个单链表中。若哈希表的长度是m，则可以将哈希表定义为一个有m个头指针组成的指针数组。散列地址为i的记录，均插到以指针数组第i个单元为头指针的单链表中。 性能指标衡量查找效率的主要性能指标就是平均查找长度（ASL）。ASL（succ）=（比较次数之和）/（关键字个数）。 比较次数代表的是关键字放入散列地址中为避免地址冲突需要跟当前散列地址中是否已经有值而进行判断的次数。 ASL越小，性能越好。 哈希表有了前面的基础，我们来试着自己构建一个哈希表，并实现哈希表的插入，查找和删除。 哈希表的创建 首先将表中各节点的关键字置空； 使用插入算法将给定的关键字序列一次插入哈希表中。 哈希表的插入 通过查找算法找到待插记录在表中的位置； 若在表中找到待插记录，则不必插入；若没有找到，查找算法给出一个单元空闲的散列地址，并插入到该地址单元中。 哈希表的查找 根据待查找记录的关键字和建表时的哈希函数计算散列地址； 若该地址单元为空，则查找失败；若不为空，则将该单元中的关键字与待查记录的关键字进行比较： 如果相等，则查找成功； 如果不等，则按建表时设定的处理冲突的方法找下一个地址。 重复上述步骤2，直至某个单元为空，则查找失败或者与待查记录的关键字进行比较，相等则查找成功。 哈希表的删除基于开放地址法的哈希表不能实现真正的删除，只能给被删除节点设置删除标志，以免在删除后找不到比它晚插入的节点且发生过冲突的节点，也就是说，如果执行真正的删除操作，会中断查找路径，如果必须对哈希表做真正的删除操作，最好采用链地址法处理冲突的哈希表。 哈希表的装填因子 α = 哈希表中已存入的元素个数 / 哈希表的长度 α越小，冲突的可能性就越小，但空间利用率就越低；α越大，冲突的可能性就越大，但空间利用率就越高。 哈希表的存储效率为何一般只有50%根据上面的装填因子我们可以得知，α越大，产生冲突的的几率也就越大，查找的次数就会变多，然后我们可以看一下查找的时间复杂度计算公式：1/( 1 - n/m )。 n/m就是上面所说的装填因子，我们可以发现，当装填因子大于1/2的时候，查找的时间复杂度就会大于二，所以我们一般会说哈希表的存储效率只有50%。 哈希表的实现代码（C语言）github链接：哈希表]]></content>
      <categories>
        <category>数据结构与算法</category>
      </categories>
      <tags>
        <tag>哈希</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java网络爬虫--正则表达式之详解贪婪、逐步、独吐量词]]></title>
    <url>%2F2017%2F05%2F28%2FJava%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB-%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F%E4%B9%8B%E8%AF%A6%E8%A7%A3%E8%B4%AA%E5%A9%AA%E3%80%81%E9%80%90%E6%AD%A5%E3%80%81%E7%8B%AC%E5%90%90%E9%87%8F%E8%AF%8D%2F</url>
    <content type="text"><![CDATA[除过正则表达式的基本概念与特性还有使用方法之外，我们在解析html的时候，如果要进行字符串的匹配，必须还要熟悉正则表达式之中量词的使用法则，今天我们就来谈谈贪婪、逐步、独吐这三种量词的使用。 贪婪量词我们先来看一下经常使用的贪婪量词都有哪些： X?: X项目（项目也可以理解为X代表的变量，项目比较准确）出现一次或没有。 X*: X项目出现0次或多次。 X+: X项目至少出现1次。 X{n}: X项目出现n次。 X{n, }: X项目至少出现n次。 X{n, m}: X项目出现n次但不超过m次。 下来我们解释贪婪量词为何贪婪。当我们使用贪婪量词进行字符串匹配的时候，如果比较器（Matcher）看到贪婪量词，会将剩下的字符串全部吃掉，然后从字符串的末尾一个个再吐出来，在这个过程中，它还会将吐出来的字符与规则表达式进行比较，如果吐出来的字符串符合规则表达式，而吃下的字符串也符合贪婪量词那么就比较成功，我们可以预想到，贪婪量词之所以贪婪，就是因为它会尽可能的找出长度最长的符合文字。 举个例子： 文字：xfooxxxxxxfoo，使用规则表达式 “.*foo“比较，当碰见贪婪量词 “ .×“ (*da不出来～～)，比较器会吃掉剩下的字符串，在这个例子中也就是整个字符串，然后比较器再从后向前一个个将字符吐出来，当吐出foo的时候，foo符合贪婪量词之后的规则表达式，而剩下没吐的字符串也符合贪婪量词的部分，所以就匹配到了整个字符串，所以它尽可能的找出长度最长的符合文字。（“.”代表任一字符） 逐步量词逐步量词也称非贪婪量词或懒惰量词，顾名思义，它和贪婪量词是相对的，它的形式是在贪婪量词之后加上“?”，形如：“.*?”，在使用逐步量词时，比较器会一边吃掉剩余文字，一边看吃下的文字是否符规则表达式，结果就是逐步量词会找出长度最短的符合文字。 还是上面的例子： 文字：xfooxxxxxxfoo，使用规则表达式 “.×?foo”比较，当碰见逐步量词 “ .×?” (*da不出来～～)，比较器会边吃文字边比较而不会一下将剩余的文字全部吃掉，所以第一个匹配到的字符串就是“xfoo”，第二个匹配到的字符串就是“xxxxxxfoo”，所以它尽可能的找出长度最短的符合文字。 独吐量词在贪婪量词之后加上“+”，形如：“.*+”就是一种独吐量词，在使用独吐量词的时候，如果比较器看到独吐量词，它会将剩下的字符串全部吃掉，然后和贪婪量词相反，它不会吐出文字和剩下的规则表达式比较，而是会将吃下的文字和独吐量词进行比较，如果吃下的字符串符合独吐量词，那么它将不会将吃下的字符串吐出。 依然是上面的例子： 文字：xfooxxxxxxfoo，使用规则表达式 “.*+foo”比较，当碰见独吐量词 “ .×+” (*da不出来～～)，比较器会吃掉剩下的字符串，在这个例子中也是整个字符串，然后它发现吃下的整个字符串已经满足独吐量词“.×+”，所以它就不会将字符串再吐出，然后会因为没有剩余的字符串符合剩下的规则表达式“foo”而产生没有找到相匹配的字符串的情况，最后的结果就是没有任何文字符合。]]></content>
      <categories>
        <category>Java网络爬虫</category>
      </categories>
      <tags>
        <tag>正则表达式</tag>
        <tag>字符串匹配</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MySQL数据库--MySQL数据库语句、坑点总结]]></title>
    <url>%2F2017%2F04%2F14%2FMySQL%E6%95%B0%E6%8D%AE%E5%BA%93-MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E8%AF%AD%E5%8F%A5%E3%80%81%E5%9D%91%E7%82%B9%E6%80%BB%E7%BB%93%2F</url>
    <content type="text"><![CDATA[MySQL实用语句操作1.清空数据库表的同时将id的增长顺序重新设为从0开始： 1TRUNCATE TABLE 数据库表名 2.在知道数据库原密码的时候进行修改密码： 1mysqladmin -u root -p password "new password" 3.将选定数据库导出至sql脚本：（只导出表结构）： 1mysqldump -u root -p -d "数据库名" &gt; "sql脚本" 4.将选定数据库导出至sql脚本（表结构和数据全部导出）： 1mysqldump -u root -p "数据库名" &gt; "sql脚本" 5.将mysql脚本导入至数据库： 1mysql -u root -p "数据库名" &lt; "sql脚本" 6.查看字段编码： 1SHOW FULL COLUMNS FROM &lt;表名&gt; 7.查看表编码： 1SHOW CREATE TABLE &lt;表名&gt; 解决中文乱码原文链接：整理 ： 查看和修改 mysql库、表、字段编码 文中好像有些许错误… … 忘记数据库密码，我们应该怎么办？此方法在Centos7下进行了有效测试： 1.编辑/etc/my.cnf实现免密登录，确保你的数据库不会被其他人进行恶意修改。找到 [mysqld] 段，并给段中任意地方加上一句：skip-grant-tables： 1skip-grant-tables 2.重启mysql服务： 1systemctl restart mysql.service 3.在数据库中修改密码： 123use mysql;UPDATE user SET Password = password('new password') WHERE User = 'root';flush privileges; 4.最后将免密功能再去掉，就可以了。 Unknown column ‘xxx’ in ‘where clause’列名不存在的结论，但是，很多时候起始并不是由于列名出错造成的。而是由于拼凑sql语句时对字符类型数据没有用引号引起来造成的。 这个错误我改了三个小时，做了无数测试，一度以为是玄学，来一看一下正确的写法吧，还有这个错误在要拼接的SQL语句是int型的时候，一般是不会出现的… …. 12statement = connection.prepareStatement( "SELECT * FROM book_class WHERE name = '" + labelName + "'"); 避免使用IN或者OR(OR会导致扫表)，使用UNION ALL： 1(select * from article where article_category=2) UNION ALL (select * from article where article_category=3) ORDER BY article_id desc limit 5 UNION 和UNION ALL 的区别：在数据库中，UNION和UNION ALL关键字都是将两个结果集合并为一个，但这两者从使用和效率上来说都有所不同。UNION在进行表链接后会筛选掉重复的记录，所以在表链接后会对所产生的结果集进行排序运算，删除重复的记录再返回结果。 从效率上说，UNION ALL 要比UNION快很多，所以，如果可以确认合并的两个结果集中不包含重复的数据的话，那么就使用UNION ALL。 创建索引执行一个很普通的查询： 1SELECT * FROM `article` WHERE article_category=11 ORDER BY article_id DESC LIMIT 5 执行时间大约要5秒左右。 建一个索引（组合索引）： 1CREATE INDEX idx_u ON article(article_category, article_id); 1SELECT * FROM `article` WHERE article_category=11 ORDER BY article_id DESC LIMIT 5 减少到0.0027秒。 排序数据排序： 12345678// 升序（默认）SELECT * FROM 表名 ORDER BY 字段 ASC;// 降序SELECT * FROM 表名 ORDER BY 字段 DESC;// 多个列排序SELECT 字段1，字段2，字段3 FROM 表名 ORDER BY 字段1 DESC，字段2; 注意：用非检索的列排数据是完全合法的。首先按字段1进行排序。 过滤数据不匹配检查： 12// 不返回字段1的值为1003的行（一般使用!=）SELECT 字段1，字段2 FROM 表名 WHERE 字段1 &lt;&gt; 1003; 范围值检查： 1SELECT * FROM 表名 WHERE 字段 BETWEEN 5 AND 10; 注意：SQL在处理OR操作符前，优先处理AND操作符。解决方式像其他语言一样使用括号。 12// IN的功能和OR的完全一样，推荐使用IN，执行更快，更直观，可包含其他SELECT语句SELECT 字段 FROM 表名 WHERE 字段 IN (1002，1003) ORDER BY 字段; 通配符： 12// %表示任何字符出现任意次数（包括0）（所有以jet开头的单词）SELECT 字段1，字段2 FROM 表名 WHERE 字段 LIKE 'jet%'; 注意：使用通配符需要在意尾空格。还有一个通配符_只匹配单个字符而不是多个字符。（不要过度使用通配符，时间代价比其它操作符更高，但它的确很有用） 注意： 其中like要求整个数据都要匹配，而REGEXP只需要部分匹配即可。 匹配特殊字符使用\\为前导 如果需要匹配反斜杠（\）字符本身，需要使用\\\ 计算字段拼接（Concat函数）： 1SELECT Concat(字段1，字段2，... ...) FROM 表名; 删除右侧多余空格（RTrim函数）（LTrim，Trim）： 1SELECT Concat(RTrim(字段1)，RTrim(字段2)) FROM 表名; 使用别名： 1SELECT Concat(RTrim(字段1), RTrim(字段2)) AS 别名 FROM 表名; 执行算术计算： 1SELECT 字段1，字段2，字段1*字段2 AS 字段名 FROM 表名; 数据处理函数1SELECT 字段，Upper(字段) AS 字段 FROM 表名; 文本处理函数： 函数 说明 Length() 返回串左边长度 Lower() 将串转换为小写 Soundex() 返回串的SOUNDEX值（建议百度） Upper() 将串转换为大写 日期时间处理函数： 12345// 只比较日期SELECT 字段 FROM 表名 WHERE Date(字段) = 'YYYY-MM-DD';// 只比较时间SELECT 字段 FROM 表名 WHERE Time(字段) = 'hh-mm-ss'; 函数 说明 CurDate() 返回当前日期 CurTime() 返回当前时间 Date() 返回日期时间的日期部分（推荐使用） DateDiff() 计算两个日期之差 Date_Format() 返回一个格式化的日期或时间串 Now() 返回当前日期和时间 Time() 返回一个日期时间的时间部分（推荐使用） 汇总数据聚集函数： 123456SELECT AVG(字段) AS 字段 FROM 表名;// DISTINCT用来聚集不同的值（MySQL 4.x不能使用）SELECT AVG(DISTINCT 字段) AS 字段 FROM 表名;SELECT COUNT(*) AS 字段，MIN(字段) AS 字段，MAX(字段) AS 字段，AVG(字段) AS 字段 FROM 表名; 函数 说明 AVG() 返回某列平均值 COUNT() 返回某列行数（注意COUNT(*)与COUNT(column)） MAX() 返回某列最大值 (可应用于日期与文本数据) MIN() 返回某列最小值 SUM() 返回某列之和 分组数据1SELECT 字段1，COUNT(*) AS 字段 FROM 表名 GROUP BY 字段1; 过滤分组： 1234SELECT 字段1，COUNT(*) AS 字段 FROM 表名 GROUP BY 字段1 HAVING COUNT(*) &gt;= 2;//HAVING与WHERESELECT 字段1，COUNT(*) AS 字段 FROM 表名 WHERE 字段 &gt;= 10 GROUP BY 字段1 HAVING COUNT(*) &gt;= 2; SELECT 子句顺序： 子句 是否必须使用 SELECT 是 FROM 否 WHERE 否 GROUP BY 否 HAVING 否 ORDER BY 否 LIMIT 否 子查询1234SELECT 字段1 FROM 表名1 WHERE 字段2 IN (SELECT 字段3 FROM 表名2 WHERE 字段4 = 字符串);SELECT 字段1, (SELECT COUNT(*) FROM 表名1 WHERE 字段2 = 字段3) AS 字段 FROM 表名2; 联结创建联结： 1select vend_name, prod_name, prod_price(指定列) from vendors, products(关系表) where vendors.vend_id = products.vend_id order by vend_name, pro_prod_name; 1select vend_name, prod_name, prod_price(指定列) from vendors inner join products on vendors.vend_id = products.vend_id; 建议使用第二个语法。 笛卡尔积（叉联结）： 1select vend_name, prod_name, prod_price(指定列) from vendors, products(关系表) order by vend_name, pro_prod_name; 由没有联结条件的表关系返回的结果为笛卡尔积。检索出的行的数目将是第一个表的行数乘以第二个表中的行数。 联结多个表： 1SELECT prod_name, vend_name, prod_price, quantity FROM orderitems, products, vendors WHERE prodicts.vend_id = vendors.vend_id AND orderitems.prod_id = products.prod_id AND order_num = 20005; 高级联结表别名： 1SELECT cust_name, cust_contact FROM customers AS c, orders AS o, orderitems AS oi WHERE c.cust_id = o.cust_id AND oi.order_num = o.order_num AND prod_id = 'TNT2'; 自联结： 12// 我觉得使用SELECT子查询也许更简单SELECT p1.prod_id, p1.prod_name FROM products AS p1, products AS p2 WHERE p1.vend_id = p2.vend_id AND WHERE p2.prod_id = 'DTNTR'; 自然联结：（常用，重点，基本上每个内部联结都是自然联结） 1SELECT c.*, o.order_num, o.order_date, oi.prod_id, o.quantity, oi.item_price FROM customers AS c, orders AS o, orderitems AS oi WHERE c.cust_id = o.cust_id AND oi.order_num = o.order_num AND prod_id = 'FB'; 外部联结： 1select customers.cust_id, orders.order_num from customers left outer join oders on customers.cust_id = orders.cust_id; 带聚集的联结： 1SELECT customers.cust_name, customers.cust_id, COUNT(orders.order_num) AS num_ord FROM customers INNER JOIN orders ON customers.cust_id = orders.cust_id GROUP BY customers.cust_id; 插入检索出的数据INSERT SELECT 语句： 1INSERT INTO 表名1(cust_id, cust_concact, cust_email, cust_name) SELECT cust_id, cust_concact, cust_email, cust_name FROM 表名2; 全文本搜索1SELECT 字段 FROM 表名 WHERE Match(字段) Against(搜索文本); 布尔文本搜索： 1SELECT 字段 FROM 表名 WHERE Match(字段) Against('+rabbit +fat' IN BOOLEAN MODE); 说明：返回的行必须同时包含关键字 rabbit 和 fat。 1SELECT 字段 FROM 表名 WHERE Match(字段) Against('rabbit good' IN BOOLEAN MODE); 说明：没有指定操作符，搜索包含rabbit或good的行（其中的一个或两个）。 1SELECT 字段 FROM 表名 WHERE Match(字段) Against('+fat +(&gt;rabbit)' IN BOOLEAN MODE); 说明：文本包含fat和rabbit的行，且增加后者的优先级值。 布尔操作符 描述 + 包含指定值 - 排除指定值 &gt; 包含指定值,并且增加优先级值 &lt; 包含指定值,并且减少优先级值 存储过程执行存储过程： 1CALL 存储过程名(@参数1，@参数2，@参数3); 创建存储过程： 1234CREATE PROCEDURE 存储过程名()BEGIN SELECT AVG(字段) AS 字段 FROM 表名;END; 删除存储过程： 1DROP PROCEDURE 存储过程名; 使用参数： 123456789CREATE PROCEDURE ordertotal(IN onumber INT, OUT ototal DECIMAL(8,2))BEGIN SELECT SUM(字段1*字段2) FROM 表名 WHERE order_num = onumber INTO ototal;END;CALL ordertotal(20005, @total);SELECT @total; 说明：创建存储过程PROCEDURE，传入onumber，存储变量为ototal，WHERE子句中使用onumber，将查询结果存储在ototal。 触发器建立触发器： 1CREATE TRIGGER 触发器名 AFTER INSERT ON 表名 FOR EACH ROW SELECT '字符串'; 说明：FOR EACH ROW 表明代码对每个插入行执行。 删除触发器： 1DROP TRIGGER 触发器名; INSERT触发器： 1CREATE TRIGGER 触发器名 AFTER INSERT ON 表名 FOR EACH ROW SELECT NEW.字段; 说明：每向表中插入一条数据之后，就返回最新插入数据中的字段值。 DELETE 触发器： 12CREATE TRIGGER 触发器名 BEFORE DELETE ON 表名 FOR EACH ROW BEGIN INSERT INTO 表名(字段1， 字段2) VALUES (字段值1， 字段值2); END; UPDATE触发器： 1CREATE TRIGGER 触发器名 BRFORE UPDATE ON 表名 FOR EACH ROW SET NEW.字段 = Upper(NEW.字段); 事务处理事务开始： 1START TRANSACTION; 123456SELECT * FROM ordertotals;START TRANSACTION;DELETE FROM ordertotals;SELECT * FROM ordertotals;ROLLBACK;SELECT * FROM ordertotals; 说明：ROLLBACK 回退START TRANSACTION 之后的所有语句。CREATE和DROP操作不能回退。 使用COMMIT： 1234START TRANSACTION;DELETE FROM orderitems WHERE order_num = 20010;DELETE FROM orders WHERE order_num = 20010;COMMIT; 使用保留点： 12SAVEPOINT 保留点名称1;ROLLBACK TO 保留点名称1; 管理用户创建用户帐号： 1CREATE USER 用户名 IDENTIFIED BY '密码'; 删除用户帐号： 1DROP USER 用户名; 授予权限： 1GRANT SELECT ON crashcourse.* TO 用户名; 说明：此GRANT 允许用户在crashcourse数据库的所有表上使用SELECT权限。 查看用户权限： 1SHOW GRANTS FOR 用户名; 撤销用户权限： 1REVOKE SELECT ON crashcourse.* FROM 用户名; 说明：撤销用户在crashcourse数据库所有表上的读权限。 更改口令： 1SET PASSWORD FOR 用户 = PASSWORD('new password');]]></content>
      <categories>
        <category>MySQL数据库</category>
      </categories>
      <tags>
        <tag>数据库</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java网络爬虫--JSON数据的解析]]></title>
    <url>%2F2017%2F04%2F02%2FJava%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB-JSON%E6%95%B0%E6%8D%AE%E7%9A%84%E8%A7%A3%E6%9E%90%2F</url>
    <content type="text"><![CDATA[有时候，我们抓取下来一个html页面，发现浏览器页面可以显示的东西在html源码中却没有，这时候我们就要考虑服务器是以JSON格式将这部分数据发送到客户端的，对于这种情况的处理方式我们一般是在chrome的开发者工具中找到对应的JSON包，然后构建其URL，对JSON数据所在的源地址进行访问，然后使用一些工具对JSON数据进行解析，从而得到我们想要的东西。 阿里巴巴FastJson是一个Json处理工具包，包括“序列化”和“反序列化”两部分，它具备如下特征： 速度最快，测试表明，FastJson具有极快的性能，超越任其他的Java Json parser。包括自称最快的JackJson；功能强大，完全支持Java Bean、集合、Map、日期、Enum，支持范型，支持自省；无依赖，能够直接运行在Java SE 5.0以上版本；支持Android；开源 (Apache 2.0) 源码地址：https://github.com/alibaba/fastjson FastJson下载地址：http://download.csdn.net/detail/pdsyzbaozi/8199419 FastJson使用Fastjson API入口类是com.alibaba.fastjson.JSON，常用的序列化操作都可以在JSON类上的静态方法直接完成。 12345678public static final Object parse(String text); // 把JSON文本parse为JSONObject或者JSONArray public static final JSONObject parseObject(String text); //把JSON文本parse成JSONObject public static final T parseObject(String text, Class clazz); // 把JSON文本parse为JavaBean public static final JSONArray parseArray(String text); // 把JSON文本parse成JSONArray public static final List parseArray(String text, Class clazz); //把JSON文本parse成JavaBean集合 public static final String toJSONString(Object object); // 将JavaBean序列化为JSON文本 public static final String toJSONString(Object object, boolean prettyFormat); // 将JavaBean序列化为带格式的JSON文本 public static final Object toJSON(Object javaObject); //将JavaBean转换为JSONObject或者JSONArray。 首先给大家一个我在知乎上看到的有关JavaBean的解释（看百度百科我也是很迷茫… …）感觉这个很言简意赅，它就是一个Java类： 所有属性为private 提供默认构造方法 提供getter和setter 实现serializable接口 下来我上具体的代码： 1234567891011121314151617181920212223242526272829303132/** * Created by paranoid on 17-4-2. */public class TestPerson &#123; private int age; private String name; public TestPerson() &#123; &#125; public TestPerson(int age, String name) &#123; this.age = age; this.name = name; &#125; public int getAge() &#123; return age; &#125; public void setAge(int age) &#123; this.age = age; &#125; public String getName() &#123; return name; &#125; public void setName(String name) &#123; this.name = name; &#125;&#125; 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263import com.alibaba.fastjson.JSON;import com.alibaba.fastjson.JSONArray;import com.alibaba.fastjson.JSONObject;import java.util.ArrayList;import java.util.List;import static java.lang.System.out;/** * Created by paranoid on 17-4-2. * * fastJson在反序列化的时候需要调用对象的默认构造方法，如果该对象之中还包含其他的对象， * 那么都需要创建默认的无参构造方法. */public class TestFastJson &#123; public static void main(String[] args) &#123; TestPerson testPerson = new TestPerson(19, "李明"); List&lt;TestPerson&gt; list = new ArrayList&lt;&gt;(); list.add(testPerson); list.add(new TestPerson(12, "张三")); //将集合或对象序列化为Json out.println(JSON.toJSON(testPerson)); out.println(JSON.toJSON(list)); //JSON串反序列化为对象 TestPerson person = JSON.parseObject("&#123;\"age\":19,\"name\":\"李明\"&#125;", TestPerson.class); out.printf("name:%s, age:%d\n", person.getName(), person.getAge()); //字符串对象反序列化成集合 String str = "[&#123;\"name\":\"李明\",\"age\":19&#125;,&#123;\"name\":\"张三\",\"age\":12&#125;]"; List&lt;TestPerson&gt; listPerson = JSON.parseArray(str,TestPerson.class); for(TestPerson item : listPerson)&#123; out.println(item.getName() ); out.println(item.getAge()); &#125; //没有对象直接解析JSON对象(常用) JSONObject jsonObject = JSON.parseObject("&#123;\"name\":\"李明\",\"age\":19&#125;"); System.out.printf("name: %s, age: %d\n", jsonObject.getString("name"), jsonObject.getBigInteger("age")); //没有对象直接解析JSON数组 JSONArray jsonArray = JSON.parseArray("[&#123;\"name\":\"李明\",\"age\":19&#125;," + "&#123;\"name\":\"张三\",\"age\":12&#125;]"); for(int i = 0; i &lt; jsonArray.size(); i++)&#123; JSONObject temp = jsonArray.getJSONObject(i); System.out.printf("name: %s, age: %d\n", temp.getString("name"), temp.getBigInteger("age")); &#125; for(Object obj : jsonArray)&#123; System.out.println(obj.toString()); &#125; &#125;&#125; 参考阅读Java的Json解析包FastJson使用]]></content>
      <categories>
        <category>Java网络爬虫</category>
      </categories>
      <tags>
        <tag>FastJson</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java网络爬虫--使用Jsoup的select语法进行元素查找]]></title>
    <url>%2F2017%2F03%2F30%2FJava%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB-%E4%BD%BF%E7%94%A8Jsoup%E7%9A%84select%E8%AF%AD%E6%B3%95%E8%BF%9B%E8%A1%8C%E5%85%83%E7%B4%A0%E6%9F%A5%E6%89%BE%2F</url>
    <content type="text"><![CDATA[使用Jsoup进行元素的查找有两种方法。有使用DOM方法来遍历一个文档，也有使用选择器语法来查找元素，而后者类似于CSS或jQuery的语法来查找和操作元素。对于这两个方法到底使用哪个感觉好上手我觉得因人而异，在我尝试了两种方法之后我还是选择select，所以我就只总结select的语法使用了，对于DOM方法感兴趣的，可以看一下这一篇博客：【使用JSOUP实现网络爬虫】使用DOM方法来遍历一个文档，看之前最好先了解一下Java网络爬虫–HTML DOM（HTML 基础）。 select详解Document 继承自 Element 类。select方法将返回一个Elements集合。 通过标签名查找测试代码： 12&lt;span&gt;33&lt;/span&gt;&lt;span&gt;25&lt;/span&gt; select写法： 1Elements elements = document.select("span"); 下面的例子都按照上面的格式来写，就不进行重复的标注了。 通过id查找12&lt;span id=\"mySpan\"&gt;36&lt;/span&gt;&lt;span&gt;20&lt;/span&gt; 12Elements elements = document.select("#mySpan");//通过id来查找，使用方法跟css指定元素一样，用# 通过class名查找12&lt;span class="myClass"&gt;36&lt;/span&gt;&lt;span&gt;20&lt;/span&gt; 12Elements elements = document.select(".myClass");// 使用方法跟css指定元素一样，用. 利用标签内属性名查找元素12&lt;span class="class1" id="id1"&gt;36&lt;/span&gt;&lt;span class="class2" id="id2"&gt;36&lt;/span&gt; 12Elements elements = document.select("span[class=class1]span[id=id1]");// 规则为 标签名【属性名=属性值】，多个属性即多个【】，如上 利用标签内属性名前缀查找元素12&lt;span class="class1" &gt;36&lt;/span&gt;&lt;span class="class2" &gt;22&lt;/span&gt; 12Elements elements = document.select("span[^cl]");// 规则为 标签名【^属性名前缀】，多个属性即多个【】 利用标签内属性名+正则表达式查找元素对正则表达式不了解的同学下去一定要学习正则表达式哦，因为它在爬虫中可是很重要的。 12&lt;span class="ABC" &gt;36&lt;/span&gt;&lt;span class="ADE" &gt;22&lt;/span&gt; 12Elements elements = document.select("span[class~=^AB]");// 规则为 标签名【属性名~=正则表达式】，以上的正则表达式的意思是查找class值以AB为开头的标签 利用标签文本包含某些内容查找元素12&lt;span&gt;36&lt;/span&gt;&lt;span&gt;22&lt;/span&gt; 12Elements elements = document.select("span:contains(3)");// 规则为 标签名:contains(文本值) 利用标签文本包含某些内容+正则表达式查找元素12&lt;span&gt;36&lt;/span&gt;&lt;span&gt;22&lt;/span&gt; 12Elements elements = document.select("span:matchesOwn(^3)");// 规则为 标签名:matchesOwn(正则表达式)，以上的正则表式的意思是以文本值以3为开头的标签 当然select还有其他强大的功能，如果对select感兴趣的同学可以查看select API，我只是列出了获取网页特定内容所需要的select的基本语法，基本上对于大部分的爬虫需求来说已经足够了。 下来给大家展示一个使用select获取特定元素值的代码： 12345678910111213141516public class SelectDemo &#123; public static void test(String html) &#123; //采用Jsoup解析 Document doc = Jsoup.parse(html); //System.out.println(html); //获取html标签中的内容 Elements elements = doc.select("ul[class=gl-warp clearfix]") .select("li[class=gl-item]"); for (Element ele : elements) &#123; String JdbookID = ele.attr("data-sku"); //out.println(JdbookID); &#125; &#125;&#125; 上面的代码是我爬京东图书提取图书的id的部分截取，可以看到select的用法与前面讲的没有什么区别。对于Element 这个类来说，如果我们要获取一个标签中的属性值或文本内容可以这样来做： 123456for (Element ele : elements) &#123; String JdbookID = ele.attr("data-sku"); //获取data-sku的属性值 //out.println(JdbookID); String text = ele.text(); //获取当前标签（元素）的文本值 //out.println(text);&#125;]]></content>
      <categories>
        <category>Java网络爬虫</category>
      </categories>
      <tags>
        <tag>网络爬虫</tag>
        <tag>Jsoup</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java网络爬虫--HttpClient设置头部信息与模拟登录策略]]></title>
    <url>%2F2017%2F03%2F26%2FJava%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB-HttpClient%E8%AE%BE%E7%BD%AE%E5%A4%B4%E9%83%A8%E4%BF%A1%E6%81%AF%E4%B8%8E%E6%A8%A1%E6%8B%9F%E7%99%BB%E5%BD%95%E7%AD%96%E7%95%A5%2F</url>
    <content type="text"><![CDATA[在网络爬虫中我们经常需要设置一些头部信息，使我们进行网页抓取的行为更加像使用浏览器浏览网页一般，并且我们有时需要将头部信息设置正确，才能得到正确的数据，要不然有可能得到的信息和浏览器所展示的页面有出入。 设置头部还可以进行模拟登录，我们可以设置Cookie，来得到登录后的页面，来抓取我们需要的数据。 接下来我会讲到进行模拟登录的两种方法。 添加头部Cookie进行模拟登录代码如下： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455import org.apache.http.Header;import org.apache.http.client.ClientProtocolException;import org.apache.http.client.methods.CloseableHttpResponse;import org.apache.http.client.methods.HttpGet;import org.apache.http.impl.client.CloseableHttpClient;import org.apache.http.impl.client.HttpClients;import java.io.IOException;import static java.lang.System.out;/** * Created by paranoid on 17-3-26. */public class HttpClientDemo &#123; public static void main(String[] args)&#123; //创建客户端 CloseableHttpClient closeableHttpClient = HttpClients.createDefault(); //创建请求Get实例 HttpGet httpGet = new HttpGet("https://www.baidu.com"); //设置头部信息进行模拟登录（添加登录后的Cookie） httpGet.setHeader("Accept", "text/html,application/xhtml+xml," + "application/xml;q=0.9,image/webp,*/*;q=0.8"); httpGet.setHeader("Accept-Encoding", "gzip, deflate, sdch, br"); httpGet.setHeader("Accept-Language", "zh-CN,zh;q=0.8"); //httpGet.setHeader("Cookie", "......."); httpGet.setHeader("User-Agent", "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36" + " (KHTML, like Gecko) Chrome/55.0.2883.87 Safari/537.36"); try &#123; //客户端执行httpGet方法，返回响应 CloseableHttpResponse closeableHttpResponse = closeableHttpClient.execute(httpGet); //得到服务响应状态码 if (closeableHttpResponse.getStatusLine().getStatusCode() == 200) &#123; //打印所有响应头 Header[] headers = closeableHttpResponse.getAllHeaders(); for (Header header : headers) &#123; out.println(header.getName() + ": " + header.getValue()); &#125; &#125; else &#123; //如果是其他状态码则做其他处理 &#125; &#125; catch (ClientProtocolException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; httpClient.close(); &#125; &#125;&#125; 只要在上述代码中添加了你登录后的Cookie就可以进行模拟登录啦，至于为什么添加了Cookie之后可以进行模拟登录，你可以在百度上搜索一下Cookie的机制与作用，相信你就明白了。 对于上述那些参数，大家可以在chrome的开发者工具里面进行获得，cookie是大家手动登录后产生的，也可以获得，设置了这些参数之后，我们既模拟了浏览器的行为又进行了模拟登录。 大家可以将没有添加Cookie的结果和添加了Cookie之后的结果进行比对，可以发现的确可以实现模拟登录。 我们接下来讨论实现模拟登录的另一种方法，比较麻烦，也有一定局限性（验证码），但却是使用最普遍的。具体选择哪种模拟登录的方式，到时你有项目需求了，就会明白～ HttpClient发送Post请求我们既然需要模拟登录，那么登录时所必须的参数：“用户名”，“密码”等都是可以通过HttpClient发送给对方的服务器的，所以我说HttpClient模仿的就是浏览器的行为，它可以满足我们对Web访问的基本需求。接下来用代码告诉大家怎么手动设计登录时所必要的参数并发送给对方服务器。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465import org.apache.http.Header;import org.apache.http.NameValuePair;import org.apache.http.client.ClientProtocolException;import org.apache.http.client.entity.UrlEncodedFormEntity;import org.apache.http.client.methods.CloseableHttpResponse;import org.apache.http.client.methods.HttpPost;import org.apache.http.impl.client.CloseableHttpClient;import org.apache.http.impl.client.HttpClients;import org.apache.http.message.BasicNameValuePair;import java.io.IOException;import java.io.UnsupportedEncodingException;import java.util.ArrayList;import java.util.List;import static java.lang.System.out;/** * Created by paranoid on 17-3-27. * * 模拟登录人人网 * * 提示：我们在模拟登录之前都要先手动登录然后通过抓包查看登录成功需要给对方服务器发送哪些参数，然后我们将这些参数进行提取，通过Post方法发送给* 对方服务器 */ public class SimulationLoginDemo &#123; public static void main(String[] args)&#123; //创建默认客户端 CloseableHttpClient closeableHttpClient = HttpClients.createDefault(); //创建Post请求实例 HttpPost httpPost = new HttpPost("http://www.renren.com/"); //创建参数列表 List&lt;NameValuePair&gt; nvps = new ArrayList&lt;&gt;(); nvps.add(new BasicNameValuePair("domain", "renren.com")); nvps.add(new BasicNameValuePair("isplogin", "true")); nvps.add(new BasicNameValuePair("submit", "登录")); nvps.add(new BasicNameValuePair("email", "")); nvps.add(new BasicNameValuePair("passwd", "")); //向对方服务器发送Post请求 try &#123; //将参数进行封装，提交到服务器端 httpPost.setEntity(new UrlEncodedFormEntity(nvps, "UTF8")); CloseableHttpResponse httpResponse = closeableHttpClient.execute(httpPost); //如果模拟登录成功 if(httpResponse.getStatusLine().getStatusCode() == 200) &#123; Header[] headers = httpResponse.getAllHeaders(); for (Header header : headers) &#123; out.println(header.getName() + ": " + header.getValue()); &#125; &#125; &#125; catch (UnsupportedEncodingException e) &#123; e.printStackTrace(); &#125; catch (ClientProtocolException e) &#123; e.printStackTrace(); &#125; catch (IOException e) &#123; e.printStackTrace(); &#125; finally &#123; httpPost.abort(); //释放资源 &#125; &#125;&#125; 参考阅读 网络爬虫中的模拟登陆获取数据（实例教学） 使用HttpClient模拟登录百度账号]]></content>
      <categories>
        <category>Java网络爬虫</category>
      </categories>
      <tags>
        <tag>网络爬虫</tag>
        <tag>模拟登陆</tag>
        <tag>HttpClient</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Java网络爬虫--HTML DOM（HTML 基础）]]></title>
    <url>%2F2017%2F02%2F27%2FJava%E7%BD%91%E7%BB%9C%E7%88%AC%E8%99%AB-HTML-DOM%EF%BC%88HTML-%E5%9F%BA%E7%A1%80%EF%BC%89%2F</url>
    <content type="text"><![CDATA[最近一直在学习Java方面的网络爬虫，然后要使用Jsoup解析html页面（htmlparse已经不建议使用了），但是对于Jsoup中的很多类和方法都很疑惑，查阅相关资料后发现使用Jsoup之前首先要了解一些HTML DOM方面的知识，就决定将网上的HTML DOM相关知识进行汇总，方便和我有相同兴趣爱好的伙伴学习。 不管是学习网络爬虫还是HTML DOM，都需要有一定的HTML基础，我推荐大家看看HTML教程，里面的内容很多，我只看了自己需要的部分，大家也可以根据自己的情况选择性的观看。 接下来我们进入正题！ DOM 简介HTML DOM 定义了访问和操作 HTML 文档的标准方法。DOM 将 HTML 文档表达为树结构。如下图： DOM 节点 根据 W3C 的 HTML DOM 标准，HTML 文档中的所有内容都是节点： 整个文档是一个文档节点 每个 HTML 元素是元素节点 HTML 元素内的文本是文本节点 每个 HTML 属性是属性节点 注释是注释节点 在上面我已经给出了节点树结构。 节点父、子和同胞节点树中的节点彼此拥有层级关系。父（parent）、子（child）和同胞（sibling）等术语用于描述这些关系。父节点拥有子节点。同级的子节点被称为同胞（兄弟或姐妹）。 在节点树中，顶端节点被称为根（root），每个节点都有父节点、除了根（它没有父节点），一个节点可拥有任意数量的子节点，同胞是拥有相同父节点的节点。 下面的图片展示了节点树的一部分，以及节点之间的关系： DOM 访问访问 HTML 元素（节点）访问 HTML 元素等同于访问节点。 您能够以不同的方式访问 HTML 元素： 通过使用 getElementById() 方法通过使用 getElementsByTagName() 方法通过使用 getElementsByClassName() 方法 getElementById() 方法：getElementById() 方法返回带有指定 ID 的元素。下面的例子获取 id=”intro” 的元素： 1document.getElementById("intro"); getElementsByTagName() 方法：getElementsByTagName() 返回带有指定标签名的所有元素。下面的例子返回包含文档中所有 p 元素的列表： 1document.getElementsByTagName("p"); 下面的例子返回包含文档中所有 p元素的列表，并且这些 p 元素应该是 id=”main” 的元素的后代（子、孙等等）： 1document.getElementById("main").getElementsByTagName("p"); getElementsByClassName() 方法：如果您希望查找带有相同类名的所有 HTML 元素，请使用这个方法。下面的例子返回包含 class=”intro” 的所有元素的一个列表： 1document.getElementsByClassName("intro"); DOM 方法方法是我们可以在节点（HTML 元素）上执行的动作。 编程接口可通过 JavaScript （以及其他编程语言）对 HTML DOM 进行访问。所有 HTML 元素被定义为对象，而编程接口则是对象方法和对象属性。方法是您能够执行的动作（比如添加或修改元素）。属性是您能够获取或设置的值（比如节点的名称或内容）。 HTML DOM 对象 - 方法和属性 一些常用的 HTML DOM 方法： getElementById(id) - 获取带有指定 id 的节点（元素）。 appendChild(node) - 插入新的子节点（元素）。 removeChild(node) - 删除子节点（元素）。 一些常用的 HTML DOM 属性： innerHTML - 节点（元素）的文本值。 parentNode - 节点（元素）的父节点。 childNodes - 节点（元素）的子节点。 attributes - 节点（元素）的属性节点。 一些 DOM 对象方法这里提供一些常用方法： DOM 属性获取元素内容的最简单方法是使用 innerHTML 属性。innerHTML 属性对于获取或替换 HTML 元素的内容很有用。 innerHTML 属性下面的代码获取 id=”intro” 的 p 元素的 innerHTML： 123456789101112&lt;html&gt; &lt;body&gt; &lt;p id="intro"&gt;Hello World!&lt;/p&gt; &lt;script&gt; var txt=document.getElementById("intro").innerHTML; document.write(txt); &lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 在上面的例子中，getElementById 是一个方法，而 innerHTML 是属性。innerHTML 属性可用于获取或改变任意 HTML 元素，包括 html 和 body。 nodeName 属性 nodeName 属性规定节点的名称。nodeName 是只读的。 元素节点的 nodeName 与标签名相同。 属性节点的 nodeName 与属性名相同。 文本节点的 nodeName 始终是 #text。 文档节点的 nodeName 始终是 #document。 nodeValue 属性 nodeValue 属性规定节点的值。 元素节点的 nodeValue 是 undefined 或 null。 文本节点的 nodeValue 是文本本身。 属性节点的 nodeValue 是属性值。 获取元素的值下面的例子会取回 p id=”intro” 标签的文本节点值： 123456789101112&lt;html&gt; &lt;body&gt; &lt;p id="intro"&gt;Hello World!&lt;/p&gt; &lt;script type="text/javascript"&gt; x=document.getElementById("intro"); document.write(x.firstChild.nodeValue); &lt;/script&gt; &lt;/body&gt;&lt;/html&gt; nodeType 属性nodeType 属性返回节点的类型。nodeType 是只读的。比较重要的节点类型有： DOM 导航通过 HTML DOM，您能够使用节点关系在节点树中导航。 HTML DOM 节点列表getElementsByTagName() 方法返回节点列表。节点列表是一个节点数组。下面的代码选取文档中的所有 p 节点： 1var x = document.getElementsByTagName("p"); 可以通过下标号访问这些节点。如需访问第二个 p节点，你可以这么写： 1y = x[1]; HTML DOM 节点列表长度length 属性定义节点列表中节点的数量。你可以使用 length 属性来循环节点列表： 1234567x=document.getElementsByTagName("p");for (i=0;i&lt;x.length;i++)&#123; document.write(x[i].innerHTML); document.write("&lt;br /&gt;");&#125; 导航节点关系你能够使用三个节点属性：parentNode、firstChild 以及 lastChild ，在文档结构中进行导航。请看下面的 HTML 片段： 1234567891011&lt;html&gt; &lt;body&gt; &lt;p&gt;Hello World!&lt;/p&gt; &lt;div&gt; &lt;p&gt;DOM 很有用!&lt;/p&gt; &lt;p&gt;本例演示节点关系。&lt;/p&gt; &lt;/div&gt; &lt;/body&gt;&lt;/html&gt; 首个 p 元素是 body 元素的首个子元素（firstChild）；div 元素是 body 元素的最后一个子元素（lastChild）；body 元素是首个 &gt; 元素和 div 元素的父节点（parentNode）。 firstChild 属性可用于访问元素的文本： 123456789101112&lt;html&gt; &lt;body&gt; &lt;p id="intro"&gt;Hello World!&lt;/p&gt; &lt;script&gt; x=document.getElementById("intro"); document.write(x.firstChild.nodeValue); &lt;/script&gt; &lt;/body&gt;&lt;/html&gt; DOM 根节点这里有两个特殊的属性，可以访问全部文档： document.documentElement - 全部文档；document.body - 文档的主体。 12345678910111213141516&lt;html&gt; &lt;body&gt; &lt;p&gt;Hello World!&lt;/p&gt; &lt;div&gt; &lt;p&gt;DOM 很有用!&lt;/p&gt; &lt;p&gt;本例演示 &lt;b&gt;document.body&lt;/b&gt; 属性。&lt;/p&gt; &lt;/div&gt; &lt;script&gt; alert(document.body.innerHTML); &lt;/script&gt; &lt;/body&gt;&lt;/html&gt; childNodes 和 nodeValue除了 innerHTML 属性，您也可以使用 childNodes 和 nodeValue 属性来获取元素的内容。下面的代码获取 id=”intro” 的 p 元素的值： 123456789101112&lt;html&gt; &lt;body&gt; &lt;p id="intro"&gt;Hello World!&lt;/p&gt; &lt;script&gt; var txt=document.getElementById("intro").childNodes[0].nodeValue; document.write(txt); &lt;/script&gt; &lt;/body&gt;&lt;/html&gt; 对于HTML DOM我就总结这么多，学习之后相信掌握Jsoup的基本方法应该是不在话下，HTML DOM还有很多知识，我只是给大家提取出来对于学习Java网络爬虫这方面需要掌握的基本知识与概念而已，如果大家想要深入学习，还是要在找找其他资料。]]></content>
      <categories>
        <category>Java网络爬虫</category>
      </categories>
      <tags>
        <tag>HTML DOM</tag>
      </tags>
  </entry>
</search>
